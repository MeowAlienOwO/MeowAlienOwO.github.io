---
title: 穿越到异世界的魔物使生活：线性模型
tags: ['ml', '西瓜书']
categories: ['异世界机器学习']
cover:
  image: null
  author: null
  source: null
date: 2019-12-18 10:40:03
---

姑且是准备入 PhD 坑了，基础的东西更需要好好复习。本文的内容会覆盖西瓜书第三章的内容。在线性模型的部分， MLPR 里头学的一些东西也会尽量地放进来。上次看到某学弟的深度学习入门文章写得很不错，要么尝试一下转变文风吧(

<!-- more -->

# 背景故事：穿越到异世界的召唤师

你穿越到了一个异世界。

在落地的一阵晕眩后，你努力地试图理解发生了什么：你研究生毕业，单身，在某大手IT会社工作半年。长年累月的月月火水木金金的学习工作让你变强了，也变秃了。虽然你还单身，天天被隔壁的美工与销售嘲讽，但是只要再忍一忍就有望转职成能手搓火球的大魔法师，有着光明的未来。这天，你被老板派遣去某个客户那里进行一些现场工作，但是你刚出门就被一道白光所笼罩……

不管怎么说，你来到了这个世界，也没有短时间能回去的迹象。在这个新世界生存下来是当务之急的第一任务。这个世界有大量的
迷宫一般的地下城，地下城里除了或凶猛或狡猾的魔物，还有大量的素材，矿物，草药等等高价物品的出产。因此，冒险者经济蓬
勃发展。战士，法师，牧师，<del>暗牧</del>，弓箭手……各种各样的职业冒险家组成队伍探索地下城。当他们返回地上时，往
往带着大包小包的商品。为了照顾冒险家的需求，地下城的附近如雨后春笋一般冒出了旅馆、酒馆、教会、道具店等各种各样的设施。不知运气好是不好，你刚好落在了一个由于地下城而形成的聚居地。

按照你熬夜打游戏看小说的经验，一般来钱的方法要么就是到地下城打怪挖宝，然后卖东西；要么就是拥有类似钓鱼，狩猎之类的
生产技能，能够通过销售生产的物资来赚钱。你打开了技能栏，显然的，由于在穿越之前过着一日三餐靠外卖，每天上班996的生
活，生产技能可是一个也没有。而在职业技能栏，大大的“召唤师”三个字把你转职成大魔法师的希望一扫而空，而翻遍了整个技能
栏，你能找到的只有“召唤史莱姆”这么一个技能。

一般来说，召唤师在哪里都是职业鄙视链最底层的存在：召唤出来的魔物要么弱智不堪，需要召唤者发号施令，浪费精力，抓不住
时机；要么就完全不可操控，自行其是，搞乱整个战场，痛击队友。更何况，史莱姆也是魔物界最弱鸡的存在。但是眼下也没有其
他办法，总之先把史莱姆召唤出来，先挣点钱保证生存再说。

在好一阵鸡飞狗跳过后，你好不容易带着你的史莱姆拳打南山史莱姆幼儿园，脚踢北海哥布林敬老院，再加上捡其他冒险者落下的素材武器，好不容易凑出了能够生存几天的费用。在旅馆里，你一边碎碎念于刚才被奸商狠宰的一笔面包钱，一边鼓捣着自己的那只弱鸡史莱姆，左戳戳，右戳戳，看看能不能改善一下它在战斗中跟睡着了一样毫无反应的问题。不知怎么的，史莱姆突然瘫成了一滩水一样的平面。你赶紧打开手边那本《召唤师入门大全》，希望能够找到问题所在。

出乎你意料的，看了半天你看不懂但是莫名其妙能明白的文字，你突然明白了你召唤的史莱姆是一种特别珍稀的**可控制**史莱、
姆。这只史莱姆的攻击防御跟其他的史莱姆没什么不同，但是可以通过一个线性函数来控制它攻击的时机与力道。而你，在穿越之
前，正好是项目组里的机器学习担当。这意味着，虽然能力相同，但是你完全可以靠着控制史莱姆的攻击与防御来实现更好的攻击
效率。另一方面，现有的机器学习模型能够帮你解决一些现实问题--比如，如何防止再次掉入奸商的面包价格陷阱。

“什么嘛，我学到的东西果然还是有用的嘛！”你这么想着。只要不停下来，机器学习之道就会向前延伸——

<!-- 你决定首先解决自己的温饱问题。刚穿越过来，用来吃饭的钱自然是一份都没有，身上的装备都是从新手村外的垃圾场翻出来的，也卖不出多少钱。你阴差阳错地打开了技能栏，看到了你现在仅有的技能：史莱姆契约。在为了没有任何生活技能沮丧了一阵后（在穿越前，你过着一日三餐靠外卖，每天上班996，唯一的娱乐是周末在家睡一天的生活，也很正常），你下决心要靠着这个仅有的技能先把每天的饭钱挣出来。 -->

<!-- 同绝大多数异世界一样，史莱姆也是这里魔物界的最底端物种之一，不论是攻击防御法抗都处于新手也能打爆的程度。现在的你只能操纵一只史莱姆，与你的5战斗力加在一起也仅仅能勉强打败一只跟史莱姆同级的魔物，掉落加上任务奖励也就刚刚够一天的饭钱。雪上加霜的是，你完全没有看到自己升级的迹象。不能升级就意味着不能掌握更强的技能，不能提升自己的能力，一辈子永远在新手村打转。 -->

<!-- 有一天，你突然发现史莱姆的一个奇怪的特性：如果喂给史莱姆不同数量的石子，史莱姆会把石子合成为不同大小的石块排出，而排出的石块的大小可以通过给史莱姆喂食不同的药草进行调整。而史莱姆可以分裂，每个小史莱姆战斗力也因此减半，不过对石子合成的能力并没有改变。虽然分裂在战斗中用处不大，但是联想到这两天的补给品价格波动把自己好不容易攒下的钱都搭了进去，作为前AI工程师的你灵机一动，打算训练一个史莱姆模型来预测补给品的价格。 -->

# 第三章 线性模型


## 机器学习的普遍形式
如果我们从更高一层次的角度来思考机器学习，我们大致可以将机器学习定义成通过样本的特征，通过学习器，输出标签的过程。其中，通过不断地更新学习器，我们可以提升机器学习的表现（正确率等）。在史莱姆模型中，我们的任务可以定义如下：

1. 样本的特征: 影响补给品的价格的因素，比如面包的类型（黑面包，白面包等），今日回城的冒险者数量，面包的日产量，历史价格等，用不同数量的石子输入
2. 输出标签：补给品的价格，用石子表示
3. 学习器：弱小可怜但能吃的史莱姆，吃进代表样本特征的石头，吐出代表标签的石头。史莱姆的状态可以通过喂食不同的草药进行调整

除此之外，我们需要一个损失函数来衡量我们的学习器距离目标有多“远”。这个目标可以是正确的标签，也可以是另外的衡量标准等。我们还需要一个优化器，来将我们的学习器调整到正确的状态。对于史莱姆模型而言：
1. 损失函数：我们要衡量我们预测的价格跟正确的价格之间的差距
2. 优化器：你自己，通过不断地喂食来改变史莱姆的状态。


## 线性模型的基本形式

线性模型是最基本的机器学习模型之一。线性模型认为，我们需要的标签是输入特征的*线性组合*：给定每个输入特征以某个权重，输出标签由这些权重与相应特征值的加权所决定。令`$f$`为模型，`$w$`为参数，`$x$` 为样本的特征，我们可以将线性模型简单地表示如下：

$$
f(x_1, x_2, ... x_n) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b 
$$

其中 `$b$` 是截距。

根据输出标签的种类划分，机器学习有两大任务：
1. 回归任务，输出的标签是一个连续值，例如补给品的价格。
2. 分类任务，输出的标签是一个离散值，例如明天是否会下雨等。

线性模型可以解决这两个方面的问题。根据故事中的你的需求，我们先来看看如何解决预测补给品价格这种回归类问题。

## 线性回归

线性回归使用线性模型的加权值作为预测的标签，即上文的函数`$f$`的函数值。线性回归的目标是求得一组恰当的权重(`$w$`, `$b$`)，
从而使得函数 `$f$` 能够尽可能准确地输出预测值。换句话说，我们希望函数的预测值 `$f(x)$` 与真实的标签 `$y$` 之间
的“距离”最小。一般而言，在线性回归任务中，我们使用均方误差函数来衡量这个“距离”：

$$
J(\theta) = \sum_{i=0}^N (f(x_i) - y_i)^2
$$
其中 `$\theta$` 指模型的参数，在线性回归即指的是权重 w 与截距 b。
注意到均方误差同欧氏距离有着紧密的联系，从可解释性的角度来说，均方误差很直观地表现了“距离目标有多远”这个概念。因此，只要使均方误差函数最小，我们就可以得到一个最优化的预测函数。

### 史莱姆模型：一元线性回归预测面包的价格
现在，你回忆起了线性回归的相关知识，但是由于你的能力还比较弱小，你决定先用面包的价格来测试一下构建史莱姆模型是否可
行。已知面包的价格由面包的重量，种类，店家等因素影响。你想先用一个一元线性回归模型来预测一下面包的重量与价格的关
系。于是，你定义机器学习任务如下：

1. 输入特征：面包的重量，种类，店家……
2. 输出标签：面包的价格
3. 学习器：线性模型
4. 损失函数：均方误差函数
5. 优化器：在这个阶段，你决定先用手动调节的方式进行调参。

![权重史莱姆](/images/slime_weight.png)

按照这个要求，你从收集的价格数据里面摘出参数如下：

| 重量(x) | 价格(y) |
|---|---|
| 100 | 25 |
| 200 | 35 |
| 300 | 45 |
| 50  | 20 |
| 150 | 32 |
| 250 | 39 |
| 350 | 51 |
| 99  | 25 |
| 199 | 35 |
| 299 | 44 |
| 399 | 53 |

现在，你需要调整史莱姆的状态，使得**损失函数**的值在面包数据集上最小。为了简单清晰起见，我们用数学的方式表示出来：
$$
\begin{aligned}
\min J(w, b) &= \sum_i^N (y_i - f(x_i))^2\\
&= \sum_i^N (y_i - w x_i - b)^2
\end{aligned}
$$

根据基本的微积分知识（谢天谢地，你还记得导数是什么），很显然的，对于一个二次函数而言，导数为 0 的时候有最值，而我们的损失函数显然是一个开口向上的二次函数，也就是说导数为 0 的时候有最小值。对于多元函数而言（这里的元是权重与截距，而不是 x, y)，二次多元函数的每个分量取偏导数为 0 的时候有最小值。对这个函数我们可以列方程如下：

$$
\begin{aligned}
\frac{\partial J}{\partial w} &= 2(w \sum_i x_i^2 + \sum_i(b - y_i)x_i) = 0\\
\frac{\partial J}{\partial b} &= 2(Nb + \sum_i(wx_i - y_i)) = 0
\end{aligned}
$$
解方程可得:
$$
\begin{aligned}
w &= \frac{\sum_i(y_i - \overline y)x_i}{\sum_i (x_i - \overline x)x_i}\\
b &= \overline y - w \overline x
\end{aligned}
$$

在变量上加横线如 `$\overline y$` 这样的形式来表示平均值。

代入面包数据，计算可得 w = 0.096, b = 15.75。剩下的工作，就是不断试错调整史莱姆的状态

### 史莱姆模型：实现截距史莱姆与多元线性回归
经过一番九牛二虎之力，你终于搞定了权重 w 与 截距 b。但是，你希望有一种更加简洁，统一的方式来表示权重与截距。

一种最简单的方式：每次喂给史莱姆一个常数值“1”，这样子史莱姆每次都会输出定值。写成公式表示如下：

$$
\begin{aligned}
f(x) &= w_1 x_1 + w_2 x_2\\
x_1 &= x\\
x_2 &= 1
\end{aligned}
$$

现在，你发现了权重跟截距都有统一的表达方式，也就是说，可以用同一种方式调♂教截距与权重了。
你又发现，如果需要增加一个特征的话，你可以很简单地增加一个权重，即`$wx$`项。
根据你回忆起的知识，这叫做**多元线性回归**。

不用尝试，你也知道如果特征不断增加，你你将会调整一大堆权重的情况，一个个解方程显然非常累。
你想到了用矩阵的方式来表述这个公式：

$$
\begin{aligned}
  f(\mathbf{x}) &= \mathbf{w}^T\mathbf{x} =\sum_j w_j x_j\\

  \mathbf{w} &= \left [\begin{matrix}
                w_1 \\
                w_2 \\
                ... \\
                b
                \end{matrix}\right]\\
  \mathbf{x} &= \left [\begin{matrix}
                x_1\\
                x_2\\
                ...\\
                1
                \end{matrix}\right]\\
\end{aligned}
$$

这里 `$T$` 表示矩阵的转置，一般用列向量统一表示权重向量与特征向量。这样带来的一个好处是，如果使用一个二维矩阵`$\mathbf{x}$` 来表示整个数据集，我们可以简单地表示将函数作用于整个数据集所得到的向量：
$$
\mathbf{y}' = \mathbf{X}\mathbf{w} =  \left[ \begin{matrix} 
  f(x_1)\\
  f(x_2)\\
  ...\\
  f(x_N)
\end{matrix}\right]
$$

目标函数也因为矩阵表述变成了这样子：
$$
J(\theta=\mathbf{w}) = (\mathbf{y} - \mathbf{X}\mathbf{w})^T(\mathbf{y} - \mathbf{X}\mathbf{w})
$$

通过对权重进行求导（注：求导方法在之后讨论），我们可以得到如下方程：

$$
\frac{\partial J}{\partial \mathbf{w}} = 2\mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})
$$

由于这里涉及到线性代数的逆运算，有这么几种情况：

a) 当矩阵`$\mathbf{X}^T\mathbf{X}$`是满秩矩阵或者正定矩阵，则逆矩阵存在，解方程可得解析解

$$
\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

b) 当这个矩阵不满秩，则意味着其对应的线性方程组有多个解，我们需要在这些解中选择一个作为输出。一种简单的方法是添加正则项，从而抑制绝对值过大的参数。关于正则化，我们留到以后讨论。


| 重量(x_1) | 店家(x_2) | 价格(y) |
|---|---|
| 100 | 1 | 25 |
| 200 | 1 | 35 |
| 300 | 1 | 45 |
| 50  | 2 | 20 |
| 150 | 2 | 32 |
| 250 | 2 | 39 |
| 350 | 2 | 51 |
| 99  | 3 | 25 |
| 199 | 3 | 35 |
| 299 | 3 | 44 |
| 399 | 3 | 53 |

我们将数据集稍微扩展一下，加入了店家的因素。由于店家是离散的标号而不是一个定值，我们需要使用 one-hot 编码来对其进行处理。One-hot 编码的表现方式如下：
1. 店家1: [1, 0, 0]
2. 店家2: [0, 1, 0]
3. 店家3: [0, 0, 1]

可以看出，one-hot 编码的本质可以看做是在运算的时候，根据离散特征的不同取值，抽出不同的权重输入模型。比起直接输入 1, 2, 3 这样的序数， one-hot 编码可以完全避免序数的取值所造成的影响，同时由于每个离散值都有不同的权重，可以更好地表达离散特征的特点。
我们的数据变为这个样子：
| 重量(x_1) | 店家(x_2) | 价格(y) |
|---|---|
| 100 | 1, 0, 0 | 25 |
| 200 | 1, 0, 0 | 35 |
| 300 | 1, 0, 0 | 45 |
| 50  | 0, 1, 0 | 20 |
| 150 | 0, 1, 0 | 32 |
| 250 | 0, 1, 0 | 39 |
| 350 | 0, 1, 0 | 51 |
| 99  | 0, 0, 1 | 25 |
| 199 | 0, 0, 1 | 35 |
| 299 | 0, 0, 1 | 44 |
| 399 | 0, 0, 1 | 53 |



### 解决非线性问题：核函数与多项式

To be continued...