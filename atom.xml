<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>the Garden of Sinners</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://MeowAlienOwO.github.io/"/>
  <updated>2020-03-14T13:34:00.008Z</updated>
  <id>http://MeowAlienOwO.github.io/</id>
  
  <author>
    <name>MeowAlien喵星人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>穿越到异世界的魔物使生活：线性模型</title>
    <link href="http://MeowAlienOwO.github.io/2019/12/18/ml-watermelon-2/"/>
    <id>http://MeowAlienOwO.github.io/2019/12/18/ml-watermelon-2/</id>
    <published>2019-12-18T02:40:03.000Z</published>
    <updated>2020-03-14T13:34:00.008Z</updated>
    
    <content type="html"><![CDATA[<p>姑且是准备入 PhD 坑了，基础的东西更需要好好复习。本文的内容会覆盖西瓜书第三章的内容。在线性模型的部分， MLPR 里头学的一些东西也会尽量地放进来。上次看到某学弟的深度学习入门文章写得很不错，要么尝试一下转变文风吧(</p><a id="more"></a><h1 id="背景故事：穿越到异世界的召唤师"><a href="#背景故事：穿越到异世界的召唤师" class="headerlink" title="背景故事：穿越到异世界的召唤师"></a>背景故事：穿越到异世界的召唤师</h1><p>你穿越到了一个异世界。</p><p>在落地的一阵晕眩后，你努力地试图理解发生了什么：你研究生毕业，单身，在某大手IT会社工作半年。长年累月的月月火水木金金的学习工作让你变强了，也变秃了。虽然你还单身，天天被隔壁的美工与销售嘲讽，但是只要再忍一忍就有望转职成能手搓火球的大魔法师，有着光明的未来。这天，你被老板派遣去某个客户那里进行一些现场工作，但是你刚出门就被一道白光所笼罩……</p><p>不管怎么说，你来到了这个世界，也没有短时间能回去的迹象。在这个新世界生存下来是当务之急的第一任务。这个世界有大量的<br>迷宫一般的地下城，地下城里除了或凶猛或狡猾的魔物，还有大量的素材，矿物，草药等等高价物品的出产。因此，冒险者经济蓬<br>勃发展。战士，法师，牧师，<del>暗牧</del>，弓箭手……各种各样的职业冒险家组成队伍探索地下城。当他们返回地上时，往<br>往带着大包小包的商品。为了照顾冒险家的需求，地下城的附近如雨后春笋一般冒出了旅馆、酒馆、教会、道具店等各种各样的设施。不知运气好是不好，你刚好落在了一个由于地下城而形成的聚居地。</p><p>按照你熬夜打游戏看小说的经验，一般来钱的方法要么就是到地下城打怪挖宝，然后卖东西；要么就是拥有类似钓鱼，狩猎之类的<br>生产技能，能够通过销售生产的物资来赚钱。你打开了技能栏，显然的，由于在穿越之前过着一日三餐靠外卖，每天上班996的生<br>活，生产技能可是一个也没有。而在职业技能栏，大大的“召唤师”三个字把你转职成大魔法师的希望一扫而空，而翻遍了整个技能<br>栏，你能找到的只有“召唤史莱姆”这么一个技能。</p><p>一般来说，召唤师在哪里都是职业鄙视链最底层的存在：召唤出来的魔物要么弱智不堪，需要召唤者发号施令，浪费精力，抓不住<br>时机；要么就完全不可操控，自行其是，搞乱整个战场，痛击队友。更何况，史莱姆也是魔物界最弱鸡的存在。但是眼下也没有其<br>他办法，总之先把史莱姆召唤出来，先挣点钱保证生存再说。</p><p>在好一阵鸡飞狗跳过后，你好不容易带着你的史莱姆拳打南山史莱姆幼儿园，脚踢北海哥布林敬老院，再加上捡其他冒险者落下的素材武器，好不容易凑出了能够生存几天的费用。在旅馆里，你一边碎碎念于刚才被奸商狠宰的一笔面包钱，一边鼓捣着自己的那只弱鸡史莱姆，左戳戳，右戳戳，看看能不能改善一下它在战斗中跟睡着了一样毫无反应的问题。不知怎么的，史莱姆突然瘫成了一滩水一样的平面。你赶紧打开手边那本《召唤师入门大全》，希望能够找到问题所在。</p><p>出乎你意料的，看了半天你看不懂但是莫名其妙能明白的文字，你突然明白了你召唤的史莱姆是一种特别珍稀的<strong>可控制</strong>史莱、<br>姆。这只史莱姆的攻击防御跟其他的史莱姆没什么不同，但是可以通过一个线性函数来控制它攻击的时机与力道。而你，在穿越之<br>前，正好是项目组里的机器学习担当。这意味着，虽然能力相同，但是你完全可以靠着控制史莱姆的攻击与防御来实现更好的攻击<br>效率。另一方面，现有的机器学习模型能够帮你解决一些现实问题—比如，如何防止再次掉入奸商的面包价格陷阱。</p><p>“什么嘛，我学到的东西果然还是有用的嘛！”你这么想着。只要不停下来，机器学习之道就会向前延伸——</p><!-- 你决定首先解决自己的温饱问题。刚穿越过来，用来吃饭的钱自然是一份都没有，身上的装备都是从新手村外的垃圾场翻出来的，也卖不出多少钱。你阴差阳错地打开了技能栏，看到了你现在仅有的技能：史莱姆契约。在为了没有任何生活技能沮丧了一阵后（在穿越前，你过着一日三餐靠外卖，每天上班996，唯一的娱乐是周末在家睡一天的生活，也很正常），你下决心要靠着这个仅有的技能先把每天的饭钱挣出来。 --><!-- 同绝大多数异世界一样，史莱姆也是这里魔物界的最底端物种之一，不论是攻击防御法抗都处于新手也能打爆的程度。现在的你只能操纵一只史莱姆，与你的5战斗力加在一起也仅仅能勉强打败一只跟史莱姆同级的魔物，掉落加上任务奖励也就刚刚够一天的饭钱。雪上加霜的是，你完全没有看到自己升级的迹象。不能升级就意味着不能掌握更强的技能，不能提升自己的能力，一辈子永远在新手村打转。 --><!-- 有一天，你突然发现史莱姆的一个奇怪的特性：如果喂给史莱姆不同数量的石子，史莱姆会把石子合成为不同大小的石块排出，而排出的石块的大小可以通过给史莱姆喂食不同的药草进行调整。而史莱姆可以分裂，每个小史莱姆战斗力也因此减半，不过对石子合成的能力并没有改变。虽然分裂在战斗中用处不大，但是联想到这两天的补给品价格波动把自己好不容易攒下的钱都搭了进去，作为前AI工程师的你灵机一动，打算训练一个史莱姆模型来预测补给品的价格。 --><h1 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h1><h2 id="机器学习的普遍形式"><a href="#机器学习的普遍形式" class="headerlink" title="机器学习的普遍形式"></a>机器学习的普遍形式</h2><p>如果我们从更高一层次的角度来思考机器学习，我们大致可以将机器学习定义成通过样本的特征，通过学习器，输出标签的过程。其中，通过不断地更新学习器，我们可以提升机器学习的表现（正确率等）。在史莱姆模型中，我们的任务可以定义如下：</p><ol><li>样本的特征: 影响补给品的价格的因素，比如面包的类型（黑面包，白面包等），今日回城的冒险者数量，面包的日产量，历史价格等，用不同数量的石子输入</li><li>输出标签：补给品的价格，用石子表示</li><li>学习器：弱小可怜但能吃的史莱姆，吃进代表样本特征的石头，吐出代表标签的石头。史莱姆的状态可以通过喂食不同的草药进行调整</li></ol><p>除此之外，我们需要一个损失函数来衡量我们的学习器距离目标有多“远”。这个目标可以是正确的标签，也可以是另外的衡量标准等。我们还需要一个优化器，来将我们的学习器调整到正确的状态。对于史莱姆模型而言：</p><ol><li>损失函数：我们要衡量我们预测的价格跟正确的价格之间的差距</li><li>优化器：你自己，通过不断地喂食来改变史莱姆的状态。</li></ol><h2 id="线性模型的基本形式"><a href="#线性模型的基本形式" class="headerlink" title="线性模型的基本形式"></a>线性模型的基本形式</h2><p>线性模型是最基本的机器学习模型之一。线性模型认为，我们需要的标签是输入特征的<em>线性组合</em>：给定每个输入特征以某个权重，输出标签由这些权重与相应特征值的加权所决定。令<script type="math/tex">f</script>为模型，<script type="math/tex">w</script>为参数，<script type="math/tex">x</script> 为样本的特征，我们可以将线性模型简单地表示如下：</p><script type="math/tex; mode=display">f(x_1, x_2, ... x_n) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b</script><p>其中 <script type="math/tex">b</script> 是截距。</p><p>根据输出标签的种类划分，机器学习有两大任务：</p><ol><li>回归任务，输出的标签是一个连续值，例如补给品的价格。</li><li>分类任务，输出的标签是一个离散值，例如明天是否会下雨等。</li></ol><p>线性模型可以解决这两个方面的问题。根据故事中的你的需求，我们先来看看如何解决预测补给品价格这种回归类问题。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归使用线性模型的加权值作为预测的标签，即上文的函数<script type="math/tex">f</script>的函数值。线性回归的目标是求得一组恰当的权重(<script type="math/tex">w</script>, <script type="math/tex">b</script>)，<br>从而使得函数 <script type="math/tex">f</script> 能够尽可能准确地输出预测值。换句话说，我们希望函数的预测值 <script type="math/tex">f(x)</script> 与真实的标签 <script type="math/tex">y</script> 之间<br>的“距离”最小。一般而言，在线性回归任务中，我们使用均方误差函数来衡量这个“距离”：</p><script type="math/tex; mode=display">J(\theta) = \sum_{i=0}^N (f(x_i) - y_i)^2</script><p>其中 <script type="math/tex">\theta</script> 指模型的参数，在线性回归即指的是权重 w 与截距 b。<br>注意到均方误差同欧氏距离有着紧密的联系，从可解释性的角度来说，均方误差很直观地表现了“距离目标有多远”这个概念。因此，只要使均方误差函数最小，我们就可以得到一个最优化的预测函数。</p><h3 id="史莱姆模型：一元线性回归预测面包的价格"><a href="#史莱姆模型：一元线性回归预测面包的价格" class="headerlink" title="史莱姆模型：一元线性回归预测面包的价格"></a>史莱姆模型：一元线性回归预测面包的价格</h3><p>现在，你回忆起了线性回归的相关知识，但是由于你的能力还比较弱小，你决定先用面包的价格来测试一下构建史莱姆模型是否可<br>行。已知面包的价格由面包的重量，种类，店家等因素影响。你想先用一个一元线性回归模型来预测一下面包的重量与价格的关<br>系。于是，你定义机器学习任务如下：</p><ol><li>输入特征：面包的重量，种类，店家……</li><li>输出标签：面包的价格</li><li>学习器：线性模型</li><li>损失函数：均方误差函数</li><li>优化器：在这个阶段，你决定先用手动调节的方式进行调参。</li></ol><p><img src="/images/slime_weight.png" alt="权重史莱姆"></p><p>按照这个要求，你从收集的价格数据里面摘出参数如下：</p><div class="table-container"><table><thead><tr><th>重量(x)</th><th>价格(y)</th></tr></thead><tbody><tr><td>100</td><td>25</td></tr><tr><td>200</td><td>35</td></tr><tr><td>300</td><td>45</td></tr><tr><td>50</td><td>20</td></tr><tr><td>150</td><td>32</td></tr><tr><td>250</td><td>39</td></tr><tr><td>350</td><td>51</td></tr><tr><td>99</td><td>25</td></tr><tr><td>199</td><td>35</td></tr><tr><td>299</td><td>44</td></tr><tr><td>399</td><td>53</td></tr></tbody></table></div><p>现在，你需要调整史莱姆的状态，使得<strong>损失函数</strong>的值在面包数据集上最小。为了简单清晰起见，我们用数学的方式表示出来：</p><script type="math/tex; mode=display">\begin{aligned}\min J(w, b) &= \sum_i^N (y_i - f(x_i))^2\\&= \sum_i^N (y_i - w x_i - b)^2\end{aligned}</script><p>根据基本的微积分知识（谢天谢地，你还记得导数是什么），很显然的，对于一个二次函数而言，导数为 0 的时候有最值，而我们的损失函数显然是一个开口向上的二次函数，也就是说导数为 0 的时候有最小值。对于多元函数而言（这里的元是权重与截距，而不是 x, y)，二次多元函数的每个分量取偏导数为 0 的时候有最小值。对这个函数我们可以列方程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J}{\partial w} &= 2(w \sum_i x_i^2 + \sum_i(b - y_i)x_i) = 0\\\frac{\partial J}{\partial b} &= 2(Nb + \sum_i(wx_i - y_i)) = 0\end{aligned}</script><p>解方程可得:</p><script type="math/tex; mode=display">\begin{aligned}w &= \frac{\sum_i(y_i - \overline y)x_i}{\sum_i (x_i - \overline x)x_i}\\b &= \overline y - w \overline x\end{aligned}</script><p>在变量上加横线如 <script type="math/tex">\overline y</script> 这样的形式来表示平均值。</p><p>代入面包数据，计算可得 w = 0.096, b = 15.75。剩下的工作，就是不断试错调整史莱姆的状态</p><h3 id="史莱姆模型：实现截距史莱姆与多元线性回归"><a href="#史莱姆模型：实现截距史莱姆与多元线性回归" class="headerlink" title="史莱姆模型：实现截距史莱姆与多元线性回归"></a>史莱姆模型：实现截距史莱姆与多元线性回归</h3><p>经过一番九牛二虎之力，你终于搞定了权重 w 与 截距 b。但是，你希望有一种更加简洁，统一的方式来表示权重与截距。</p><p>一种最简单的方式：每次喂给史莱姆一个常数值“1”，这样子史莱姆每次都会输出定值。写成公式表示如下：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= w_1 x_1 + w_2 x_2\\x_1 &= x\\x_2 &= 1\end{aligned}</script><p>现在，你发现了权重跟截距都有统一的表达方式，也就是说，可以用同一种方式调♂教截距与权重了。<br>你又发现，如果需要增加一个特征的话，你可以很简单地增加一个权重，即<script type="math/tex">wx</script>项。<br>根据你回忆起的知识，这叫做<strong>多元线性回归</strong>。</p><p>不用尝试，你也知道如果特征不断增加，你你将会调整一大堆权重的情况，一个个解方程显然非常累。<br>你想到了用矩阵的方式来表述这个公式：</p><script type="math/tex; mode=display">\begin{aligned}  f(\mathbf{x}) &= \mathbf{w}^T\mathbf{x} =\sum_j w_j x_j\\  \mathbf{w} &= \left [\begin{matrix}                w_1 \\                w_2 \\                ... \\                b                \end{matrix}\right]\\  \mathbf{x} &= \left [\begin{matrix}                x_1\\                x_2\\                ...\\                1                \end{matrix}\right]\\\end{aligned}</script><p>这里 <script type="math/tex">T</script> 表示矩阵的转置，一般用列向量统一表示权重向量与特征向量。这样带来的一个好处是，如果使用一个二维矩阵<script type="math/tex">\mathbf{x}</script> 来表示整个数据集，我们可以简单地表示将函数作用于整个数据集所得到的向量：</p><script type="math/tex; mode=display">\mathbf{y}' = \mathbf{X}\mathbf{w} =  \left[ \begin{matrix}   f(x_1)\\  f(x_2)\\  ...\\  f(x_N)\end{matrix}\right]</script><p>目标函数也因为矩阵表述变成了这样子：</p><script type="math/tex; mode=display">J(\theta=\mathbf{w}) = (\mathbf{y} - \mathbf{X}\mathbf{w})^T(\mathbf{y} - \mathbf{X}\mathbf{w})</script><p>通过对权重进行求导（注：求导方法在之后讨论），我们可以得到如下方程：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{w}} = 2\mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})</script><p>由于这里涉及到线性代数的逆运算，有这么几种情况：</p><p>a) 当矩阵<script type="math/tex">\mathbf{X}^T\mathbf{X}</script>是满秩矩阵或者正定矩阵，则逆矩阵存在，解方程可得解析解</p><script type="math/tex; mode=display">\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}</script><p>b) 当这个矩阵不满秩，则意味着其对应的线性方程组有多个解，我们需要在这些解中选择一个作为输出。一种简单的方法是添加正则项，从而抑制绝对值过大的参数。关于正则化，我们留到以后讨论。</p><div class="table-container"><table><thead><tr><th>重量(x_1)</th><th>店家(x_2)</th><th>价格(y)</th></tr></thead><tbody><tr><td>100</td><td>1</td><td>25</td></tr><tr><td>200</td><td>1</td><td>35</td></tr><tr><td>300</td><td>1</td><td>45</td></tr><tr><td>50</td><td>2</td><td>20</td></tr><tr><td>150</td><td>2</td><td>32</td></tr><tr><td>250</td><td>2</td><td>39</td></tr><tr><td>350</td><td>2</td><td>51</td></tr><tr><td>99</td><td>3</td><td>25</td></tr><tr><td>199</td><td>3</td><td>35</td></tr><tr><td>299</td><td>3</td><td>44</td></tr><tr><td>399</td><td>3</td><td>53</td></tr></tbody></table></div><p>我们将数据集稍微扩展一下，加入了店家的因素。由于店家是离散的标号而不是一个定值，我们需要使用 one-hot 编码来对其进行处理。One-hot 编码的表现方式如下：</p><ol><li>店家1: [1, 0, 0]</li><li>店家2: [0, 1, 0]</li><li>店家3: [0, 0, 1]</li></ol><p>可以看出，one-hot 编码的本质可以看做是在运算的时候，根据离散特征的不同取值，抽出不同的权重输入模型。比起直接输入 1, 2, 3 这样的序数， one-hot 编码可以完全避免序数的取值所造成的影响，同时由于每个离散值都有不同的权重，可以更好地表达离散特征的特点。<br>我们的数据变为这个样子：<br>| 重量(x_1) | 店家(x_2) | 价格(y) |<br>|—-|—-|<br>| 100 | 1, 0, 0 | 25 |<br>| 200 | 1, 0, 0 | 35 |<br>| 300 | 1, 0, 0 | 45 |<br>| 50  | 0, 1, 0 | 20 |<br>| 150 | 0, 1, 0 | 32 |<br>| 250 | 0, 1, 0 | 39 |<br>| 350 | 0, 1, 0 | 51 |<br>| 99  | 0, 0, 1 | 25 |<br>| 199 | 0, 0, 1 | 35 |<br>| 299 | 0, 0, 1 | 44 |<br>| 399 | 0, 0, 1 | 53 |</p><h3 id="解决非线性问题：核函数与多项式"><a href="#解决非线性问题：核函数与多项式" class="headerlink" title="解决非线性问题：核函数与多项式"></a>解决非线性问题：核函数与多项式</h3><p>To be continued…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;姑且是准备入 PhD 坑了，基础的东西更需要好好复习。本文的内容会覆盖西瓜书第三章的内容。在线性模型的部分， MLPR 里头学的一些东西也会尽量地放进来。上次看到某学弟的深度学习入门文章写得很不错，要么尝试一下转变文风吧(&lt;/p&gt;
    
    </summary>
    
      <category term="异世界机器学习" scheme="http://MeowAlienOwO.github.io/categories/%E5%BC%82%E4%B8%96%E7%95%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
      <category term="西瓜书" scheme="http://MeowAlienOwO.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书笔记：1-2章(绪论，统计基础)</title>
    <link href="http://MeowAlienOwO.github.io/2019/12/17/ml-watermelon-1/"/>
    <id>http://MeowAlienOwO.github.io/2019/12/17/ml-watermelon-1/</id>
    <published>2019-12-17T09:16:00.000Z</published>
    <updated>2019-12-18T02:43:13.961Z</updated>
    
    <content type="html"><![CDATA[<p>开始认真刷西瓜书了，作为面试的准备。规划是重点公式+有价值的练习题推一遍或者编程实现，由于同时也要刷 leetcode 与找内推所以更新不定。本文会包括 1-2 章的内容以及习题。</p><p>一些碎碎念：之前跟朋友聊的时候说到在瓶颈期要保持产出——不论是知识还是产品，那么现在姑且就把知识整理一下吧，虽然做的有点晚，但是总比不做好。博客改成hexo以后好多以前的文章暂时没空搬过来，可能要留到以后整理了，因为好多图放在七牛但是七牛现在对未备案域名有限制所以全挂了，怎么处理得慢慢来了。</p><p>迁移到hexo的时候也有好多坑，可能另外有空说一下，包括但不限于 Mathjax 引入，代码高亮，分享……（想在leancloud注册账号，要人脸认证但是我手机不给认证……）等。而且实际上主题没写完，手机端的样式没仔细调，一些页面的样式没写，等等。但是毕竟是从头写起的一个主题，拖延了好久的想法终于实现80%，感觉还是不错的。<br><a id="more"></a></p><h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="一些基本概念的定义"><a href="#一些基本概念的定义" class="headerlink" title="一些基本概念的定义"></a>一些基本概念的定义</h2><ul><li>机器学习：使用计算的手段，利用经验改进自身性能。</li><li>模型：从数据中学的的结果。模型习得的是关于规律的一个假设 (hypothesis)，真实的规律被称为 ground truth</li><li>数据集： 数据的集合，每条记录是一个样本。也有认为数据集是一个对整个样本空间进行采样的样本。</li><li>特征向量：样本对应的属性的向量化，表示为特征空间的一个向量。</li><li>分类：预测值为离散值</li><li>回归：预测值为连续值</li><li>训练集，测试集: 训练集用于学习模型，测试集用于测试模型的性能</li><li>有监督学习：训练集上的样本给定标记信息</li><li>无监督学习：训练集上的样本无标记信息或者监督信息</li><li>泛化：学习的模型在未见过的样本上的适应能力</li><li>假设空间：所有可能的假设（模型）所组成的空间，学习的过程可以看做是在假设空间中搜索一个最适用的模型的过程</li><li>版本空间：与训练集一致的假设集合</li></ul><h2 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h2><p>由于训练集只是样本空间的某些采样的集合，仅仅依赖训练样本我们可以训练出若干个模型，但是没有办法判断哪种模型更好。为了选出更优的模型，我们会对某些类型有一些偏好（比如泛化能力等），这种偏好叫做假设偏好。</p><p>根据假设偏好，我们能够选出更优的模型；但是只有当假设偏好于问题匹配的时候，我们才能根据假设偏好选择出更适用于实际问题的模型。<br>根据“没有免费午餐”定理（证明见书），在真实目标函数是均匀分布的情况下，任何学习算法的期望性能都相同。换言之，讨论学习算法的性能与好坏必须基于实际问题，归纳偏好一定要与实际问题相符才能得出正确的结论。</p><p>机器学习的发展历史略。</p><h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><h3 id="西瓜数据集"><a href="#西瓜数据集" class="headerlink" title="西瓜数据集"></a>西瓜数据集</h3><p>先给出西瓜数据集1.1：</p><div class="table-container"><table><thead><tr><th>编号</th><th>色泽</th><th>根蒂</th><th>敲声</th><th>好瓜</th></tr></thead><tbody><tr><td>1</td><td>青绿</td><td>蜷缩</td><td>浊响</td><td>T</td></tr><tr><td>2</td><td>乌黑</td><td>蜷缩</td><td>浊响</td><td>T</td></tr><tr><td>3</td><td>青绿</td><td>硬挺</td><td>清脆</td><td>F</td></tr><tr><td>4</td><td>乌黑</td><td>稍卷</td><td>沉闷</td><td>F</td></tr></tbody></table></div><p>从表中我们得到</p><ul><li>特征空间：<br>  1.色泽：青绿、乌黑<br>  2.根蒂：蜷缩、硬挺、稍卷<br>  3.敲声：浊响、清脆、沉闷</li><li>标记空间：<ul><li>好瓜：T、F</li></ul></li></ul><h3 id="习题及解答"><a href="#习题及解答" class="headerlink" title="习题及解答"></a>习题及解答</h3><blockquote><p>1.1  表中若只包含1， 4两个样例，求版本空间</p></blockquote><p>假设空间的大小为每个属性的取值数 +1 （泛化在所有取值的操作符 *) 的连乘+1 再加上空集（即概念无效）</p><script type="math/tex; mode=display">1 + \prod_{k=1}^K (\#k+1)</script><p>我们要做的是两步:</p><ol><li>删除不包含数据1（正例）的假设</li><li>在上一步的基础上删除包含数据1与数据4的假设</li></ol><p>结果如下：</p><ol><li><code>青绿，蜷缩，浊响</code>(均无泛化的情况下，有唯一正例)</li><li><code>青绿，蜷缩，\*</code>；<code>青绿，\*，浊响</code>；<code>\*，蜷缩，浊响</code>(一个泛化)</li><li><code>青绿，\*, \*</code>；<code>\*, 蜷缩, \*</code>；<code>\*, \*, 浊响</code>(两个泛化)</li></ol><p>排除空集（有正例）与全部泛化（有反例）</p><blockquote><p>1.2 用析合范式表示数据集1.1的假设空间，计算可能的假设数，析合范式包含 k 个合取式</p></blockquote><p>首先，基于数据集我们可以计算出合取式的假设空间 (k=1) 为 3 * 4 *4 + 1 = 49</p><p>以下讨论定义<script type="math/tex">\#</script> 为计数函数。</p><ol><li>单独处理空集，取消空集后的选择数为 \(N = C_{48}^k\) ，加上空集的组合为 \(2N+1\)(所有组合都可并一个空集，加上单独的空集自身)</li><li>对三个属性泛化的表达式 <code>A = (a=*, b=*, c=*)</code> 与其他所有的泛化式均有：<code>(A or B) = A</code>，即<script type="math/tex">C_{47}^{k-1}</script></li><li>对两个属性泛化的表达式形如 <code>A = (a=0, b=*, c=*)</code>，重复的冗余形如 <code>B = (a=0, b=1, c=*)</code>。我们可以删除一个属性，<code>A&#39; = (b=*, c=*)</code>求其所有可能的泛化组合：<script type="math/tex">C_{\#_b\times\#_c+1}^{k-1}</script>，b, c 为非a的属性。两个属性的泛化表达式一共有<script type="math/tex">\sum_i \#_i</script>种。</li><li>对一个属性泛化的表达式形如 <code>A = (a=0, b=1, c=*)</code>，重复的冗余形如 <code>B = (a=0, b=1, c=2)</code>。其泛化组合即为<script type="math/tex">C_{\#c}^{k-1}</script>，一共有<script type="math/tex">\sum_i \#_a \#_b</script>种，其中<code>a, b</code>为非泛化的属性。</li><li>补集的情况不知道是不是算冗余，如果是的话情况会比较复杂，会跟k有关。</li></ol><p><a href="https://blog.csdn.net/icefire_tyh/article/details/52065626" target="_blank" rel="noopener">这里</a> 有一个编程实现，应当是考虑了补集的。</p><blockquote><p>1.3 若数据包含噪声，设计一种归纳偏好</p></blockquote><p>简单地说，特征类似应当具有类似的输出，但是这个要具体问题具体分析（反例：特征工程做得不好，有很多无效的特征,e.g. 特征向量维度 100 但是实际有用的维度只有1，此时这个归纳偏好就不太好）。</p><blockquote><p>1.4 用其他性能度量证明没有免费午餐定理</p></blockquote><p>TODO</p><h1 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章 模型评估与选择"></a>第二章 模型评估与选择</h1><h2 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h2><p>对于分类任务而言，若样本数为 m，分类错误数为 a， 则错误率定义为<script type="math/tex">E=a/m</script>，精度为<script type="math/tex">A = 1-E</script>。更一般的，我们将实际预测输出与样本真实输出的差异称作误差。在训练集上的误差为训练误差或者经验误差(training error)，在新样本上的误差为泛化误差(generalization error)。</p><p>过拟合指的是学习器在训练样本上学习的“太好”，以至于在非训练样本上表现的不好，即泛化误差大于训练误差。欠拟合指的是在训练样本上没有恰当地学习到模型。在真实的任务中，欠拟合比较少见而过拟合比较多见，所以我们通常集中于讨论如何避免过拟合。</p><h2 id="测试集及其划分"><a href="#测试集及其划分" class="headerlink" title="测试集及其划分"></a>测试集及其划分</h2><p>测试集通常用于判断样本的泛化能力。给定有 m 个数据的数据集 D，我们将其划分为测试集 T 与训练集 S。一般而言划分有如下方法： </p><ul><li>留出法：直接将数据集划分成两个互斥集合。我们在划分数据集的时候通常需要保证数据分布的一致性，避免划分的偏差影响结果。另外，测试集过小会导致测试结果不够稳定准确，测试集过大会导致学习的模型的性能不够好。一般而言训练集占比为 2/3 到 4/5。</li><li>交叉验证(cross validation) 将数据集先划分为 k 个互斥子集，对于每个子集尽量保持数据一致性；然后抽出一个作为测试集，剩余的合并成为训练集，从而进行 k 次训练。结果是这 k 次的均值。特例：留一法：子集的粒度划分到每一个样本。其好处是，评估的效果会跟真实效果比较接近；坏处是计算量偏大。</li><li>自助法(bootstrapping): 留出与交叉验证都会引入一些根据数据分布的不稳定因素，留一法能够最大程度上减小这种估计  偏差但是计算量太大。自助法以自助采样为基础，给定大小为 m 的数据集 D，每次随机挑选一个样本放入采样数据集 D’。执行 m 次后我们获得了一个包含 m 个样本的数据集 D’。 样本不被采集到的的概率为<code>1-(1/m)^m</code>，取 m 趋于无穷的极限得到大约有 1/e (36.5%)的样本不被采集到。于是我们可以将 D’ 用作训练集， <code>D - D&#39;</code> 用作测试集。自助法用于数据比较小的情况，而且自助法能够产生不同的数据集，对集成学习有好处；但是对初始数据集分布的改变也会引入偏差，因此在数据集足够的时候用的比较少。</li></ul><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><h3 id="均方误差，精度与错误率"><a href="#均方误差，精度与错误率" class="headerlink" title="均方误差，精度与错误率"></a>均方误差，精度与错误率</h3><p>如上文所述，分类任务通常使用错误率与精度来衡量学习器的性能。对于回归任务而言， 常用的性能度量是均方误差(mean square error)：</p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m} \sum_{i=1}^m (f(\mathbf{x}_i)-y_i)^2</script><p>其积分形式可以用于描述更为一般的数据分布:</p><script type="math/tex; mode=display">E(f;\mathcal{D}) = \int_{x\sim \mathcal{D}} (f(\mathbf{x}) - y)^2p(\mathbf{x})d\mathbf{x}</script><p>类似的，分类错误率定义为：</p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m} \sum_{i=1}^m \mathbb{I}(f(\mathbf{x}_i) \neq y_i)\\E(f;\mathcal{D}) = \int_{x\sim \mathcal{D}} \mathbb{I}(f(\mathbf{x}) \neq y)p(\mathbf{x})d\mathbf{x}</script><p>精度定义为：</p><script type="math/tex; mode=display">E(f;D) = \frac{1}{m} \sum_{i=1}^m \mathbb{I}(f(\mathbf{x}_i) = y_i)\\E(f;\mathcal{D}) = \int_{x\sim \mathcal{D}} \mathbb{I}(f(\mathbf{x}) = y)p(\mathbf{x})d\mathbf{x}</script><h3 id="混淆矩阵与F1-score"><a href="#混淆矩阵与F1-score" class="headerlink" title="混淆矩阵与F1 score"></a>混淆矩阵与F1 score</h3><p>单单使用精度或者错误率不能满足所有任务需求。我们通常用查准率，查全率与 F-1 score 来更全面地描述学习器的性能。<br>对二分类任务，我们有真正例(TP)，假正例(FP)，真反例(TN)，假反例(FN)四种情况，如下即为相关的混淆矩阵：</p><div class="table-container"><table><thead><tr><th style="text-align:center">真实/预测</th><th style="text-align:center">正例</th><th style="text-align:center">反例</th></tr></thead><tbody><tr><td style="text-align:center">正例</td><td style="text-align:center">TP</td><td style="text-align:center">FN</td></tr><tr><td style="text-align:center">反例</td><td style="text-align:center">FP</td><td style="text-align:center">TN</td></tr></tbody></table></div><p>我们定义查准率 P 与查全率 R:</p><script type="math/tex; mode=display">P = \frac{TP}{TP+FP}\\R = \frac{TP}{TP+FN}</script><p>查准率-查全率是有一定矛盾的，希望查准率高，就会尽可能地减少数量，这就会导致查全率变少；提高查全率则反之。为了更全面地比较，我们可以画出P-R曲线图。一般曲线能够完全包住另一个学习器的情况，就说明该算法较优。另一方面，F-1 score 是另一种结合上述两种度量的性能度量</p><script type="math/tex; mode=display">F1 =\frac{2PR}{P + R}</script><p>对于多次训练/多个数据集的情况，我们通常对一次训练计算一次混淆矩阵，然后求全局的平均P, R 值，再计算全局的 F1 score。</p><h3 id="ROC-AUC"><a href="#ROC-AUC" class="headerlink" title="ROC, AUC"></a>ROC, AUC</h3><p>考虑这样一种分类器：其输出值是 0-1 之间的实数，我们设定一个分类阈值，比如 0.5，来决定输出的标签是正类还是反类。我们可以将样本按照输出值进行排序。如果不考虑阈值，这个实值的好坏决定了分类器的好坏：如果这个值分的更开，说明分类越好。我们可以直观的理解其下界：以均匀分布随机输出实值。在这个排序中，调整截断点的位置可以体现对 P 或者 R 的重视程度，而排序本身体现了期望泛化性能的好坏。</p><p>ROC 曲线就是一种应用了这种思想的曲线，纵轴是真正例率(TPR)，横轴是假正例率(FPR)。其计算如下：</p><script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN}\\FPR = \frac{FP}{TN+FP}</script><p>类似的，我们可以与P-R曲线一样判断 ROC 曲线下学习器的优劣：能够全包的曲线一般更优。在没有办法全包的情况，我们一般计算曲线包围的面积 AUC。由于我们一般在离散的样本上进行学习测试，其曲线也是曲折的，我们用一个个小矩形来近似 AUC:</p><script type="math/tex; mode=display">AUC = \frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1} - x_i)(y_i + y_{i+1})</script><h3 id="代价敏感错误率与代价曲线"><a href="#代价敏感错误率与代价曲线" class="headerlink" title="代价敏感错误率与代价曲线"></a>代价敏感错误率与代价曲线</h3><p>现实中不同类型的错误造成的后果不同：比如在医疗诊断中，将患者诊断为健康可能造成不可控的后果，但是将健康人诊断为患病的后果就相对比较小。为了权衡不同类型错误的损失，可以为错误赋予非均等代价。我们可以设定代价矩阵：</p><div class="table-container"><table><thead><tr><th>真实类别/预测类别</th><th>第0类</th><th>第1类</th></tr></thead><tbody><tr><td>第0类</td><td>0</td><td>c01</td></tr><tr><td>第1类</td><td>c10</td><td>0</td></tr></tbody></table></div><p>我们可以计算代价敏感错误率，令<script type="math/tex">D^+</script>, <script type="math/tex">D^-</script> 分别为正，反例子集：</p><script type="math/tex; mode=display">E(f;D; c) = \frac{1}{m} (\sum_{\mathbf{x}_i \in D^+}\mathbb{I}(f(\mathbf{x}_i \neq y_i))\times c_{01}\\+ \sum_{\mathbf{x}_j \in D^-}\mathbb{I}(f(\mathbf{x}_j \neq y_i)) \times c_{10})</script><p>在非均等代价下， ROC 曲线不能直接反映期望总体代价，我们使用代价曲线来取代。其横轴为<script type="math/tex">[0,1]</script>的正例概率代价</p><script type="math/tex; mode=display">P(+)cost = \frac{p\times c_{01}}{p\times c_{01} + (1-p)\times{c10}}</script><p>取<script type="math/tex">p</script>为正例概率。纵轴是取值为<script type="math/tex">[0,1]</script>的归一化代价：</p><script type="math/tex; mode=display">c_{norm} = \frac{FNR\times p \times c_{01} + FPR\times(1-p) \times c_{10}}{p\times c_{01} + (1-p)\times c{10}}</script><p>在ROC曲线上，每一点的坐标是(TPR, FPR)。我们可以计算出假反例率 FNR=1-TPR，从而可以在代价敏感图中画出一条从(0, FNR) 到 (1, FPR) 的线段，所有线段的下界包起来的面积即为期望总体代价。</p><h2 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h2><p>我们通常用测试集上的某些性能度量来衡量学习器的性能，但是：</p><ul><li>泛化性能跟测试集上的性能未必相同</li><li>测试集的性能跟测试集自身的选择密切相关</li><li>算法本身的随机性导致测试集上的性能未必相同</li></ul><p>基于这些问题，我们使用统计假设检验(hypothesis test)帮助我们比较学习器的性能。假设检验能让我们推断出，如果测试集上观察到某个学习器A比学习器B优，那么 A 泛化性能是否比 B 好，以及该结论把握多大。根据<a href="https://www.zhihu.com/question/287895170" target="_blank" rel="noopener">知乎</a>的解释，西瓜书这里的公式2.27有误，正确的推理如下：</p><p>令泛化错误率为<script type="math/tex">\epsilon</script>, 测试错误率为<script type="math/tex">\hat \epsilon</script>。在测试集上，恰有<script type="math/tex">\hat\epsilon\times m</script>个样本被误分类。根据泛化错误恰好将测试集上错误分类的概率为：</p><script type="math/tex; mode=display">P(\hat\epsilon;\epsilon) =                             \begin{pmatrix}m\\                            \hat\epsilon \times m\\                            \end{pmatrix}                            \epsilon^{\hat\epsilon\times m}(1-\epsilon)^{m-\hat\epsilon\times m}</script><p>当测试错误率与泛化错误率相等时，<script type="math/tex">P</script>最大。考虑假设<script type="math/tex">\epsilon < \epsilon_0</script>（泛化错误率小于某个阈值）。<br>原假设<script type="math/tex">H_0: \epsilon \leq \epsilon_0</script>，备择假设<script type="math/tex">H_1: \epsilon>\epsilon_0</script>。</p><p>若原假设成立，那么采样测试错误率小于某个临界值<script type="math/tex">\hat\epsilon \leq \overline \epsilon</script>。从条件概率的角度理解：<script type="math/tex">P(\hat\epsilon \leq \overline \epsilon |H_0) >= 1-\alpha</script>。等价于<script type="math/tex">P(\hat\epsilon \gt \overline \epsilon | H_0) < \alpha</script>。 其中，<script type="math/tex">1-\alpha</script>为置信度，意思即为原假设成立的概率。</p><p>我们需要计算这个观测临界值<script type="math/tex">\overline\epsilon</script>。考虑西瓜书图2.6类似的概率分布，误分类样本为右侧的阴影表示。这个概率分布表示所有可能的误分类数发生的概率。想象在概率分布图上画一根竖线分割概率密度函数曲线为两个部分，右侧为小概率事件发生，需要拒绝原假设（拒绝域）；左侧为大概率事件发生，接受原假设（接受域）。这条线<script type="math/tex">0 < l < 1</script>是接受域的上限，拒绝域的下限。我们需要拒绝域的面积最大为<script type="math/tex">\alpha</script>，则我们需要寻找一个最小的<script type="math/tex">l</script>，使得右边阴影面积最大为<script type="math/tex">\alpha</script></p><script type="math/tex; mode=display">\overline\epsilon = \min \epsilon~s.t. \sum_{i=\epsilon\times m+1}^{m} \begin{pmatrix}m\\                                         i \\                                         \end{pmatrix}\epsilon_0^i(i-\epsilon_0)^{m-i} < \alpha</script><p>这个<script type="math/tex">\overline\epsilon</script>就是我们的接受域上界。</p><h3 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h3><p>我们对使用交叉验证或者多次重复留出法可以得到多个测试错误率，此时可以使用 t 检验。首先计算平均错误率与方差：</p><script type="math/tex; mode=display">\mu = \frac{1}{k} \sum_{i=1}^k \hat\epsilon_i\\\sigma^2 = \frac{1}{k-1} \sum_{i=1}^k(\hat\epsilon_i - \mu)^2</script><p>我们将其看做是对泛化错误率的独立采样:</p><script type="math/tex; mode=display">\tau_t = \frac{\sqrt{k}(\mu - \epsilon_0)}{\sigma}</script><p>这个统计量服从于自由度为 <script type="math/tex">t-1</script> 的 t 分布。类似的，左右两边的阴影大小分别应该为 <script type="math/tex">\frac{\alpha}{2}</script>。如果平均错误率与 <script type="math/tex">\epsilon_0</script>之差在分布的中间，则可以接受原假设。</p><h3 id="交叉验证t检验"><a href="#交叉验证t检验" class="headerlink" title="交叉验证t检验"></a>交叉验证t检验</h3><p>我们对 A, B 两个学习器分别求 k-fold 交叉验证，得到两组错误率；同时，我们又可以得到两组错误率之差。因此，我们可以对这两组错误率之差做 t 检验。计算差值的平均值与方差，令变量<script type="math/tex">\tau_t = |\frac{\sqrt{k}\mu}{\sigma}|</script>。我们将原假设设为两个学习器的性能没有区别，用类似的方法来处理这个变量，以判断是否接受原假设。</p><h3 id="McNemar-检验"><a href="#McNemar-检验" class="headerlink" title="McNemar 检验"></a>McNemar 检验</h3><p>对于二分类问题，我们有两个学习器A，B，不仅可以计算其错误率，也可以计算其分类结果的差别：</p><div class="table-container"><table><thead><tr><th>B/A</th><th>正确</th><th>错误</th></tr></thead><tbody><tr><td>正确</td><td>e00</td><td>e01</td></tr><tr><td>错误</td><td>e10</td><td>e11</td></tr></tbody></table></div><p>如果我们假设两学习器性能相同，则e01 = e10，那么 |e01 - e10| 应服从正态分布，均值为1，方差为 e01+e10。令变量</p><script type="math/tex; mode=display">\tau_{\chi^2} = \frac{(|e_{01} - e_{10}|-1)^2}{e_{01} + e_{10}}</script><p>服从于自由度为1的卡方分布。</p><h3 id="Friedman-检验-Nemenyi检验"><a href="#Friedman-检验-Nemenyi检验" class="headerlink" title="Friedman 检验 Nemenyi检验"></a>Friedman 检验 Nemenyi检验</h3><p>见书，感觉用的不是很多，要用的时候查一下吧。<br>F检验大致的思想：对各个算法在不同数据集上的性能进行排序，如果他们性能相同，那么他们的平均序值应当相同。<br>如果其性能不相同，那么我们可以用 Nemenyi 后续检验来进一步区分各个算法：给出一个临界值，如果两个算法平均序值之差超过临界值，则拒绝两个算法性能相同这一假设。</p><h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><p>偏差-方差分解是一个比较重要的工具。给定数据集 D 学习器 f, 对测试样本 x，令 yd 为数据集中的标记，y 为真实标记。<br>对于回归任务，学习算法的期望预测：</p><script type="math/tex; mode=display">\hat f(\mathbf{x}) = \mathbb{E}_D[f(\mathbf{x}; D)]</script><p>样本相同，不同的训练集产生的方差为：</p><script type="math/tex; mode=display">var(\mathbf{x}) = \mathbb{E}_D[(f(\mathbf{x};D)-\hat f(\mathbf{x}))^2]</script><p>噪声为</p><script type="math/tex; mode=display">\epsilon^2 = \mathbb{E}_D[(y_D - y)^2]</script><p>期望输出与真实标记之间的差别称为偏差(bias)：</p><script type="math/tex; mode=display">bias^2(\mathbf{x}) = (\hat f(\mathbf{x}) - y)^2</script><p>推导过程见书，从期望泛化误差开始推导。我们证明泛化误差可分解为偏差，方差与噪声之和。<br>偏差度量学习器输出与真实的偏离程度，即算法本身的拟合能力；方差度量同样大小的训练集变动所导致的学习性能变化，即数据扰动的影响；噪声定义了期望泛化误差的下界。这意味着泛化性能由学习器本身的学习能力，数据的充分性与学习任务本身的难度共同决定。</p><p>一般而言方差与偏差是有冲突的，训练不足，学习器拟合能力不强，偏差主导对泛化性能的影响；训练充分，学习能力足够强，训练数据的扰动被学习器习得，从而方差的影响程度逐步加深；当训练过拟合，数据的微小扰动都会导致方差的巨大变化。</p><h2 id="习题-1"><a href="#习题-1" class="headerlink" title="习题"></a>习题</h2><blockquote><p>1000个样本, 500正例500反例。划分为包含70%的训练集与30%的测试集用于留出法评估，估算划分方式</p></blockquote><p>没有额外信息，我们保证正例跟反例在训练集跟测试集里的比重相等。</p><script type="math/tex; mode=display">C_{500}^{350} \times C_{500}^{350}</script><blockquote><p>100个样本，正，反例各一半，假定学习算法的新模型是将新样本预测为训练样本较多的类。给出用10折交叉验证与留一法分别对错误率进行评估所得的结果</p></blockquote><p>10折：理想情况下正，反例在整个数据集与每个fold的分布都是均匀的，错误率期望是 50%<br>留一法：每次取一个样本，剩下的样本会有49个同类，50个相反，错误率是100%</p><blockquote><p>若学习器 A 的 F1 值比 B 高，求 A 的 BEP 值是否也更高</p></blockquote><p>不一定，感觉这两个没有明确的关系，见<a href="https://zhuanlan.zhihu.com/p/42435889" target="_blank" rel="noopener">知乎</a></p><blockquote><p>真正例率，假正例率，查准率，查全率之间的关系</p></blockquote><script type="math/tex; mode=display">TPR = \frac{TP}{TP + FN}\\FPR = \frac{FP}{FP+TN} \\P = \frac{TP}{TP+FP} \\R = \frac{TP}{TP+FN} \\</script><p>显然，TPR = R</p><blockquote><p>证明式 2.22</p></blockquote><p>数学玩法参考<a href="https://zhuanlan.zhihu.com/p/42435889" target="_blank" rel="noopener">这里</a></p><p>我们要证明的是：</p><script type="math/tex; mode=display">\mathcal{l}_{rank} = \frac{1}{m^+m^-} \sum_{\mathbf{x}^+\in D^+}\sum{\mathbf{x}^-\in D^-} (\mathbb{I}(f(\mathbf{x}^+)< f(\mathbf{x}^-))+ \frac{1}{2}\mathbb{I}(f(\mathbf{x}^+) = f(\mathbf{x}^-))) \\ AUC = 1 - \mathcal{l}_{rank}</script><p><a href="https://blog.csdn.net/icefire_tyh/article/details/52065867" target="_blank" rel="noopener">这里</a>有比较详细的说明。</p><!-- 借用一下 ROC 曲线的图片： --><p><img src="/images/roc-ml.jpg" alt="ROC_curve"></p><p>如图，横轴纵轴分别被分割为<script type="math/tex">1/m^-</script>，<script type="math/tex">1/m^+</script>的小条，划分出每个小块面积为<script type="math/tex">1/m^-1/m^+</script>都代表一对样本对(a, b)。当<script type="math/tex">f(a) > f(b)</script>且a为正例，b为反例的时候，这个方块在ROC曲线下方；当两者相等，则用一个占一半面积的三角形替代。于是损失函数等同于在上方的方块数+一半的边界三角形数乘以单位面积，即为ROC曲线上方的面积。</p><blockquote><p>ROC曲线与错误率的关系</p></blockquote><p>ROC曲线上每一点都对应一个错误率。<br>不考虑代价，我们有：</p><script type="math/tex; mode=display">E = \frac{FP+FN}{TP+FP+TN+FN}\\</script><p>TPR, FPR见上。我们有</p><script type="math/tex; mode=display">FP = FPR \times (FP+TN)\\FN = (1-TPR)\times(TP+FN)</script><p>另外注意到FP+TN, TP+FN 分别为正例，反例的数量，所以我们可以得到</p><script type="math/tex; mode=display">E = (FPR \times m^- + (1-TPR)\times m^+)/ (m^+ + m^-)</script><blockquote><p>试证明任意一条ROC曲线都有一条代价曲线与之对应，反之亦然。</p></blockquote><p>代价曲线是代价敏感图的下界，由于每一条代价线都是(0, FNR)到(1, FPR)的线段，而TPR=1-FNR，可以得出每一条代价线都能转换成(TPR, FNR)的形式，即ROC线上一点。由于代价曲线是这些线段族下确界组成的多边形，每个代价曲线都可以写成若干在ROC曲线上的点。反之，则可根据ROC曲线上的点做线段，取下界得到代价曲线。</p><blockquote><p>min-max规范与z-score规范化的优缺点</p></blockquote><p>min-max适用于规范化最大，最小值已知的情况，缺点是对极端数据敏感，出现极端数据需要取舍<br>z-score适用于未知的情况，或者有极端数据。另外数据归零化。缺点是计算量大，出现新数据要重新计算。</p><blockquote><p>卡方分布检验过程，Friedman检验</p></blockquote><p>略，不懂统计……</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开始认真刷西瓜书了，作为面试的准备。规划是重点公式+有价值的练习题推一遍或者编程实现，由于同时也要刷 leetcode 与找内推所以更新不定。本文会包括 1-2 章的内容以及习题。&lt;/p&gt;
&lt;p&gt;一些碎碎念：之前跟朋友聊的时候说到在瓶颈期要保持产出——不论是知识还是产品，那么现在姑且就把知识整理一下吧，虽然做的有点晚，但是总比不做好。博客改成hexo以后好多以前的文章暂时没空搬过来，可能要留到以后整理了，因为好多图放在七牛但是七牛现在对未备案域名有限制所以全挂了，怎么处理得慢慢来了。&lt;/p&gt;
&lt;p&gt;迁移到hexo的时候也有好多坑，可能另外有空说一下，包括但不限于 Mathjax 引入，代码高亮，分享……（想在leancloud注册账号，要人脸认证但是我手机不给认证……）等。而且实际上主题没写完，手机端的样式没仔细调，一些页面的样式没写，等等。但是毕竟是从头写起的一个主题，拖延了好久的想法终于实现80%，感觉还是不错的。&lt;br&gt;
    
    </summary>
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/categories/ml/"/>
    
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
      <category term="西瓜书" scheme="http://MeowAlienOwO.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
      <category term="statistic" scheme="http://MeowAlienOwO.github.io/tags/statistic/"/>
    
  </entry>
  
  <entry>
    <title>强化学习导论（三）</title>
    <link href="http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-3/"/>
    <id>http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-3/</id>
    <published>2019-10-31T09:09:06.000Z</published>
    <updated>2019-10-31T09:11:23.227Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习导论第三部分，planning, value function approximation, eligibility traces, policy gradient methods</p><a id="more"></a><h1 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h1><p>之前我们涉及到的强化学习方法中，DP是在$p$函数已知的情况下，其余的MC，TD都是通过统计方法来近似价值函数的期望，不需要一个先验的模型(model-free)。但是，在<br>许多情况下，我们有很多对模型的先验知识，如果抛弃这些先验知识可能会使得训练时间过长从而不实际。我们通过引入一个包含先验知识的模型来解决这个问题。</p><p>我们定义<code>计划</code>(planning)为使用模型，进行一系列动作规划的过程，根据这个定义，动态规划就是一种计划，因为其使用了$p$函数来描述环境模型。</p><p>对于模型而言有两种主要的分类：</p><ol><li>分布模型(distributional model): 解析地表示出给定的分布函数$p$</li><li>模拟模型(simulation model): 生成一系列从模型的分布中采样的数据</li></ol><p>实际而言，很多问题的分布模型很复杂或者无法用解析方法表示，在这种情况下，我们选用模拟模型。</p><h2 id="Dyna-Q"><a href="#Dyna-Q" class="headerlink" title="Dyna-Q"></a>Dyna-Q</h2><p>Dyna-Q 算法是一种将planning 与 learning结合起来的算法。其具体流程如下：</p><pre><code>Init  Q(s, a), Model(s, a) for s in S, a in ARepeat forever:  S := current state  A := e-greedy(S, Q)  Execute action A, observe R, S&#39;  # this step is direct RL:  Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S&#39;,a)) - Q(S, A)]  # learning model  Model(S, A).update(R, S&#39;)   # use model planning  Repeat n times:    S := random previously observed state    A := random action previously taken in S    R, S&#39; := Model(S, A)    Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S&#39;, A)) - Q(S, A)]</code></pre><p>Dyna-Q 算法的关键步骤大致分为三步：第一步，正常的通过RL方法更新动作价值函数$Q$；第二步，根据历史的经验，更新模型，这里单纯地记忆当前的模型的下一步的反馈；第三步，将过去的经验中，已经经历过的(s,a)对提取出来，使用模型计算该(s, a)对的下一状态与奖励，然后更新$Q$函数。</p><p>这里，我们重复一下模型，价值函数，策略之间的关系:</p><ol><li>通过价值函数与策略我们可以生成当前需要进行的动作</li><li>在环境中执行动作(action)我们可以得到经验(experience)</li><li>我们可以通过经验来优化我们的价值函数，或者策略</li><li>经验同样地可以被用来进行模型的学习(model learning)</li><li>根据学习后的模型，通过计划，我们同样可以得到价值函数或者策略</li></ol><p><img src="/images/3-planning-learning-relation.png" alt></p><p>Dyna-Q 相比普通的Q-learning，其提升在于: Q-learning一开始只会对经验过的policy进行更新；但是Dyna-Q利用的planning,可以继续生成一系列模拟的经验，从而有着更好的更新覆盖率。如果用机器人走迷宫的例子来说，第一个episode，只有终点的奖励，Q-learning只会更新终点前一格的价值；但是另一方面，通过planning，我们可以生成一系列的(s, a)对，从而对前面若干步的格子的价值函数进行更新。</p><p>另一方面，模型有可能是错的：环境可能改变，先验的知识也不一定符合现状。环境变化有两种可能性：一种是，环境变“坏”了，即我们当前的策略在新的环境下没有办法获取或者只能获取低水平的奖励；第二种是，环境变“好”了，我们的解依然能够获取同样水平的奖励，但是存在一个更优的解。在环境变坏的情况下，通过planning我们一般不能得到最优的policy，但是通过不断的exploit，模型能够发现这个这个解不是最优，从而慢慢找到更新的解。但是在环境变好的情况下，模型很难去发现更优的解法，而是会陷入在原有的策略中不断exploit。为了解决这个问题，我们需要平衡探索与exploit。</p><p>Dyna-Q+是一个变种的Dyna-Q算法。在这个算法中，我们用$R + k\sqrt{\tau}$来代替奖励信号，其中$\tau$是一个衡量上一次访问该状态距今的时间，$k$是控制平衡的系数。这种做法能够赋予那些长时间未访问的状态更高权重，从而让模型能够更新那些很久都没有访问过的状态的价值。</p><p>另一种Dyna-Q的变种Prioritized Sweeping Dyna-Q考虑从模型中选取状态的随机性问题。随机性容易产生很多无效的(s, a)对，在遍历这些状态的时候，我们的价值实际上不更新。换而言之，虽然planning扩展了更新价值的范围，但是这些扩展的范围仍旧局限在终点附近。对于每一步，我们将[Target-OldEstimate]这一项作为优先级，将我们的历史经验存储在优先级队列之中。选取模型的时候，我们从优先级队列中选取，并且将新生成的(s, a)对重新按照优先级放入优先级队列中。优先级系数有一个截断阈值$\theta$来控制。这种算法有着更优的模型收敛速度。</p><h2 id="Rollout-Planning"><a href="#Rollout-Planning" class="headerlink" title="Rollout Planning"></a>Rollout Planning</h2><p>Dyna-Q的模型重用了历史经验，而rollout planning使用模型来模拟将来的轨迹(trajectory)。每个轨迹从当前状态出发来</p><p>我们给出前向更新的rollout算法：</p><pre><code>Input   Model for simulationInit Q(s, a) for all s, afor t = (0, ...) do:  S_t := current state  for n times(n rollouts) do    S := S_t    while s non-terminal/within fixed length do:      action A based on Q(S,:), with exploration # i.e. e-greedy      (R, S&#39;) sample from Model(S, A)      Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S&#39;, a)) - Q(S, A)]      S := S&#39;  Select action A_t greedly from Q(S_t, :)</code></pre><p>这种算法下，如果模型是正确的且满足每个状态访问无限次，该算法能够学习到最优策略，反之如果模型不正确，则无法得到最优解。</p><p>我们给出反向更新的rollout算法，这个算法的目的是更好的利用奖励</p><pre><code>Input   Model for simulationInit   Q(s, a) for all s, a  trace = [], stackfor t = (0, ...) do:  S_t := current state  for n times(n rollouts) do    S := S_t    # rollout    while s non-terminal/within fixed length do:      action A based on Q(S,:), with exploration # i.e. e-greedy      (R, S&#39;) sample from Model(S, A)      trace.push(S,A,R,S&#39;)      S := S&#39;    # backprop    while trace != [] do:      (S,A,R,S&#39;) := trace.pop()      Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S&#39;, a)) - Q(S, A)]  Select action A_t greedly from Q(S_t, :)</code></pre><p>在走迷宫问题上，前向传播可以看做是这一次的奖励更新奖励点前最后一个状态与上一次的奖励更新前面所有状态的复合，而反向传播会直接用当前的奖励更新所有的状态。在这个意义上，反向传播有着较正向传播稍快的学习速度。</p><h2 id="Monte-Carlo-Tree-Search-蒙特卡洛树搜索"><a href="#Monte-Carlo-Tree-Search-蒙特卡洛树搜索" class="headerlink" title="Monte-Carlo Tree Search 蒙特卡洛树搜索"></a>Monte-Carlo Tree Search 蒙特卡洛树搜索</h2><p>蒙特卡洛树搜索(MCTS)是一种更加通用，高效的rollout planner。<br>对于每一个状态，MCTS都被用于动作的选取。每次执行MCTS都是一个迭代过程，这个迭代过程模拟从当前状态触发到终止状态的轨迹/衰减率限制。<br>MCTS的核心是通过不断拓展当前状态的一部分在早期模拟中获得高回报(high evaluation)的轨迹，来集中在多个模拟上(原文比较难懂)<br>MCTS不保存从一个动作选取到下一个的近似价值函数或者策略。</p><p>自己的理解：MCTS存储一个部分的(partial)$Q$函数，每次MCTS会从最可能的那一个节点进行扩展。</p><p><img src="/images/MCTS.jpg" alt></p><p>MCTS根据模拟的输出结果构造搜索树，由以下四个主要步骤组成：</p><ol><li>选择(selection): 从根节点开始，递归的选择最优的子节点直到叶节点(不同于终止状态)</li><li>扩展(expansion): 如果选择的节点不是终止状态，则对这个节点进行扩展，创建这个节点的一个或者多个子节点</li><li>模拟(simulation): 选取上步创建的一个子节点，从子节点开始使用默认的策略(default policy)来模拟，直到终止状态</li><li>反向传播(back propagation):用模拟的输出结果，更新从根节点到扩展的子节点的价值</li></ol><p>一般的MCTS算法如下：</p><pre><code>Init  S_0: init state   DefaultPolicy: default policy   Q: {v_0: S_0} MCTS tree with root node S_0S := S_0Repeat :  v := node in Q s.t. state(v) = S  while computationally possible:    v_t := TreePolicy(v_0) # best polict by tree    delta := DefaultPolicy(state(v_t))    Backprop(v_t, delta)  action A is BestChild(v_0)</code></pre><p>在模拟这一部分，我们使用的是一个不同于Tree policy的策略。这个策略可以是最简单地随机策略，也可以使用一些启发式搜索或者多重模拟的平均。</p><h3 id="Upper-Confidence-Bounds-for-Trees-UCT-上确界树"><a href="#Upper-Confidence-Bounds-for-Trees-UCT-上确界树" class="headerlink" title="Upper Confidence Bounds for Trees(UCT) 上确界树"></a>Upper Confidence Bounds for Trees(UCT) 上确界树</h3><p>如果我们将选择步骤的每一层都看做多臂赌博机问题，我们可以选用不同的方法来平衡探索与收益。<br>UCT使用UCB来进行每一个子节点的选择。在这里，UCB公式可以写成如下形式:</p><script type="math/tex; mode=display">v_i + C \sqrt{\frac{\log N}{n_i}}</script><p>其中$v_i$是节点的估计价值，$n_i$是节点被访问的次数，$N$是父节点被访问的次数，$C$是平衡系数。</p><h2 id="在线-离线学习"><a href="#在线-离线学习" class="headerlink" title="在线/离线学习"></a>在线/离线学习</h2><p>RL同样有在线学习与离线学习两种。</p><p>在线学习:</p><ol><li>在真正的游戏执行之前，用MDP找到最优策略</li><li>策略是完备的：对所有可能的状态,都能够找到最优策略</li><li>Use as much time as needed to find policy<br>Dyna-Q, DP<br>离线学习:</li><li>在游戏中使用MDP寻找最优策略</li><li>策略是不完备的：仅对当前的状态寻找最优策略</li><li>有限的时间（e.g.下棋读秒）</li></ol><h1 id="Value-Function-Approximation"><a href="#Value-Function-Approximation" class="headerlink" title="Value Function Approximation"></a>Value Function Approximation</h1><p>对于非常巨大的状态空间而言，直接存储价值表是不现实的，我们自然就会想到使用一个函数$\hat{v}<em>\pi$来近似$v</em>\pi$，这样我们可以在有限的内存下仍然能够处理巨大的状态空间。<br>另一个问题是，相比较于状态空间，能获取的样本只是其中很小的一个部分，因此遍历所有的状态是不可能的。由这两个问题，我们引出近似价值函数。</p><p>我们的目标是，使用参数化的函数来近似价值表:</p><script type="math/tex; mode=display">\hat{v}(s, \mathbf{w}) \simeq v_\pi(s)\hat{q}{s, a, \mathbf{w}} \simeq q_\pi(s, a)</script><p>这带来两个好处：</p><ol><li>参数的数量一般而言远小于状态空间的大小</li><li>泛化能力，一个参数的变化可以对应若干对应的状态/动作价值的变化</li></ol><p>价值函数的学习一般而言通过有监督学习，我们使用一个状态-价值对来训练我们的函数：(S, U)</p><ol><li>MC: U = G</li><li>TD(0): U = R + gamma * v(S’, wt)</li><li>n-step TD: U = R + … + gamma^n-1 Rn + gamma^n v(Sn, wn-1)</li></ol><p>这种方法一般而言能够进行增量更新，以及能够处理噪音。</p><p>我们一般采取均方误差来作为我们的损失函数。</p><p>在损失函数可导的情况下，我们采用随机梯度下降来增量学习参数$w$</p><script type="math/tex; mode=display">\mathbf{w}_{t+1} = \mathbf{w}_t - \frac{1}{2}\alpha \nabla J(\mathbf{w}_t) = \mathbf{w}_t + \alpha [U_t - \hat{v}(S_t, \mathbf{w}_t) \nabla\hat{v}(S_t, ]\mathbf{w}_t)]</script><p>对于MC而言，$\mathbb{E}[U|S]$是一个对$v_\pi$的无偏估计，我们可以放心使用；但是对于TD而言，由于其为bootstrap方法，$U$是一个有偏估计。<br>在这种情况下，我们使用半梯度(semi-gradient)。之所以叫半梯度是因为$U=R+\gamma\hat{v}(S,\mathbb{w})$跟$w$有关：</p><pre><code>Input:  pi: policy   v_hat: differenciable function , v_hat(terminal, :) = 0  alpha: step size   w: weights, arbitrarily initloop for each episode:  init S  for each step in episode:    A chosen from pi(S)    take action A, observe R, S&#39;    w := w + alpha[R+gamma v_hat(S&#39;, w) - v_hat(S, w)] d(v_hat(S,w))/dw    S := S&#39;  until S terminal</code></pre><p>在线性的情况下，只有一个最优：</p><ul><li>MC收敛至全局最优</li><li>TD收敛至接近全局最优的不动点</li></ul><p>我们通过coarse/tile coding 来抽取特征：<br>粗编码(coarse)将整个状态空间划分为重叠的若干子空间，每一个空间内的点点亮当前的子空间，并且同时点亮同该空间互相重叠的子空间—邻域，<br>在粗编码的情况下，我们可以仅仅对在状态空间内相邻的状态值进行更新。<br>片编码(tile)对状态点附近的一个方形区域进行切割，然后将该区域上下左右平移得到新的领域。</p><h2 id="控制问题"><a href="#控制问题" class="headerlink" title="控制问题"></a>控制问题</h2><p>同状态价值相似，我们可以直接写出动作价值的梯度表示，从而实现控制。在线性函数下，我们有:</p><ol><li>SARSA: U = R + gamma q(S, A, w)</li><li>Q-learning: U = R + gamma max_a q(S, a, w)</li><li>Expected SARSA: U = R + gamma sum_a \pi(a|S) q(S, a, w)</li></ol><h1 id="Eligibility-Trace-资格迹"><a href="#Eligibility-Trace-资格迹" class="headerlink" title="Eligibility Trace 资格迹"></a>Eligibility Trace 资格迹</h1><p>资格迹可以看做是另外一种MC/TD的插值方式。<br>在n-step中，我们逐步计算n步的奖励，之后的奖励用现在的v值近似。<br>n-step 存在的问题是：更新需要等待n步，有时太长了。<br>我们可以换一种思路：对不同的n的G值做加权平均，可以保证其价值函数的收敛性。</p><h2 id="TD-lambda"><a href="#TD-lambda" class="headerlink" title="TD(lambda)"></a>TD(lambda)</h2><p>TD(lambda)的基本思路是：对从当前时刻开始的所有G进行加权求和，使用一个指数权重来控制从当前时刻开始每一个时刻$t+k$的期望，我们称这个期望为lambda-return。</p><script type="math/tex; mode=display">G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}</script><p>这个公式可以写成如下形式，以方便我们观察Termination：</p><script type="math/tex; mode=display">G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_t</script><p>我们可以认为这个式子截断了n步以后的情形，以后项来代替截断后的期望。可以看出，lambda项等于0的时候相当于TD(0), 等于1的时候相当于MC方法。<br>线下的lambda-return算法的更新公式为：$\alpha[G_t^\lambda-\hat{v}(S_t,\mathbb{w_t})]\nabla\hat{v}(S_t,\mathbb{w}_t)$</p><h3 id="前向与后向解释"><a href="#前向与后向解释" class="headerlink" title="前向与后向解释"></a>前向与后向解释</h3><p>TD(lambda)的前向视角就是lambda-return的视角：向前看若干步，然后进行lambda加权平均。</p><blockquote><p>对于每个访问到的state，我们都是从它开始向前看所有的未来reward，并决定如何结合这些reward来更新当前的state。每次我们更新完当前state，我们就到下一个state，永不再回头关心前面的state</p></blockquote><p>我们来更形象地表述一下后向视角的过程：每次在当前访问的状态得到一个误差量的时候，这个误差量都会根据之前每个状态的资格迹来分配当前误差。这就像是一个小人，拿着当前的误差，然后对准前面的状态们按比例扔回去。<br>TD(λ)的后向视角非常有意义，因为它在概念上和计算上都是可行而且简单的。具体来说，前向视角只提供了一个非常好但却无法直接实现的思路，因为它在每一个timestep都需要用到很多步之后的信息，这在工程上很不高效。而后向视角恰恰解决了这个问题，采用一种带有明确因果性的递增机制来实现TD(λ)，最终的效果是在on-line case和前向视角近似，在off-line case和前向视角精确一致。</p><h2 id="资格迹"><a href="#资格迹" class="headerlink" title="资格迹"></a>资格迹</h2><p>引入一个同每个状态都相关的变量$z$，在每一个episode的时候初始化为0，以$\gamma\lambda$为衰减率，对每一个时刻的梯度进行加权:</p><script type="math/tex; mode=display">\mathbb{z}_{-1}=0</script><script type="math/tex; mode=display">\mathbb{z}_t = \gamma\lambda\mathbb{z}_{t-1} + \nabla \hat{v}(S_t, \mathbb{w}_t)</script><p>资格迹可以看做对权重更新的平滑：当参数更新时，其与历史累计的更新幅度加载一起；当没有激活参数的更新时，之前的更新会慢慢回落至0。<br>从另一种角度看，当资格迹不作用于参数化函数时，可以看做一个梯度恒为定值的函数。</p><p>根据前述的半梯度TD，我们引入Semi-Gradient TD(lambda)算法：</p><pre><code>for each episode:  Init S  z := 0  for each step in episode:    A chosen by pi    take action A, observe R, S&#39;    z := gamma * lambda * z + dv(S,w)/dw    delta := R + gamma * v(S&#39;, w) - v(S, w)    w := w + alpha * z *delta    S := S&#39;  until S terminal</code></pre><p>其中， lambda称为衰减率。</p><h3 id="Online算法"><a href="#Online算法" class="headerlink" title="Online算法"></a>Online算法</h3><p>在线的lambda-return算法实际上使用截断的lambda-return：</p><script type="math/tex; mode=display">G_{t:h}^\lambda = (1-\lambda)\sum_{n=1}^{h-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{h-t-1}G_{t:h}</script><p>这个算法可以看做是长度为定值的lambda-return算法，我们将整个轨迹在h处截断，后面的概率使用截断后一个时间点的期望$\lambda^{h-1}G_{t:h}$来替代</p><h1 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h1><p>同Value Approximation 类似，Policy Gradient 将策略参数化，从而学习到更优的策略。</p><script type="math/tex; mode=display">\pi(a|s, \theta) = Pr{A_t = a| S_t = s, \theta_t = \theta}</script><p>Policy Gradient的优点:</p><ol><li>更容易收敛到局部最优</li><li>在高维与连续的动作空间有更好的表现</li><li>可以学习到随机策略</li></ol><p>对于离散的动作空间，我们通常使用softmax来描述动作的概率；而对于连续的动作空间，可以使用高斯分布来描述。</p><p>我们优化的目标可以是初状态的价值期望：</p><script type="math/tex; mode=display">J(\theta) \circeq v_\pi_\theta(S_0)</script><p>或者，如果是连续性的任务，可以优化平均奖励：</p><script type="math/tex; mode=display">J(\theta) \circeq \sum_s P_\pi(s) \sum_{a}\pi(a|s, \theta)\sum_{s', r}p(s',r|s,a)r</script><h2 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h2><p>对于任意的可微策略$\pi$，策略梯度为</p><script type="math/tex; mode=display">\nabla J(\theta) = \sum_s d_\pi(s) \sum_a q_\pi(s, a) \nabla \pi(a|s, \theta)</script><p>其中，$d_\pi(s)$是在策略$\pi$下的on-policy分布。</p><ul><li>对于初始状态而言我们有：$d<em>\pi(s)=\sum</em>{t=0}^{\infty}\gamma^t Pr{S_t=s|s_0, \pi}$</li><li>对于平均回报我们有：$d<em>\pi(s) = \lim</em>{t\rightarrow \infty} Pr{S_t = s |\pi}$</li></ul><p>我们在这里不需要推导环境的动态函数$p$。</p><script type="math/tex; mode=display">\begin{aligned}\nabla J(\theta) &= \sum_s d_\pi(s) \sum_a q_\pi(s, a) \nabla \pi(a|s, \theta) \\                 &= \mathbb{E}_\pi[\sum_a q_\pi(s, a) \nabla \pi(a|s, \theta)] \\                  &= \mathbb{E}_\pi[\sum_a \pi(a|s, \theta) q_\pi(s, a) \frac{\nabla \pi(a|s, \theta)}{\pi(a|s, \theta)} ]_\\                 &= \mathbb{E}_\pi[q_\pi(S_t, A_t)\nabla \ln(\pi(A_t|S_t, \theta))]\end{aligned}</script><p>注意此处对不同的时刻取期望值，我们的更新迭代公式可以写成：</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} + \alpha(q_\pi(S_t, A_t)\nabla \ln\pi(A_t|S_t, \theta))</script><p>在这里，我们有两个任务要完成：</p><ol><li>计算或者近似策略函数的导数</li><li>近似策略$q$</li></ol><p>为了避免选择无法反应状态之间的量级差异，我们有时会将其与baseline进行比较，在这里我们有：</p><script type="math/tex; mode=display">(q_\pi(S_t, A_t) - b(S_t))\nabla \ln \pi(A_t|S_t, \theta)</script><p>这个baseline 不会改变期望，但是可以降低方差。</p><p>我们给出基于策略梯度定理与baseline的蒙特卡洛更新算法:</p><pre><code>Var:  pi(a|s, theta): differenciable policy  v(s, w): differenciable state-value function  a_t, a_w: step size parameters, greater than 0init policy parameter and state-value weights randomlyfor each episode(forever):  generate episode by pi  for each step in episode:    G := sum_(k=t+1:T)(gamma^k R_k)    delta = G - v(S_t, w)    w := w + a_w * delta * dv(S_t, w)/dw    theta := theta + a_t * gamma^t * delta * d(ln(pi(A_t|S_t, theta)))/dtheta</code></pre><h2 id="Actor-Critic-Methods"><a href="#Actor-Critic-Methods" class="headerlink" title="Actor-Critic Methods"></a>Actor-Critic Methods</h2><p>上述基本算法由于采用了蒙特卡洛法，同样会有蒙特卡洛法的问题：直到一个episode完了才能进行更新，导致学习很慢。<br>这里，我们使用同样的将蒙特卡洛换成TD的思路，给出Actor-Critic 方法。</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} + \alpha [R_t + \gamma v(S_{t+1}, w) - v(S_t, w)]\nabla \ln\pi(A_t|S_t, \theta)</script><p>具体算法如下：</p><pre><code>Var:  pi(a|s, theta): differenciable policy  v(s, w): differenciable state-value function  a_t, a_w: step size parameters, greater than 0init policy parameter and state-value weights randomlyfor each episode:  Init S  I := 1  for each step in episode:    A from pi    take action A, observe S&#39;, R    delta := R + gamma * v(S&#39;, w) - v(S, w)    w := w + a_w * delta * dv(S, w)/dw    theta := theta + alpha * delta * I * d(ln(pi(A|S, theta)))/dtheta    I := gamma I    S := S&#39;  until S terminate</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;强化学习导论第三部分，planning, value function approximation, eligibility traces, policy gradient methods&lt;/p&gt;
    
    </summary>
    
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
      <category term="rl" scheme="http://MeowAlienOwO.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习导论（二）</title>
    <link href="http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-2/"/>
    <id>http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-2/</id>
    <published>2019-10-31T09:06:37.000Z</published>
    <updated>2019-10-31T09:11:18.658Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习第二部分</p><a id="more"></a><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>动态规划的基本思路是：将问题划分为可存储的子优化问题，通过解决子问题来最终解决父问题。<br>在强化学习中，由于MDP的贝尔曼方程的存在，我们可以很容易地将问题递归表示。</p><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>基本的动态规划算法为策略迭代。策略迭代分为两大部分：</p><ol><li>对策略的价值估计</li><li>对策略的优化</li></ol><p>具体而言，就是先用当前的策略进行价值估计得到$v_t$, 然后根据估计的价值来更新$\pi_t$，在下一时刻，使用更新后的策略继续估算价值。<br>价值估计与优化问题合起来被称为控制问题，这两部分是强化学习所重点关注的地方。</p><h3 id="Iterative-Policy-Evaluation-迭代策略估计"><a href="#Iterative-Policy-Evaluation-迭代策略估计" class="headerlink" title="Iterative Policy Evaluation 迭代策略估计"></a>Iterative Policy Evaluation 迭代策略估计</h3><p>利用贝尔曼方程我们可以递归的计算$v_\pi$:</p><script type="math/tex; mode=display">v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s', r} p(s' r | s, a)[r + \gamma v_k(s')]</script><p>基本的算法为:</p><pre><code>input policy piinit array V[s]=0 for s in SRepeat   delta := 0  for each s in S:    v := V(s)    V(s) := sum_a(pi(a|s) * sum_s&#39;,r(p(s&#39;, r| s,a)[r + gamma V[s&#39;]]))    delta := max(delta, |v - V(s)|)until delta &lt; thetareturn V</code></pre><p>TODO: 收敛性证明</p><h3 id="策略优化"><a href="#策略优化" class="headerlink" title="策略优化"></a>策略优化</h3><p>我们首先给出策略优化原理：</p><blockquote><p>对于一个策略$\pi’$，如果对于所有的状态$s$都有$\sum<em>a \pi’(a|s) q</em>\pi(s, a) \geq \sum<em>a \pi(a|s) q</em>\pi(s, a) $</p></blockquote><p>那么，策略$\pi’$就不坏于策略$\pi$。<br>对于任意的$v<em>\pi(s)$, 数值上，有$v</em>\pi(s) \leq q<em>\pi(s,\pi’(s)) \leq v</em>{\pi’}(s)$<br>后者可以通过期望式展开成序列形式为：$\leq \mathbb{E}[R<em>{t+1} + \gamma v</em>\pi(S<em>{t+1})|S_t = s] \leq \mathbb{E}</em>{\pi’}[R<em>t+1 + \gamma q</em>\pi(S<em>{t+1}, \pi’(S</em>{t+1}| S_t = s))]$<br>TODO: 详细证明</p><p>策略优化算法如下：</p><pre><code>init V[s] in R and pi(s) in A(s) arbitrarily, for all s in S# policy evaluationRepeat   delta := 0  for each s in S:    v := V[s]    V[s] := sum_s&#39;,r(p(s&#39;, r|s, a)[r + gamma * V[s&#39;]])  until delta &lt; theta# policy improvementRepeat  policy-stable := true  for each s in S:    a := pi(s)    pi(s) := argmax_a sum_s,r(p(s&#39;, r|s, a)[r + gamma V[s&#39;]])    if a != pi(s) then policy-stable := false  if policy-stable == true then stop; else goto evaluation</code></pre><h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><p>价值迭代与策略迭代的不同在于：策略迭代每次先进行evaluation,然后根据evaluation的结果选择动作，而价值迭代直接计算每一个动作的期望，根据期望来选取动作。</p><p>算法如下:</p><pre><code>init array V abitrarilyRepeat  delta := 0  for each s in S:    v := V[s]    V[s] := max_a sum_s,r(p(s&#39;, r| s, a)[r + gamma * V(s&#39;)])    delta := max(delta, |v - V[s])  until delta &lt; theta  output deterministic policy pi, where    pi(s) = argmax_a sum_s,r(p(s&#39;, r|s, a)[r + gamma*V[s&#39;]])</code></pre><h1 id="Monte-Carlo-Methods蒙特卡洛法"><a href="#Monte-Carlo-Methods蒙特卡洛法" class="headerlink" title="Monte-Carlo Methods蒙特卡洛法"></a>Monte-Carlo Methods蒙特卡洛法</h1><p>动态规划成立的前提是，我们知道环境动态函数$p$，而蒙特卡洛法是为了解决在没有环境动态函数的情况下进行强化学习的问题的统计学方法。</p><p>蒙特卡洛法是一种通过经验学习价值函数的方法，其中的经验有两种：</p><ol><li>实际经验，从环境中真实学习的经验。</li><li>模拟经验，使用一个模型来近似真实的环境</li></ol><p>在蒙特卡洛法中，价值$v_\pi$被定义成对回报的采样的平均数。</p><script type="math/tex; mode=display">v_\pi(s) \circeq \mathbb{E}[\sum_{k=0}^{T-1}\gamma^kR_{k+1}] \sim \frac{1}{\Epsilon(s)} \sum_{t_i \in \Epsilon(s)}\sum_{k=t_i}^{\gamma^{k-1}R^i_{k+1}}</script><h2 id="蒙特卡洛估计"><a href="#蒙特卡洛估计" class="headerlink" title="蒙特卡洛估计"></a>蒙特卡洛估计</h2><p>蒙特卡洛有两种计算方法，当每个状态，动作的访问次数趋于无穷时，它们是等价的:</p><ol><li>first-visit MC: 只考虑每个episode第一次访问到的(S, A) 对</li><li>every-visit MC: 对所有的(S, A)对进行采样</li></ol><p>我们这里给出first-visit 的算法：</p><pre><code>Init:  pi := policy to be evaluated  V := arbitrarily init  Returns[s] := [] for all s in SRepeat forever:  Generate an episode using pi  for each state s in episode:    G := return following first occurence of s    Append G to Returns[s]    V[s] := average(Return[s])</code></pre><p>蒙特卡洛法同样可以对动作价值进行估计：</p><script type="math/tex; mode=display">q_\pi(s, a) \circeq \mathbb{E}[G_t | S_t = s, A_t = a]</script><h2 id="MC控制"><a href="#MC控制" class="headerlink" title="MC控制"></a>MC控制</h2><p>采取贪婪策略进行动作选择满足策略优化定律，我们假设MC的迭代是无限的，给出蒙特卡洛控制算法：</p><pre><code>Init   for all s in S, a in A:    Q(s, a) := arbitrarily    pi(s) := arbitrarily    Returns(s, a) := []Repeat forever:  Choose S_0 in S, A_0 in A[S_0] as start point s.t all pairs have prob &gt; 0  Generate an episode according to pi  For (s, a) in episode:    G := return of s, a    Append G to Returns(s, a)    Q(s, a) := average(Returns(s, a))  For each s in episode:    pi[s] &lt;- argmax_a Q(s, a]</code></pre><p>单纯的贪婪会使得更新参数变得很慢—很可能陷入某个局部最优然后不断强化，忽视探索其他动作，我们通常使用e-soft 策略来保证探索。e-greedy不改变期望。</p><h2 id="Off-policy-蒙特卡洛"><a href="#Off-policy-蒙特卡洛" class="headerlink" title="Off-policy 蒙特卡洛"></a>Off-policy 蒙特卡洛</h2><p>estimate的时候的策略$$ 与目标策略$pi$ 不完全相等时，我们称该方法为off-policy方法。</p><p>使用蒙特卡洛法时，对于off-policy方法，我们的estimate期望将不基于目标Policy, 而是基于我们的估计策略的policy.<br>在这种情况下，我们可以将整个采样过程看做一个重要性采样，采样的分布是基于估计策略$u$的分布。<br>我们需要用<strong>重要性系数</strong>来修正期望：</p><script type="math/tex; mode=display">\rho_{t:T} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{u(A_k)|S_k}</script><p>在重要性系数修正下：</p><script type="math/tex; mode=display">\mathbb{E}[\rho_{t:T}G_t|S_t = s] = v_\pi(s)</script><p>由于重要性采样会导致方差存在上升至无限大的可能性(将重要性采样看做是在某一区间特别密集地取值)，我们还需要对采样的平均长度进行修正：</p><script type="math/tex; mode=display">\eta^{-1} = \sum_{t_i in \Epsilon(s)}\rho_{t:T}</script><h1 id="时间差分算法-Temporal-Difference"><a href="#时间差分算法-Temporal-Difference" class="headerlink" title="时间差分算法(Temporal-Difference)"></a>时间差分算法(Temporal-Difference)</h1><p>最基本的时间差分算法(TD(0))可以看做每次仅仅往前方看一步进行evaluation。由于这种情况下我们没有办法获得整个序列的回报，我们用当前估计的期望来作为我们的Target。</p><h2 id="TD-Evaluation"><a href="#TD-Evaluation" class="headerlink" title="TD Evaluation"></a>TD Evaluation</h2><p>最基础的TD Evaluation 算法(TD(0))如下，可以看出同MC方法相比，TD(0)最重要的改变是将更新公式中的累计回报换成了当前的奖励信号与当前估计的下一状态的期望之和:</p><pre><code>input policy pistep size 0 &lt; alpha &lt;= 1init V[s] for all s in S, arbitrarily except V(terminal) = 0for each episode:  init S  for each step in episode:    A := action from pi(S)    take action, observe R, S&#39;    V[s] := V[s] + alpha[R + gamma * V[S&#39;] - V[S]]    S := S&#39;  until S is terminal</code></pre><p>TD算法的好处：首先，每次都要进行更新，避免了蒙特卡洛法需要迭代完整的一个episode再进行更新的问题，其次，需要的计算力与空间更少</p><h2 id="TD-Control"><a href="#TD-Control" class="headerlink" title="TD Control"></a>TD Control</h2><p>根据是否on-policy, 我们可以将TD Control 分成两种算法:</p><ol><li>SARSA: on-policy control</li><li>Q-learning: off-policy control</li></ol><h3 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h3><p>我们给出SARSA算法如下</p><pre><code>Init   Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0Repeat for each episode  Init S  Choose A from S using policy derived from Q  Repeat for each step in episode    Take action A, observe R, S&#39;    Choose A&#39; fomr S&#39; using policy    Q[S, A] := Q[S, A] + alpha[R + gamma * Q[S&#39;, A&#39;] - Q[S, A]]    S := S&#39;    A := A  Until S is terminal</code></pre><p>SARSA有一种变种：不使用<code>Q[S&#39;, A]</code> 来进行更新，而是使用下一步状态的期望<code>sum_a (pi(S&#39;)Q(S&#39;, a))</code>来进行更新，其思路主要是通过期望计算来降低方差，从而提升学习效率。</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>Q-learning 使用 算法如下：</p><pre><code>Init   Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0Repeat for each episode  Init S  Choose A from S using policy derived from Q  Repeat for each step in episode    Take action A, observe R, S&#39;    Choose A&#39; fomr S&#39; using policy    Q[S, A] := Q[S, A] + alpha[R + gamma * max_a (Q[S&#39;]) - Q[S, A]]    S := S&#39;  Until S is terminal</code></pre><p>注意，同MC Off-policy相比，这个方法不需要重要性系数。其原因是动作a此处是确定的(argmax(Q[S]))，而非随机变量。</p><h2 id="N-Step-TD"><a href="#N-Step-TD" class="headerlink" title="N-Step TD"></a>N-Step TD</h2><p>N-Step TD 是在单步TD与MC方法中间的桥梁：</p><script type="math/tex; mode=display">G_{t:t+n} = \sum_{k=1}^{n}\gamma^{k-1}R_{t+k} + \gamma^n V_{t+n-1}(S_{t+n})</script><p>在n-step TD的情况下，我们需要使用重要性系数来修正我们的off-policy算法的价值估计。</p><script type="math/tex; mode=display">Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha \rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]\rho_{t:h} = \prod_{k=t}^{\min(h,T-1)} \frac{\pi(A_k|S_k)}{u(A_k|S_k)}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;强化学习第二部分&lt;/p&gt;
    
    </summary>
    
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
      <category term="rl" scheme="http://MeowAlienOwO.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>强化学习导论（一）</title>
    <link href="http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-1/"/>
    <id>http://MeowAlienOwO.github.io/2019/10/31/reinforcement-learning-1/</id>
    <published>2019-10-30T16:00:00.000Z</published>
    <updated>2019-10-31T09:21:48.711Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用，基本可以看做是 ppt + sutton 书的一些翻译+笔记</p><a id="more"></a><h1 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h1><blockquote><p>通过与环境的持续交互学习，从而解决序列性的决策问题。</p></blockquote><p>强化学习是机器学习的一个分支，其特点为：</p><ol><li>没有监督数据，只有奖励信号</li><li>奖励信号不一定是实时的</li><li>行为与环境交互 影响数据</li><li>时间是一个重要因素 &lt;- ?</li></ol><p>我们定义智能体(agent) 作为强化学习的主体，其能通过动作(action)与环境(environment)交互，从而获取奖励信号(reward)，或者说反馈。<br>智能体在环境中进行序列性的决策，从而提高累计奖励(accumulate reward)</p><h2 id="强化学习的一些挑战"><a href="#强化学习的一些挑战" class="headerlink" title="强化学习的一些挑战"></a>强化学习的一些挑战</h2><ol><li>环境未知：有的时候我们无法解析地，或者无法有足够的数据来对环境的状态与奖励进行建模</li><li>探索-采集(exploration-exploitation) 问题：我们需要平衡探索（通过交互获取更多的环境信息）与采集（e.g.贪婪获取更优的奖励）</li><li>延迟奖励：有些奖励同历史动作相关联</li></ol><h1 id="多臂赌博机-Multi-Armed-bandit-problem"><a href="#多臂赌博机-Multi-Armed-bandit-problem" class="headerlink" title="多臂赌博机 Multi-Armed bandit problem"></a>多臂赌博机 Multi-Armed bandit problem</h1><p>多臂赌博机问题是一个最基本的强化学习问题：给定$k$个赌博机，每个时刻$t$可以选择一个赌博机进行操作，从而获取一个标量奖励。每一个赌博机的奖赏分布都是独立的不同分布。<br>获取的奖励$R_t$是一个随机变量。我们定义$q_{*}(a)$为进行动作$a$的奖励期望：</p><script type="math/tex; mode=display">q_{*} = \mathbb{E}[R_t|A_t = a]</script><p>为了求得该期望我们可以进行动作价值估计: $Q_t(a) = q_{*}(a)$。最基本的估计使用如下公式:</p><script type="math/tex; mode=display">Q_t(a) = \frac{1}{N_t(a)}\sum_{\tau=1}^{t-1}R_{\tau}[A_{\tau}= a]_1</script><p>即选取动作$a$的时候，采样的相对应的奖励的平均值。我们可以将$Q$写成递归形式来方便之后的讨论：</p><script type="math/tex; mode=display">Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]</script><p>根据我们的估计期望$Q$，我们有贪婪算法$A_t^{*} = \arg\max_a Q_t(a)$来选取最佳的动作。根据动作是否贪婪，我们可以将动作分成两部分：</p><ol><li>贪婪动作-&gt;采集</li><li>非贪婪动作-&gt; 探索</li></ol><h2 id="epsilon-greedy"><a href="#epsilon-greedy" class="headerlink" title="$\epsilon$-greedy"></a>$\epsilon$-greedy</h2><p>为了平衡搜索与采集，我们给定一个探索概率$\epsilon$, 即每次选取动作的时候我们有概率$\epsilon$随机选取概率，反之则采取贪婪算法。易得在该条件下，我们有$1-\epsilon + \frac{\epsilon}{|\mathcal{A}|}$的概率来选择贪婪动作。$\epsilon$ 用于调整探索与采集的平衡。</p><h2 id="强化学习的一般更新规则"><a href="#强化学习的一般更新规则" class="headerlink" title="强化学习的一般更新规则"></a>强化学习的一般更新规则</h2><p>根据上述的递归形式，我们不假证明地给出一般的更新规则：</p><p>NewEstimate &lt;- OldEstimate + StepSize[Target - OldEstimate]</p><p>其中，Target并不固定为单纯的奖励信号。</p><h2 id="多臂赌博机算法"><a href="#多臂赌博机算法" class="headerlink" title="多臂赌博机算法"></a>多臂赌博机算法</h2><h3 id="e-greedy-单多臂赌博机算法"><a href="#e-greedy-单多臂赌博机算法" class="headerlink" title="e-greedy 单多臂赌博机算法"></a>e-greedy 单多臂赌博机算法</h3><pre><code>init, for a = 1 to k:  Q(a) &lt;- 0  N(a) &lt;- 0loop forever:  A &lt;- 1. argmax_a Q(a) 1 - $\epsilon$       2. random a      $\epsilon$  R &lt;- bandit(A)  N(A) &lt;- N(A) + 1  Q(A) &lt;- Q(A) + $\frac{1}{N(A)}[R - Q(A)]$</code></pre><h3 id="非平稳过程"><a href="#非平稳过程" class="headerlink" title="非平稳过程"></a>非平稳过程</h3><p>我们之前假设动作价值是不变的，但是在实际中，动作价值可能会随着时间的改变而改变(non-stationary)。在这种情况下，我们不能采样求平均，而是需要用step-size parameter 来控制取一段时间的平均</p><script type="math/tex; mode=display">Q_{n+1} = Q_n + \alpha[R_n - Q_n], \alpha \in (0, 1]</script><h3 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h3><p>上确界动作选取(upper confidence bound, UCB法不对动作价值进行估计，而是估计动作价值的<strong>上确界</strong>来进行动作选取。该方法的好处是，将不确定性也一并纳入估计。<br>上确界的动作选取法如下：</p><p>```<br>A_t = 1. a if N_t(a) = 0,</p><pre><code>  2. argmax[Q_t(a) + c Sqrt(log(t)/ N_t(a))]</code></pre><p>``**<br>其中，平凡根项是对不确定性或者说方差的一个度量。c 是一个可控制的常量，用于控制不确定性影响的大小。</p><p>UCB 一般而言有更好的性能，但是对于非平稳过程的处理不像e-greedy那么简单。</p><h3 id="Gradient-bandit"><a href="#Gradient-bandit" class="headerlink" title="Gradient bandit"></a>Gradient bandit</h3><p>梯度法是一种不通过直接估计动作价值$Q$，而是直接优化动作选取的策略(policy)的强化学习方法。</p><p>我们定义$\pi_t(a)$ 为时刻$t$的动作选取策略。$\pi_t(a)$是关于当前状态动作选取概率的分布，我们可以用随机梯度上升法来优化（前提是：策略是一个可微分的函数）。</p><p>定义策略为softmax函数:</p><script type="math/tex; mode=display">\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}</script><p>其中$H_t(a)$ 定义为对动作的偏好度(preference)，从而影响动作的概率。<br>根据softmax函数的导数我们有:</p><script type="math/tex; mode=display">H_{t+1}(a) = H_t(a) + \alpha(R_t - avg R)([a = A_t] - \pi_t(a))</script><h1 id="Markov决策过程"><a href="#Markov决策过程" class="headerlink" title="Markov决策过程"></a>Markov决策过程</h1><p>我们把交互的环境看作是一个马尔科夫链：时刻$t+1$的状态与奖励仅与前一个时刻$t$的状态与采取的动作有关。据此定义马尔科夫决策过程(Markov Decision Process, MDP):</p><ol><li>状态空间$\mathcal{S}$</li><li>动作空间$\mathcal{A}$</li><li>奖励空间$\mathcal{R}$</li></ol><script type="math/tex; mode=display">Pr\{S_{t+1}, R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1},...S_0, A_0\} = Pr\{S_{t+1}, R_{t+1} | S_t, A_t\}</script><p>MDP是有限的当且仅当$\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$ 是有限的。</p><h2 id="环境动态"><a href="#环境动态" class="headerlink" title="环境动态"></a>环境动态</h2><p>我们定义函数$p:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S} \times \mathcal{R}$为MDP的环境动态(Environment Dynamic)。这个函数实际上定义为状态$s’$, 奖励$r$在给定状态$s$, 采取的动作$a$的条件概率分布，即MDP的状态之间是如何转化的。</p><script type="math/tex; mode=display">p(s', r|s, a) = Pr{S_{t+1}=s', R_{t+1}=r | S_t = s, A_t = a}</script><p>根据动态函数p, 奖励$r$的边缘概率即为状态$s’$的概率分布</p><script type="math/tex; mode=display">p(s'|s, a) = \sum_{r\in \mathcal{R}} p(s', r|s, a)</script><p>我们定义函数$r:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R}$ 为给定状态$s$, 动作$a$下的奖励期望:</p><script type="math/tex; mode=display">r(s, a) = \mathcal{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r\sum_{s'\in S} p(s', r| s, a)</script><p>MDP可以被表示成一个有限状态自动机， 见 Sutton书Example 3.3</p><h2 id="目标与奖励"><a href="#目标与奖励" class="headerlink" title="目标与奖励"></a>目标与奖励</h2><p>如前面所述，强化学习的目标实际上是尽可能多的提升累计奖励(cumulative return)。这个目标建立在<strong>奖励假设</strong> (reward hypothesis)上：</p><blockquote><p>强化学习目标是最大化标量奖励信号的累计期望</p></blockquote><p>根据这个假设，我们认为奖励信号实际上定义了我们的目标。奖励信号不说明如何实现目标，但是如果奖励信号设计的好，我们的学习将会提速。<br>奖励信号与状态空间的设计都被认为是RL中的“工程”部分。</p><h2 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h2><p>我们定义 <strong>回报</strong> (return)为奖励信号序列$R<em>{t}, R</em>{t+1}, R_{t+2}…$的一个函数，通过最大化该函数来实现我们的目标。最简单的回报函数就是线性加和：</p><script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + ... + R_{T} = R_{t+1} + G_{t+1}</script><p>其中$T$是中止时间。以上的定义在有限步骤的情况下是成立的，但是在连续(非停止)情况下，我们可以使用一个衰减概率来控制我们求和的范围:</p><script type="math/tex; mode=display">G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+1+k}</script><p>当$\gamma &lt; 1$, 求和有上界:</p><script type="math/tex; mode=display">\sum_{k=0}^{\infty} \gamma^k R_{t+1+k} \leq r_{\max} \sum_{k=0}^{\infty}\gamma^k = r_{\max} \frac{1}{1-\gamma}</script><h2 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h2><p>给定策略$\pi$, 我们可以计算在该策略下，每一个状态的价值$v_{\pi}(s)$:</p><script type="math/tex; mode=display">v_{\pi}(s) \circeq \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| S_t = s]</script><p>同理，我们可以计算给定状态的动作价值$q_\pi$:</p><script type="math/tex; mode=display">q_{\pi}(s, a) \circeq \mathbb{E}_{\pi}[G_t| S_t = s, A_t = a] = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a]</script><p>(PPT此处有一个直接求序列空间价值函数的东西，感觉没什么用就不写了，基本就是对每一个可能序列的概率进行求和，一个序列的概率是p函数与$\pi$函数的累乘)</p><h2 id="Bellman方程"><a href="#Bellman方程" class="headerlink" title="Bellman方程"></a>Bellman方程</h2><p>根据Markov性质，下一时刻的状态-奖励对由且仅由这一时刻的状态与采取的动作决定，也就是说我们可以递归地更新我们的价值函数。我们将价值函数写成递归的形式:</p><script type="math/tex; mode=display">\begin{aligned}v_\pi(s) & \circeq \mathbb{E}_\pi[G_t | S_t = s] \\         & = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]\\         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma \mathbb{E}_\pi[G_{t+1} | S_{t+1}=s']]\\         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s') ]\end{aligned}</script><p>同理，我们可以写出动作价值函数的递归形式:</p><script type="math/tex; mode=display">\begin{aligned}q_\pi(s, a) & \circeq \mathbb{E}_\pi[G_t | S_t = s, A_t=a] \\            & = \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')]\end{aligned}</script><p>以上两个函数被称为贝尔曼方程(bellman equation)</p><h3 id="最优策略贝尔曼方程"><a href="#最优策略贝尔曼方程" class="headerlink" title="最优策略贝尔曼方程"></a>最优策略贝尔曼方程</h3><p>我们用价值函数(期望)比较两个策略的好坏：当且仅当策略$\pi$每一个状态的期望价值都不低于另一个策略$\pi’$时，我们可以认为$\pi$有着更优的表现。如果存在一个策略，其对于所有可能的策略都是更优的，我们称该策略为最优策略(optimal policy)</p><p>一个策略$\pi$是最优的，当：</p><ol><li>$v<em>\pi(s) = v</em>{*}(s) = \max<em>{\pi’}v</em>{\pi’}(s)$</li><li>$q<em>\pi(s, a) = q</em>{*}(s, a) = \max<em>{\pi’} q</em>{\pi’}(s, a)$</li></ol><p>在最优策略下，我们可以用最优的动作来替代策略下的条件分布：</p><script type="math/tex; mode=display">v_{*}(s) = \max_{a} \sum_{s', r} p(s', r|s, a)[r + \gamma v_{*}(s')]</script><script type="math/tex; mode=display">q_{*}(s, a) = \sum_{s', r} p(s', r|s, a)[r + \gamma \max_{a'} q_{*}(s', a)]</script><p>对于有限的MDP与non-terminate episode而言，每个策略$\pi$都会遍历状态空间，空间中的每个状态理想情况下都会被访问无限次。<br>我们定义时间趋于无穷时，状态的分布为平稳状态分布$P<em>\pi(s) = Pr{S_t = s, |A_0, …, A</em>{t-1} \sim \pi }$。<br>此时，我们使用平均奖励(average reward)来评价策略的价值:</p><script type="math/tex; mode=display">\begin{aligned}r(\pi) &= \lim_{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}[R_t | S_0, A_0, ...]       &= \sum_{s} P_{\pi}(s)\sum_a \pi(a|s) \sum_{s', r} p(s', r\|s, a) r\end{aligned}</script><p>最大化在平稳状态分布下的回报等同于最大化平均奖励。</p><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/28084904" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28084904</a></li><li>Sutton, An Introduction To Reinforcement Learning</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用，基本可以看做是 ppt + sutton 书的一些翻译+笔记&lt;/p&gt;
    
    </summary>
    
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
      <category term="rl" scheme="http://MeowAlienOwO.github.io/tags/rl/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU：Open-vocabulary Models</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/08/nlu-5/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/08/nlu-5/</id>
    <published>2019-05-08T12:52:40.000Z</published>
    <updated>2019-10-22T09:22:09.766Z</updated>
    
    <content type="html"><![CDATA[<p>本文对 open-vocabulary model 进行一些复习跟梳理。<br><a id="more"></a></p><h1 id="Open-vocabulary-models"><a href="#Open-vocabulary-models" class="headerlink" title="Open-vocabulary models"></a>Open-vocabulary models</h1><p>一般而言，传统的 NLP 使用 one-hot 编码来对词汇进行处理。但是，通常而言自然语言的词汇表会非常庞大，这会直接导致我们的网络结构过于复杂，训练速度变慢。在翻译领域，这一表现尤其明显：首先，翻译的语料库往往包含大量的单词种类；而且在生成单词过程中，我们需要处理未见过的单词；另外名字，数字等通常有着简单的形式，但是他们自身是开放的单词类(open word classes)。因此，我们需要 open-vocabulary model 来解决庞大乃至无限大的词汇空间。</p><p>我们可以简单地将不在词汇表里的单词用一个 out of vocabulary (unk) 符号来表示。如果我们单从单词覆盖率的角度来看，需要用 unk 代替的稀有词汇其实不多 (5% for 50000 words)。但是这些词汇往往包含很多信息，比如说人名，地名，专有名词等等，而这些信息的缺失在翻译中是致命的。</p><hr><h2 id="Approximative-softmax"><a href="#Approximative-softmax" class="headerlink" title="Approximative softmax"></a>Approximative softmax</h2><p>在一个“激活”的词汇表子集上做 softmax 处理。在训练时，神经网络的输出端是原有的目标词汇表的一个子集，更新梯度的时候只处理在子集中正确的结果，然后在使用的时候在整个目标词汇表上计算单词的概率。这个方法能够使得使用更大规模的目标词汇表成为可能，但是有两个问题：1. 这个方法实际上不是 open-vocabulary 的；2. 对比较稀有的词汇，这个网络难以学习到正确的表示。</p><h2 id="Back-off-model"><a href="#Back-off-model" class="headerlink" title="Back-off model"></a>Back-off model</h2><p>我们在训练的时候依然使用 unk 来替代稀有的词汇，但是在系统生成 unk 后，对 unk 符号做对齐以映射到源语言的单词，然后用 back-off 方法(根据词汇表逐词翻译等)来对映射的单词进行翻译。这个方法同样有很多的问题：1. 很难处理一对多的关系；2. 难以处理词形(inflection)；3. 如果目标语言与源语言的字母表不同（中文，英文），需要一套转写名字的系统;4. 对齐方面，attention 不一定总是可靠的。</p><h2 id="Subword-NMT"><a href="#Subword-NMT" class="headerlink" title="Subword-NMT"></a>Subword-NMT</h2><p>Subword-NMT 的思路是，将单词拆开来成为 subword， 然后使用一些 NMT 的手段处理 sub-word。比如，我们如果要对特朗普做翻译：</p><ol><li>英文：<span style="color:red">T</span><span style="color:blue">rum</span><span style="color:green">p</span></li><li>中文：<span style="color:red">特</span><span style="color:blue">朗</span><span style="color:green">普</span></li><li>日文：<span style="color:red">ト</span><span style="color:blue">ラン</span><span style="color:green">プ</span></li></ol><p>而对于数字，专有名词等，我们在人工翻译的时候往往也是逐 subword 翻译。</p><p>我们希望 open-vocabulary 系统能够：</p><ul><li>用一个相对小的词汇表编码所有的词</li><li>能够在未出现过的单词上泛化良好</li><li>需要比较小的训练文本</li><li>相对的，有较好的翻译质量</li></ul><h3 id="Byte-pair-encoding-for-word-segmentation-字节串编码"><a href="#Byte-pair-encoding-for-word-segmentation-字节串编码" class="headerlink" title="Byte-pair encoding for word segmentation 字节串编码"></a>Byte-pair encoding for word segmentation 字节串编码</h3><p><a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">论文翻译</a><br><a href="https://www.aclweb.org/anthology/P16-1162" target="_blank" rel="noopener">论文</a></p><p>字节串编码的基本思路是使用一个新的、不在词汇表里的符号来替代原来字符串里经常出现的符号。<br>我们首先将单词表示为字母组成的序列，然后使用字节串编码对其进行压缩，压缩后的子字符就是我们的新的词汇表，我们可以使用超参数来控制词汇表的大小。</p><p>BPE 的算法简述如下：</p><ol><li>初始化符号表，先将所有的字母放到符号表中，然后将每个单词表示为符号序列。每个单词的结尾符号使用特殊的符号来标注，使得我们可以重建单词(e.g. 词尾的 w 与 词中的 w 不一样)</li><li>迭代地计算所有的符号对，将出现次数最多的符号对替换成为一个单独的符号，比如 <code>(&#39;A&#39;, &#39;B&#39;)</code> 会被换成 <code>(&#39;AB&#39;)</code> 这个操作每次相当于创建了一个 n-gram 的编码。同理，<code>(&#39;AB&#39;, &#39;C&#39;)</code> 会被换成<code>(&#39;ABC&#39;)</code>。迭代多次，可以将出现频率最高的 n-gram 编码成为新的符号</li><li>我们将新合并的符号放入原有的词汇表，最终的词汇表大小为原有的大小+合并操作次数</li></ol><p>举例，我们在原有的词汇表里面有如下单词，分别出现若干次:</p><blockquote><p>low:5 lower:2 newest:6 widest:3</p></blockquote><p>现有的词汇表如下：</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d</p></blockquote><p>观察我们的训练数据，我们可以发现 <code>(&#39;e&#39;, &#39;s&#39;)</code> 是出现次数最多的（9+3），于是我们使用 <code>(&#39;es&#39;)</code> 代替。迭代地，我们发现<code>(&#39;es&#39;, &#39;t&lt;/w&gt;&#39;)</code> 出现次数最多，于是我们也将其编码为 <code>(&#39;est&lt;w/&gt;&#39;)</code>，词汇表变为：</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d es est&lt;\w&gt;</p></blockquote><p>再一次迭代，最多的符号对变成了 <code>(&#39;l&#39;, &#39;o&#39;)</code>, 而不是 <code>(&#39;w&#39;, &#39;est&lt;\w&gt;&#39;)/(&#39;d&#39;, &#39;est&lt;\w&gt;&#39;)</code>。于是符号表变为:</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d es est&lt;\w&gt; lo</p></blockquote><p>使用 BPE 的编码方法比 back-off 会有大概 5% 左右的提升(BLEU分数), 而且在稀有词的表现更好。</p><p>如果我们将源语言与目标语言的 BPE 合并处理会有更好的一致性。而分开处理，有可能导致同样的名字在不同的语言中被不同地分割，从而影响一致性。</p><p>If we apply BPE independently, the same name may be segmented<br>differently in the two languages, which makes it<br>harder for the neural models to learn a mapping<br>between the subword units. </p><h2 id="Character-level-models"><a href="#Character-level-models" class="headerlink" title="Character-level models"></a>Character-level models</h2><ul><li>优点：<ul><li>open-vocabulary</li><li>不需要启发性/language specific 分割</li><li>神经网络可以从字符串序列学习</li></ul></li><li>缺点：<ul><li>序列长度增加导致训练与解码的耗时增加(2-8倍)</li></ul></li><li>open-questions:<ul><li>on which level represent meaning?</li><li>on which level attention operate?</li></ul></li></ul><h3 id="Hierarchical-model-back-off-revisited"><a href="#Hierarchical-model-back-off-revisited" class="headerlink" title="Hierarchical model: back-off revisited"></a>Hierarchical model: back-off revisited</h3><p>在单词层面，使用 UNK 替代， 对于每一个 UNK, 使用 character level model预测单词，基于单词的 hidden state。</p><ol><li>比查字典更灵活</li><li>比纯 character-level 翻译有更好的准确性</li><li>main model 与 back-off model 之间有独立的假设(Markov?)</li></ol><h3 id="Character-level-output"><a href="#Character-level-output" class="headerlink" title="Character-level output"></a>Character-level output</h3><ul><li>目标语言单词不需要分割，使用 character-level 表示(i.e. 字符序列)</li><li>encoder 使用 BPE-level 词汇表</li><li>EN -&gt; {DE, CS, RU, FI} 有较好表现</li><li>训练时间长</li></ul><h3 id="Character-level-input"><a href="#Character-level-input" class="headerlink" title="Character-level input"></a>Character-level input</h3><ul><li>输入层面不做分割，使用 character-level 的表示</li><li>使用 character-level lstm 来计算字符序列的向量形式</li></ul><h3 id="Fully-Character-level-NMT"><a href="#Fully-Character-level-NMT" class="headerlink" title="Fully Character-level NMT"></a>Fully Character-level NMT</h3><ul><li>跨单词处理</li><li>目标语言端使用 character-level RNN</li><li>源语言端：CNN + max-pooling</li></ul><p><img src="/images/fully-character-level-NMT.png" alt></p><h3 id="Large-capacity-character-level-NMT"><a href="#Large-capacity-character-level-NMT" class="headerlink" title="Large-capacity character-level NMT"></a>Large-capacity character-level NMT</h3><ul><li>训练深度 attentional LSTM encoder-decoder</li><li>浅层模型：BPE</li><li>深度模型：character-level 模型更好</li><li>主要的问题是训练时间过长</li><li>主要的挑战：在保证质量的前提下压缩表示方法</li></ul><h1 id="Morphology"><a href="#Morphology" class="headerlink" title="Morphology"></a>Morphology</h1><p>单词根据词法，会有不同的变化：</p><ul><li>词形(inflection)</li><li>case (格)</li><li>数量(number)</li><li>agreement </li></ul><p>对于英语来说:</p><ul><li>case</li><li>number </li><li>person</li><li>tense</li></ul><p>… 总之 词法很复杂</p><p>在书写系统中，我们定义 morpheme 为最小的对意义造成影响的单位</p><ul><li>free morpheme: 独立地作用, dog/house</li><li>bound morpheme: 只作为单词的一部分出现： un-, -ed, -ing</li></ul><p>对于中文而言，偏旁部首也作为 phoneme 存在</p><p>原则上来说，subword/character-level 模型能够学习到词法生成的规则，但是在实际上从文本中学习到词法是很困难的：</p><ul><li>subword 不一定是 morphoneme</li><li>有关联的词的形态不一定相似: stand-stood</li><li>词法上，语言所表示的含义可能不同</li></ul><p>另外，语言学的研究提供了很多词法规则可以直接拿来使用。一种最基本的方法是直接用字典形式+前后缀来替代输入中的一些变化的词(lemmatize)</p><h2 id="Morphology-on-Source-side"><a href="#Morphology-on-Source-side" class="headerlink" title="Morphology on Source side"></a>Morphology on Source side</h2><p>对于输入而言，我们可以将原本的单词向量替换成单词向量+字典形式向量(lemma)拼接后的向量。</p><h2 id="Morphology-on-Target-side"><a href="#Morphology-on-Target-side" class="headerlink" title="Morphology on Target side"></a>Morphology on Target side</h2><p>2-step 翻译: 首先用主系统预测字典形式的单词，然后将字典形式的单词使用基于统计的词形转换系统进行预测</p><p>2-step NMT：首先交叉地预测字典形式单词与词法类别，然后使用有限状态转换器(finite state transducer)来处理词形变换</p><h2 id="Neural-Inflection-Generation"><a href="#Neural-Inflection-Generation" class="headerlink" title="Neural Inflection Generation"></a>Neural Inflection Generation</h2><p>输入为单词的字典形与词法参数(时态，数量，格等)，使用 encoder-decoder 模型来预测转换后的单词</p><h2 id="Information-missing-in-source-case-study-Politeness"><a href="#Information-missing-in-source-case-study-Politeness" class="headerlink" title="Information missing in source, case study: Politeness"></a>Information missing in source, case study: Politeness</h2><p>在英语里，you 没有普通用法与尊敬用法的区别，但是在很多其他语言中，这样的区别存在。将英语翻译成别的语言的时候，我们需要判断何时应当用普通形，何时应当用敬体形。</p><p>解决问题的主要思路如下：<br>我们需要根据目标端的一些额外信息：在目标语言的语境中是否礼貌，将其作为一个额外的标注拼接在源语言序列后面。在测试的时候，我们可以控制输入是否礼貌。i.e. 对输入增加一些源语言中不存在的信息的标注。</p><p>同样的思路可以应用到时态，evidentiality，领域适配，对输出语言的控制等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文对 open-vocabulary model 进行一些复习跟梳理。&lt;br&gt;
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU：Dependency Parsing, RNNG, Semantic Role Labelling</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/07/nlu-4/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/07/nlu-4/</id>
    <published>2019-05-07T14:48:59.000Z</published>
    <updated>2019-10-22T09:22:15.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h1><p>传统的语法模型通常是成分式 (constituent structure) 的：把句子看作是不同成分的组合。</p><p>成分解析法通常将一个句子组合成一棵树，树的叶节点是终结符(单词)，其上所有的非叶节点表示语法结构。举例:</p><blockquote><pre><code>             Sentence                |  +-------------+------------+  |                          |</code></pre><p> Noun Phrase                Verb Phrase<br>      |                          |<br>    John                 +———-+————+<br>                         |                |<br>                       Verb          Noun Phrase<br>                         |                |<br>                       sees              Bill</p><p> 来源： <a href="https://zhuanlan.zhihu.com/p/31766972" target="_blank" rel="noopener">知乎</a></p></blockquote><p>但是成分解析在语义的方面比较弱。以上面的解析树为例，对于动词 sees， 我们关心的是它的主语跟宾语，但是在成分解析树中，主语 John 跟宾语 Bill 都被当成 NP 处理，语义上的关联性需要去处理许多的语法成分节点。因此，我们引入依赖(dependency)。</p><p>在 dependency parsing 中，每个单词都将根据它们之间的关系连接起来形成一张有向图，图的每一个节点都是每一条边都表<br>示依赖关系。比如上例的 dependency parsing 树可以画为：</p><blockquote><p>Root —-&gt; sees —- (obj)  —-&gt; Bill<br>           |</p><pre><code>       + ----- (subj) ---&gt; John</code></pre></blockquote><p>一棵 dependency parsign 树由如下部分组成：</p><ol><li>Head</li><li>Dependent</li><li>Label identifying H and D</li></ol><h2 id="Parsing-Techniques"><a href="#Parsing-Techniques" class="headerlink" title="Parsing Techniques"></a>Parsing Techniques</h2><p>对于 consistuent parsing 来说，通常使用一些诸如 CKY，shift-reduce parsing 的算法来进行 parsing。 但<br>是，这些方法往往以来临近的单词，而 dependency parsing 通常会依赖于非邻接的单词。我们通常用两种算法来进行这<br>种 parsing: maximum-spanning trees(MST) 以及 transition-based dependency parsing (MALT) 。</p><p>我们的目标是：在所有可能的依赖树(dependency parsing)空间内，寻找到一棵最优的树。<br>令<script type="math/tex">\mathbf{x} = x_1 ... x_n</script> 为输入的单词序列，<script type="math/tex">\mathbf{y}</script>为边(dependency edge)的集合。令<script type="math/tex">(i,j)\in\mathbf{y}</script>定义为单词 <script type="math/tex">x_i</script> 到 <script type="math/tex">x_j</script> 的边。<br>由于每个词都有唯一一个父节点，这个问题同 tagging problem 很类似：每一个词都用句子中的另一个词来做 tagging。如果我们进行对整棵树进行边分解(edge factorize)，定义树的得分为所有边的乘积，我们可以简单地直接选取每个词有着最高分数的边，只要我们满足所有的边最后能够组成一棵树的约束。</p><p>我们定义一条边的分数为一个函数 <script type="math/tex">s(i,j)</script>，依赖树的分数为：<script type="math/tex">s(\mathbf{x}, \mathbf{y})=  \sum_{(i,j)\in\mathbf{y}} s(i,j)</script>。我们 parsing 的目标是，给定句子 x, 选取有最高分数的树 y 。</p><h3 id="MST-与-Chu-Liu-Edmonds-CLE-算法"><a href="#MST-与-Chu-Liu-Edmonds-CLE-算法" class="headerlink" title="MST 与 Chu-Liu-Edmonds(CLE) 算法"></a>MST 与 Chu-Liu-Edmonds(CLE) 算法</h3><p>MST 算法可以表述成如下过程：</p><ol><li>我们假定一个全连接图 G，使得句子中每个单词都与其他单词互联，任意单词对都有一条边建立。</li><li>定义评分函数 s</li><li>在 G 中，找到一棵 MST，使得树有着最高的分数且包含了所有的 node，这棵树即为最佳的 parsing tree</li></ol><p>使用 Chu-liu-Edmonds 算法，能够在 <script type="math/tex">O(n^2)</script> 的时间内找到一棵 MST, 但是不保证这棵树是 projective 的。</p><p><img src="/images/MST_full_connect.png" alt></p><p>如上图，这是一张已经评完分的全连接图。<br>首先，对于图中的所有节点，选择一个有着最高分的入边(incoming edge)，形成新的图</p><p><img src="/images/MST_greedy.png" alt></p><p>如果这一步生成了一棵树，那么这棵树就是 MST，如果存在不连通的点则无解。第三种情况必然有环的存在。</p><p>对于环而言，我们将每个环“缩”成一个等效的点，并且重新计算这个等效点入边与出边的权重。</p><p>根据<a href="https://www.dawxy.com/article/%E6%9C%80%E5%B0%8F%E6%A0%91%E5%BD%A2%E5%9B%BE%EF%BC%8C%E6%9C%B1%E5%88%98%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3%E8%BD%AC/" target="_blank" rel="noopener">这篇文章</a><br>的解释，我们需要寻找环内的一条边打断。这个打断的过程等于消除了环上一个节点的入边，以次优的入边（最大或者最小）取代之,取<strong>打断后最优</strong>的选项，进行打断，从而消除环。如上图, John 与 saw 形成了一个环，我们要将这个环替换成一个等效的点，可以打断的选项有 (John, saw) 与 (saw, John)两条边。对于每一种打断方式而言，环会形成一条链。由于我们将环缩成了等效的点，外部的入边到环的权重变为：从该边指向的节点作为链的起始，直到链的结尾的所有入边的权重之和。</p><p>我们重复以上步骤直到只有一个点，然后按照我们的打断法展开成树，就可以得到 MST。</p><p>对于打分函数而言，我们可以选用设计好的SVM MIRA 等，最近使用神经网络的比较多。对于神经网络打分函数，我们定义函数值为给定句子<script type="math/tex">\mathbf{x}</script>与父节点<script type="math/tex">w_i</script>，连通子节点<script type="math/tex">w_j</script>的概率</p><script type="math/tex; mode=display">s(i,j) = p(w_j|w_i, \mathbf{x}) = \frac{exp(g(\mathbf{a}_j, \mathbf{a}_i))}{\sum_k exp(g(\mathbf{a}_k,\mathbf{a}_i))}</script><p>其中，向量<script type="math/tex">a</script>是通过前向-后向 RNN 生成的。函数 g 计算相关性分数，一个简单的实现为：</p><script type="math/tex; mode=display">g(a_j, a_i) = v_a^T tanh(U_a a_j + W_a a_i)</script><p>我们可以看出，相关性分数的形式与之前提到过的 Attention 的计算方式很像。</p><h1 id="RNN-based-Grammar"><a href="#RNN-based-Grammar" class="headerlink" title="RNN-based Grammar"></a>RNN-based Grammar</h1><p>自然语言的语法很大程度上是有层级的(hierarchial)。如果要使用 RNN 来对语法进行建模，我们需要对此进行考虑。</p><p>RNN语法(RNNG)的一般操作如下：</p><ol><li>使用 RNN 生成序列的符号(generate symbols sequencially)</li><li>周期性的使用一些“控制符号”(control symbols)来重写历史<ol><li>周期性的将序列“压缩”(compress)成一个单独的成分(constituent)</li><li>增强(augment?) RNN 使其能够将最近的历史压缩成为一个单独的向量(reduce)</li><li>RNN 通过历史（包含压缩后与非压缩的单词）来预测新的符号</li><li>RNN 同样要预测控制符号，来决定压缩成分的大小(size of constituent)</li></ol></li></ol><p>与普通的 RNN 相比， RNNG 会生成一棵语法树。</p><p><img src="/images/RNNG-order.png" alt></p><p>实际上 RNNG 相当于 RNN-based 的 probablistic pushdown automata(下推自动机)。<br>从这个角度考虑，RNN 需要对如下内容进行建模：</p><ol><li>之前的 terminate symbols（单词，标点等)</li><li>之前的操作</li><li>当前的 stack 状态。</li></ol><p>RNN 根据以上信息来预测下一步的操作(nt, gen, reduce 等)，如下图<br><img src="/images/RNNG-stack.png" alt></p><p>当最后整个句子被处理完毕后，最后在 stack 中剩下的内容就是整个句子语法树的向量形式。</p><p>这里我们比较关注的是 reduce 的处理方式。假设我们要对下句进行处理:</p><blockquote><p>(NP <em>The hungry cat</em>)</p></blockquote><p><img src="/images/RNNG-birnn.png" alt></p><p>如图，我们使用双向 RNN/LSTM 来处理。NP 定义为语法符号的向量，括号定义为当前序列的终结符，每一个符号都用向量进行表示。BI-RNN 从左到右与从右到左两个方向进行编码，然后将两个方向的信息综合起来输出成一个向量，用来表示部分语法元素。同样的，这个输出后的向量同样可以当成普通的符号进行处理，这样我们就可以递归的解析整棵语法树。</p><p>对于 terminal 而言，我们可以发现较前面的序列是后面的序列的前缀，所以可以很轻松地使用 LSTM 来处理。但是对于 stack 而言并没有这层关系。我们使用 stack RNN 来处理这样的栈：我们给 RNN 添加一个栈指示器，这样的 RNN 可以进行两步操作：</p><ol><li>push: 读取输入，将其放在栈最上面，与当前的栈指示器的位置拼接起来</li><li>pop: 将栈指针指向当前的父元素</li></ol><p><img src="/images/stacked-rnn.gif" alt></p><p>上图展示了 stack RNN 的运作方式。（问题：这里的连接是指的时间序列还是单纯的连接？我觉得应该是时间序列，在时间轴上可以将RNN看做一个链表）下面是 stack RNN 的实际运用。</p><p><img src="/images/stacked-rnn-example.gif" alt></p><p>到现在为止我们有 3 个 RNN：第一个 RNN 用于对句子进行建模，第二个 RNN 用于对语法符号序列进行建模，第三个 stack RNN 用于对 pushdown automata 的栈进行建模。每一次预测下一个符号的概率都是由三个 RNN 的信息进行综合。</p><p>具体的模型如下：</p><script type="math/tex; mode=display">p(\mathbf{x}, \mathbf{y}) = \prod_{t=1}^{|\mathbf{a(\mathbf{x}, \mathbf{y})}|}p(a_t|\mathbf{a}_{<t})\\\mathbf{y} = \prod_{t=1}^{|\mathbf{a(\mathbf{x}, \mathbf{y})}|} softmax(\mathbf{r}_{a_t}+b_{a_t})\\\mathbf{u} = tanh(\mathbf{W}[o_t; s_t; h_t]+c)</script><p>其中，r是动作嵌入向量，u是历史摘要向量，x是句子，y是树的向量表示。历史向量由三个 RNN 向量拼接而成。</p><h1 id="Semantic-Role-Labeling-语义角色标注"><a href="#Semantic-Role-Labeling-语义角色标注" class="headerlink" title="Semantic Role Labeling 语义角色标注"></a>Semantic Role Labeling 语义角色标注</h1><p>语义角色标注是对句子中的语义成分进行有逻辑性的标注的技术。举例，对于“发送”这个动作而言，有如下要素：</p><ol><li>发送者</li><li>接收者</li><li>发送的东西</li><li>来源(位置)</li><li>目标(位置)</li></ol><p>SRL 的目标是将这些逻辑要素进行标注，以进行进一步处理。</p><p>一个语义可以有多种表述方式，比如说如下句子：</p><blockquote><p>The marmable was shipped by Sam<br>Sam shipped the marmable</p></blockquote><p>他们在语义上的表述是相同的，但是拥有不同的语法结构。</p><p>我们用帧语义来定义语法结构：</p><ol><li>帧用来描述原型状况(prototypical situation?)</li><li>使用帧唤起元素(frame evoking element)来唤起帧</li><li>帧包含若干帧元素，包括语义角色(sem roles)与参数(arguments)</li></ol><p>我的理解是，帧就是将句子分割成有机的语义元素的东西。</p><p>帧语义有如下性质：</p><ol><li>提供了浅层语义的分析(非情态，范围)</li><li>在 universal 与 specific 之间的粒度</li><li>能够在大多数语言中泛化良好</li><li>能够作为许多其他应用的基础</li></ol><p>我们通常使用 proposition bank 里提供的分类来标注。</p><p>一般而言，传统 SRL 包含如下步骤：</p><ol><li>训练语料的解析</li><li>将帧元素与句子成分作匹配</li><li>从解析树中抽取特征</li><li>根据特征，训练概率模型</li></ol><h2 id="Deep-bi-RNN-LSTM-方案"><a href="#Deep-bi-RNN-LSTM-方案" class="headerlink" title="Deep bi-RNN/LSTM 方案"></a>Deep bi-RNN/LSTM 方案</h2><p>输入：已经标记了谓词的句子</p><p>输出：对句子的帧语义标注</p><p>由于 SRL 是一个序列标注任务，我们同样可以使用 RNN/LSTM。<br>目前应用的比较广的技术是直接使用深度双向RNN/LSTM进行端到端的语义标注。</p><ul><li>不使用外部语法信息</li><li>不需要额外的帧对应步骤</li><li>不需要特征工程</li></ul><p>给定输入句子，我们设计一个输出序列的概率模型：</p><script type="math/tex; mode=display">P(y|x) =  \prod_{i=1}^{|x|} P(y_i|x)</script><p>y 是标注序列， x 是输入序列，输入的特征为当前的单词与谓词的位置。</p><p>网络架构如下图：</p><p><img src="/images/SRL-bi-rnn.png" alt></p><p>该图的 DB-LSTM 是 LSTM 的一种变种。 bidirectional LSTM 一般包含两层，同相同的输入与输出层连接，对同一句子进行双向处理。这里的 bi-LSTM 有所不同，第一层 LSTM 的输出是第二层反向 LSTM 的输入，这实际上将多重 LSTM 堆叠起来形成了深度学习网络。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Dependency-Parsing&quot;&gt;&lt;a href=&quot;#Dependency-Parsing&quot; class=&quot;headerlink&quot; title=&quot;Dependency Parsing&quot;&gt;&lt;/a&gt;Dependency Parsing&lt;/h1&gt;&lt;p&gt;传统的语法模
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU: Seq2seq模型, Evaluation, Attention</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/07/nlu-3/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/07/nlu-3/</id>
    <published>2019-05-06T23:45:30.000Z</published>
    <updated>2019-10-22T09:22:20.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sequence-to-Sequence-Model"><a href="#Sequence-to-Sequence-Model" class="headerlink" title="Sequence-to-Sequence Model"></a>Sequence-to-Sequence Model</h1><p>首先我们来看一下语言模型与翻译模型的区别：语言模型是给定前面的序列，求解当前单词的概率分布；而翻译模型则不然，翻译模型是给定源语言序列，求解目标语言序列。如果用语言模型的方法对翻译模型建模如下：</p><script type="math/tex; mode=display">p(T|S) = \prod_{i=1}^T p(y_i|y_1, ... y_{i-1},x_1, ... x_m)</script><p>会碰到这些问题：</p><ol><li>我们不关心源语言单词的概率</li><li>源语言与目标语言的词汇表不同，或者有不同的结构</li></ol><p>因此，比起用一个 RNN 建模源语言-目标语言序列，我们通常选用两个 RNN 来对源语言与目标语言分别建模。</p><h2 id="Encoder-decoder-模型"><a href="#Encoder-decoder-模型" class="headerlink" title="Encoder-decoder 模型"></a>Encoder-decoder 模型</h2><p><img src="/images/encoder-decoder.png" alt></p><p>模型分成 encoder 与 decoder 两个部分。首先是 encoder, 他的目的是对源语言句子进行建模。encoder 的输出可以看做是源语言句子的 “摘要”，这个“摘要”我们使用一个中间向量来进行表示。在对多语言进行学习的情况下，这个向量代表着语言无关的语义。在 decoder 端，中间向量作为decoder的第一个输入，用于生成第一个单词；之后使用前面单元生成的词汇与历史信息作为输入，递归的生成整个句子。</p><p>使用中间向量来连接 encoder 与 decoder 会出现问题：随着句子长度的增加，固定长度的中间向量表现会变差，一种解决方法是将句子倒过来处理；但是更加有效的优化方法是引入注意力机制（attention）。</p><p>注意力机制的作用方式如下：我们使用一层前馈网络来计算源句中每一个单词的权重，然后将每一步 encoder 的输出加权形成一个新的输入向量。对于 decoder 的每一个单词，注意力权重都不同，这样一来我们就可以将源句中的信息分配给比较适合的单词。</p><p>注意力机制带来了一个副产物：通过注意力，我们可以某种程度上完成词对齐的工作，但是由于信息流同样递归地流动，实际上不保证注意力机制能够完全替代词对齐。</p><p>注意力模型不仅限于处理 NLP 任务，对于视觉任务，e.g. 语义识别等也有一席之地。</p><p>Encoder-decoder 模型常见的应用有：</p><ol><li>给翻译打分: 计算目标句的概率</li><li>机器翻译</li></ol><h2 id="decoding-的一些技术"><a href="#decoding-的一些技术" class="headerlink" title="decoding 的一些技术"></a>decoding 的一些技术</h2><p>机器翻译实际上可以看做如下行为：在所有可能的目标语言句空间中，寻找出一个最合适，即概率最高的句子。但是，实际上我们不可能对如此庞大的空间进行搜索，所以我们在 decoding 的时候要采取若干近似手段。</p><p>其一，我们可以用从概率分布中采样，或者贪婪的选择当前概率最高的单词，直到生成完结符号。这种方法一般而言不会是最优解。<br>其二，我们使用束搜索 (beam search)来提升准确率。给定一个 beam size，我们每次搜索的时候，保留一定数量的候选单词，这些单词有着较高的评分或者概率，然后对于每一个候选词汇，我们都往后生成一系列的搜索束，直到搜索序列的长度到达 beam size， 贪婪地选择总体概率最高的序列，重复该过程直到结束。<br>其三，我们可以用 ensemble 的方式：训练若干个模型，然后在 decoding 的时候，通过投票的方式来选出最佳的翻译。基本的方法是平均(log)概率。一般而言，这些模型共享目标词汇表，以及历史的解码信息，但是训练的方式与网络架构都可以不同。</p><h1 id="NMT-的评价方法"><a href="#NMT-的评价方法" class="headerlink" title="NMT 的评价方法"></a>NMT 的评价方法</h1><p>NMT 质量的评价方法可以大致分为主观与客观两种。</p><p>Metrics:</p><ol><li>Adequacy: 翻译的句子是否与原句的意思相近？</li><li>Fluency: 翻译的句子在目标语言中是否通顺？</li></ol><p>Kappa 系数：</p><script type="math/tex; mode=display">K = \frac{p(A) - p(E)}{1 - p(E)}</script><p>p(A) 指评价者评价的概率， p(E)指随机评价的概率</p><p>提高一致性的方法还有：<br>将准确与流畅的评价分开，正则化分数，用一些 trick 将不靠谱的受试者剔除</p><p>其他需要评价的一些 criteria:</p><ol><li>speed </li><li>size</li><li>integration</li><li>customization </li></ol><h2 id="自动评价"><a href="#自动评价" class="headerlink" title="自动评价"></a>自动评价</h2><ul><li>目标：程序来评价翻译的质量</li><li>好处：成本低，易于调整，一致性好</li><li>基本操作：<ul><li>给定机翻输出</li><li>给定人类翻译</li><li>目标：比较两者相似度</li></ul></li></ul><p>precision(correct/output-length), recall(correct/reference-length), f-measure</p><p>word error rate: 最小编辑距离除以总长度</p><p>BLEU:</p><p>n-gram 覆盖率，计算长度为 1-4 的 ngram 的 precision。<br>首先，计算一个对过短翻译的惩罚：</p><p>BP = min(1, exp(1 - ref-length/out-length))</p><p>然后计算BLEU：</p><script type="math/tex; mode=display">BLEU = BP(\prod_{i=1}^4 precision_i)^{1/4}</script><p>如果没有 4-gram 能够匹配， BLEU 的值为0。一般而言, BLEU 是计算整个语料库的。</p><p>BLEU 同样可以用于计算有多个 reference 的翻译来测试 variability (多样性？)<br>n-grams 可以匹配任意的 reference, 但是计算的时候用最接近的 ref-length 做计算。</p><p>Meteor: 给予词干，同义词一定的分数，paraphrase 的使用也有分。</p><p>对 Metrics 的一些批评：</p><ol><li>无视相关的词</li><li>通常在本地水平测试，对更加广义的语法不加以考虑</li><li>分数可解释性不强</li><li>有的时候人工翻译会得到一个较低的 BLEU 分数</li></ol><p>对 Metrics 的评价体系：<br>与人工判断作比较，计算相关性</p><h1 id="Attention-的一些变种"><a href="#Attention-的一些变种" class="headerlink" title="Attention 的一些变种"></a>Attention 的一些变种</h1><p>首先，我们看一下多种的计算 attention 分数的方法(Luong et al., 2015)：</p><ol><li><script type="math/tex">h_t^T h_s</script> 点乘法计算分数</li><li><script type="math/tex">h_t^T W_a h_s</script> 一般形式</li><li><script type="math/tex">v_a^T tanh(W_a[h_t;h_s])</script> 向量拼接的形式：将源语言的输出向量 <script type="math/tex">h_t</script> 与上一个翻译的单词拼接起来</li></ol><h2 id="Condition"><a href="#Condition" class="headerlink" title="Condition"></a>Condition</h2><p>我们还可以把之前的决策纳入考虑(dl4mt-tutorial)：</p><ul><li><script type="math/tex; mode=display">s' = GRU_A(s_{i-1}, y_{i-1})</script></li><li><script type="math/tex; mode=display">c_i = ATT(C, s')</script></li><li><script type="math/tex; mode=display">s_i = GRU_B(c_i, s')</script></li></ul><p>其主要思路是：之前的 Attention 模型只与隐层 s 相关，但是真正决策的单词也会影响之后的概率。</p><h2 id="Guided-Alignment-Training-chen-et-al-2016"><a href="#Guided-Alignment-Training-chen-et-al-2016" class="headerlink" title="Guided Alignment Training(chen et.al., 2016)"></a>Guided Alignment Training(chen et.al., 2016)</h2><p>基本思路:</p><ol><li>使用外部的工具创建词对齐向量(word alignment)</li><li>如果多个源单词对应一个目标单词，需要进行正则化，以使得<script type="math/tex">\sum_j A_{ij} = 1</script></li><li>对训练时的目标函数作如下更改：<script type="math/tex">H(A, \alpha)=-\frac{1}{T_y} \sum_{i=1}^{T_y}\sum_{j=1}^{T_x} A_{ij}log\alpha_{ij}</script>。该函数的目的有二：一，同之前一样最小化交叉熵；二，最小化注意力向量与外部的词对齐向量之间的差异</li></ol><h2 id="Incorporating-Structural-Alignment-Biases-Cohn-et-al-2016"><a href="#Incorporating-Structural-Alignment-Biases-Cohn-et-al-2016" class="headerlink" title="Incorporating Structural Alignment Biases(Cohn et.al.,2016)"></a>Incorporating Structural Alignment Biases(Cohn et.al.,2016)</h2><p>通过统计词对齐算法，我们发现对齐本身会有一定的偏差(bias)。</p><ol><li>position bias: 相关的位置对对齐提供了大量的信息</li><li>fertility/coverage: (生育/覆盖) 一些词会在目标句中生成多个词来保证所有的源单词都会被覆盖到</li><li>bilingual symmetry: 双语对称性, 互为源语言与目标语言的两种语言，词对齐是对称的</li></ol><p>针对 position bias, 我们需要在创建注意力模型的时候输入位置信息。对于非递归架构来说这种方法更加有效(CNN?)<br>position encoding 的方法也有各式各样的。</p><p>针对 fertility/coverage， 我们的想法是对于目标语言，同样有 <script type="math/tex">\sum_i^{T_y}\alpha_{ij} \simeq 1</script>，即同注意力的 softmax 类似，同一源单词的注意力权重在目标句上的和应当近似于1。因此，我们可以用这样的正则项来作为训练的目标函数:<script type="math/tex">\sum_j^{T_x}(1-\sum_i^{T_y}\alpha_{ij})^2</script>。这种优化方法由于有一个预先假设，但是在实际情况中这种假设不一定成立，一种更加 general 的方式是，使用一个神经网络来学习单词的”生育”: <script type="math/tex">f_j = N\delta(W_jh_j)</script>，然后用 fertility 项来替代原式中的 1。</p><p>针对 bilingual symmetry, 我们在训练的时候可以添加一项路径奖励，来奖励那些能够生成具有对称性的注意力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sequence-to-Sequence-Model&quot;&gt;&lt;a href=&quot;#Sequence-to-Sequence-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequence-to-Sequence Model&quot;&gt;&lt;/a&gt;Sequence
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU: Perceptron, Neural Networks, RNN/LSTM</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/05/nlu-2/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/05/nlu-2/</id>
    <published>2019-05-05T07:12:16.000Z</published>
    <updated>2019-10-22T09:22:25.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><p>感知机是神经网络最基本的元件之一， 是一个加权分类器。<br>基本的感知机算法如下：</p><script type="math/tex; mode=display">u(\mathbf{x}) = \sum_{i=1}^{n} w_i x_i</script><p>当<script type="math/tex">u(x)</script>大于某个阈值时，输出1，反之输出0。</p><h2 id="逻辑门实现"><a href="#逻辑门实现" class="headerlink" title="逻辑门实现"></a>逻辑门实现</h2><p>我们可以用最基本的感知机来实现一些基本的逻辑门。</p><p>首先我们来看一下 AND 函数的真值表：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 AND x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table></div><p>AND 函数是线性可分的：在一个二维空间里，横坐标为x1, 纵坐标为 x2， 我们可以用一条直线来将空间分为两部分，分别表示激活状态与非激活状态。同理，OR 函数也是线性可分的。</p><p>我们使用如下函数来定义我们的感知机加权计算u(x)：</p><script type="math/tex; mode=display">u(x) = 0.5 x_1 + 0.5 x_2</script><p>激活的阈值<script type="math/tex">\theta=1</script>,当函数值小于1时，不激活；反之大于等于1时激活。</p><p>同理，我们可以根据如下的真值表写出 OR 的定义：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 OR x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table></div><p>感知机权重：</p><script type="math/tex; mode=display">u(x) = 0.5 x_1 + 0.5 x_2</script><p>阈值为大于0， 小于等于0.5的任意实数。实际上，我们可以将阈值从不等式<script type="math/tex">u(x)\leq \theta</script>的右边拿到左边，这样一来我们就可以使用一个截距项<script type="math/tex">b=-\theta</script>来处理不同情况下的阈值。</p><p>XOR 是另外一种情况：我们在空间中没有办法使用线性的分割线将空间分为两个部分。用感知机的解决方案是：复合多层感知机(MLP)，来应对非线性的问题。首先，我们写出 XOR 的真值表：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 XOR x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table></div><p>注意到如下事实：x1 XOR x2 = ((NOT x1) AND x2) OR (x1 AND (NOT x2))，可以写真值表验算一下。<br>我们实际上可以将 XOR 操作分解为两步： 第一步计算两个 AND， 第二部计算上一步的OR。NOT操作，我们可以用负的权重来实现。</p><p>由此，我们可以写出如下的感知机分解式子：</p><script type="math/tex; mode=display">u_1(\mathbf{x}) = -0.5 x_1 + 0.5 x_2 -1 > 0u_2(\mathbf{x}) = 0.5 x_1 - 0.5 x_2 -1 > 0v(\mathbf{u}) = 0.5 u_1 + 0.5 u_2 - 0.4 > 0</script><p>这是一个两层的感知机，其结构如下图：</p><p><img src alt="感知机XOR"></p><p>另一种分解方式为： x1 XOR x2 = (x1 NAND x2) AND (x1 OR x2)</p><h2 id="参数的学习"><a href="#参数的学习" class="headerlink" title="参数的学习"></a>参数的学习</h2><p>我们可以通过梯度下降法来学习参数：</p><script type="math/tex; mode=display">\begin{align}w_i &= w_i + \eta \delta w_i \\  &= w_i + \eta(t - o)x_i\end{align}</script><p>即：损失函数相对于参数<script type="math/tex">w_i</script>的导数。</p><p>MLP 的一般使用思路如下：</p><ol><li>初始化 MLP 参数</li><li>给定训练特征x，然后计算 MLP 的输出 y</li><li>将 y 与正确的目标 t 作对比，得到误差量 (error quantity)</li><li>根据误差量，使用梯度下降法对 MLP 的参数进行修正</li><li>重复2-5，直至收敛</li></ol><p>我们的学习目标一般是最小化损失函数，常用的损失函数有均方根误差(Mean Square Error, MSE)</p><script type="math/tex; mode=display">E(\mathbf{w}) = \frac{1}{2N} \sum_{p=1}^{N}(t - o)^2</script><h1 id="基于-MLP-的语言模型"><a href="#基于-MLP-的语言模型" class="headerlink" title="基于 MLP 的语言模型"></a>基于 MLP 的语言模型</h1><p>n-gram 角度的语言模型：输入为<script type="math/tex">w_{i-n+1}..w_i</script>，输出为单词<script type="math/tex">w_i</script>的概率，我们用一个函数来描述:<script type="math/tex">f:V^n \rightarrow \mathbb{R}_{+}</script>。<br>函数角度的语言模型：输入为 prefix 字符串，输出为一个概率分布: <script type="math/tex">f:V^{n-1} \rightarrow (V \rightarrow \mathbb{R}_{+})</script>。</p><p>我们使用 one-hot 编码来编码单词，再将单词按照顺序连接成一个大的向量，这样子我们就可以将单词表示为向量形式。同理，我们的概率分布也是向量形式。这样，我们在数学上就可以用 MLP 来处理，因为 MLP 在前向的运算实际上就是矩阵乘法的形式。在输出层，我们使用 softmax 函数来处理每个单词的概率分布。</p><p>对于 n-gram 模型， 我们通常可以使用如下的神经网络进行处理:</p><p><img src="/images/ngram-nn-lm.png" alt></p><p>首先，对每个 one-hot 向量，使用矩阵 C 进行压缩编码得到词嵌入(embedding)；然后使用矩阵 W 来计算 ngram 的完整表示向量，最后，矩阵 V 被用来计算最终的概率分布。</p><h1 id="Recurrent-Networks"><a href="#Recurrent-Networks" class="headerlink" title="Recurrent Networks"></a>Recurrent Networks</h1><p>引入递归神经网络的原因有以下几点：</p><ol><li>n-gram 模型对上下文信息利用不充分</li><li>上述的 MLP 模型虽然能够对比较广的上下文进行建模，但是其大小是固定的</li><li>与此相反，语言学的角度来说，需要的上下文范围实际上很大</li></ol><p>基于这些原因，人们提出了 RNN 模型：</p><p><img src="/images/rnn-simple.png" alt></p><p>这是一个最简单的，回看一步的 RNN 网络结构。向量 x 是时刻 t 的输入，这里我们可以理解为 one-hot 的单词。向量 y 是时刻 t 的输出向量，向量 s 是时刻 t 的隐层，对于每个时刻而言，输入都是上一时刻的 s 与 x 向量拼接后的向量。<br>我们可以看出，该网络递归的利用了所有以前的信息，从而，我们在使用 RNN 建立语言模型的时候不再需要马尔科夫假设。</p><p>训练网络的过程如下：</p><ol><li>首先，我们随机初始化参数矩阵 U, V, W, 隐层随机初始化。</li><li>对于任意的时刻 t, 我们都将其作为普通的 MLP进行处理，只不过我们的输入是 x 与 时刻t-1 的 s 的复合</li><li>我们定义训练的目标是给定历史序列，计算下一个单词的概率分布，所以错误信号定义为 <script type="math/tex">w_{t} - y(t)</script>，其中 <script type="math/tex">w_{t}</script> 是 one-hot 编码。</li><li>我们使用如上方法，来在给定的训练集上训练网络。</li></ol><h2 id="Backpropagation-through-time-BPTT"><a href="#Backpropagation-through-time-BPTT" class="headerlink" title="Backpropagation through time(BPTT)"></a>Backpropagation through time(BPTT)</h2><p>再进一步，我们需要将我们的反向传播前推至任意长度，以实现对历史上下文信息的利用。实际上，我们可以把这个过程看做是当前的单词的概率是之前 n 个单词的函数，所以反向传播需要对以前的时刻的参数进行更新。从另一个角度来看，我们可以将 BPTT 版本的 RNN 看作是一个沿着时间轴展开的大型 MLP。</p><p><img src="/images/bptt.png" alt></p><p>首先，我们定义一下公式：</p><script type="math/tex; mode=display">s_t = Vx_t + U s_{t-1}\\y_t = softmax(W s_t)</script><p>首先，令向量<script type="math/tex">a = W s_t</script>对于 softmax 函数而言，对第 i 分类的第 j 个输入我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = \frac{\frac{\partial e^{a_i}}{\sum_k e^{a_k}}}{\partial a_j}</script><p>注意，i 与 j 不一定相等。令<script type="math/tex">\Sigma = \sum_k e^{a_k}</script>使用除法求导法则我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = \frac{\frac{\partial e^{a_i}}{\partial e^{a_j}}\Sigma - e^{a_i} \frac{\partial \Sigma}{\partial e^{a_j}}}{\Sigma^2} =\frac{\frac{\partial e^{a_i}}{\partial e^{a_j}}}{\Sigma} - \frac{e^{a_i}}{\Sigma}\frac{e^{a_j}}{\Sigma}</script><p>当 <script type="math/tex">i=j</script>的时候，第一项偏导数为 <script type="math/tex">e^{a_i}</script>，从而我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_i} = y_i(1 - y_i)</script><p>反之，第一项偏导数为0，我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = -y_j y_i</script><p>使用复合函数求导法则，我们可以得到 W, s 关于 a 的偏导：</p><script type="math/tex; mode=display">\frac{d a}{dW} = \mathbf{s}^T \nabla \mathbf{a}\\\frac{d a}{ds} = \nabla \mathbf{a}^T \mathbf{W}</script><p>而对于 s 而言，它又可以传播到前一个 s’:</p><script type="math/tex; mode=display">\frac{d s'}{d s} = \nabla \mathbf{s}^T U</script><p>以此类推。</p><p>我们可以任意地回溯时间直至序列的开头，但是由于我们通常采用诸如 sigmoid, tanh 之类的激活函数，导数在数值上往往小于1，从而带来了梯度消失的风险：反向传播会以指数形式放大/缩小导数，由于反向传播过远，导致梯度在较长的时刻之前接近于0，从而无法传播。RNN 的中远时刻的信息，要么非常强，要么非常弱。</p><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>为了解决梯度消失的问题，LSTM 作为 RNN 的一个变种被提出。LSTM 的基本思路是：保持很长一段时间的信息不变，从而系统可以从较远的输入中获取信息。</p><p>与普通的 RNN 不同， LSTM 有一个额外的记忆层来存储长时间的记忆。LSTM 由如下四个元件组成：</p><ol><li>Input gate： 用于控制输入是否被存储到记忆中</li><li>Output gate: 用于控制当前激活的记忆向量是否传递至输出层</li><li>Forget gate: 控制记忆向量是否清零</li><li>Memory cell: 存储当前的记忆向量</li></ol><p><img src="/images/lstm.png" alt></p><p>图中，<script type="math/tex">\delta</script> 表示 sigmoid 函数， 加号表示向量拼接，乘号表示向量乘法。h 表示输出。<br>从左到右：</p><ol><li>首先，隐层向量与输入向量在遗忘门经过sigmoid函数转化为 0-1 区间的数，接近 0 则表示遗忘，接近 1 则表示记忆。<br>这一步是对记忆层进行遗忘，遗忘的依据是当前的输入向量与隐层</li><li>第二步是输入门，输入门由一个 sigmoid 与一个 tanh 层组成，对当前的输入(隐层向量与输入向量的拼接)进行筛选，tanh 与 sigmoid 分别做决策，然后 pointwise multiplication 综合两者，都重要的记忆留下，其余筛去。</li><li>从输入门往上，将经过遗忘的记忆向量与经过输入门筛选的向量进行 pointwise addition， 激活/强化当前的记忆</li><li>第三步是输入门右边的输出门，由一个 sigmoid 激活原来的隐层向量与输入向量的拼接，由一个 tanh 处理记忆，然后两者point-wise相乘，得到当前时刻的输出。这个输出同时会保存，作为下一时刻的隐层输入。</li></ol><p>LSTM 用于解决梯度消失的思路是：记忆层的传递本质是一个线性函数，所以在这个层面没有梯度消失的问题,导数可以一直传递下去。对于控制门而言，它们能够随着训练学习到何时应当开门激活，何时应当屏蔽门的输入。这种设计提升了网络的复杂度，也能更精细地控制历史信息。</p><p>若干数学推导参考<a href="https://www.zhihu.com/question/44895610" target="_blank" rel="noopener">https://www.zhihu.com/question/44895610</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Perceptron&quot;&gt;&lt;a href=&quot;#Perceptron&quot; class=&quot;headerlink&quot; title=&quot;Perceptron&quot;&gt;&lt;/a&gt;Perceptron&lt;/h1&gt;&lt;p&gt;感知机是神经网络最基本的元件之一， 是一个加权分类器。&lt;br&gt;基本的感知机算
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU:Introduction, Language Model</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/04/nlu-1/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/04/nlu-1/</id>
    <published>2019-05-04T13:13:03.000Z</published>
    <updated>2019-10-22T09:22:28.220Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Natural Language Understanding:</p><blockquote><p>Any computational problem that input is natural language, output is structured information of a computer can store/execute</p></blockquote><h2 id="Computational-Linguists-with-Deep-Learning"><a href="#Computational-Linguists-with-Deep-Learning" class="headerlink" title="Computational Linguists with Deep Learning"></a>Computational Linguists with Deep Learning</h2><blockquote><p>“Although current deep learning research tends to claim to<br>encompass NLP, I’m (1) much less convinced about the strength of the results, compared<br>to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision,<br>the way to go is to couple huge amounts of data with black-box learning architectures.”<br>Michael Jordan</p></blockquote><p>目前而言，深度学习在比较 high-level 的 NLP 任务中，没有能够如同在视觉领域一样大幅度减小错误率(e.g. 25-50%)，而且似乎只有在相对纯粹的信号处理中，应用深度学习能够大幅度提高性能。    </p><h2 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model:"></a>Language Model:</h2><p>我们把语言模型认为是一种关于字符串的概率模型：</p><ol><li>语音识别：给定语音信号为条件</li><li>机器翻译：给定另一种语言</li><li>自动补全：给定一句话的前面若干词<br>…</li></ol><blockquote><p>给定一个有限的词汇表$V$, 我们定义一个概率分布：<script type="math/tex">V^*\rightarrow\mathbb{R}_{+}</script></p></blockquote><ol><li>What is sample space? <script type="math/tex">V*</script></li><li>What might be some useful random variables? sentence, i.e. sequence of words, each word is variable</li><li>What constrains must <script type="math/tex">P</script> satisfy? sequence of words &lt;= product of prob of each word ,non-negative,sum to 1</li></ol><h3 id="N-gram-Model"><a href="#N-gram-Model" class="headerlink" title="N-gram Model"></a>N-gram Model</h3><p>我们令<script type="math/tex">w</script>为一个单词序列（句子），<script type="math/tex">|w|=n</script>表示这个序列的长度，<script type="math/tex">w_i</script> 表示第 i 个单词。这个序列的概率可以被<br>定义为(<script type="math/tex">W_i</script>表示单词在位置i处的随机变量)：</p><script type="math/tex; mode=display">\begin{align}P(w) &= P(w_1, w_2, ... w_{n}) \\     &= P(W_1 =w_1) \times \\     &~~~P(W_2 = w_2|W_1 = w_1) \times\\     &~~~ ...\\     &~~~ P(W_n = w_n|W_{n-1}=w_{n-1}...)\\     &= \prod_{i=1}^{n}P(W_i = w_i|W_{i-1}=w_{i-1}, W_{i-2}=w_{i-2}...W_1=w_1)\\     &= \prod_{i=1}^{n}P(w_i|w_{i-1},w_{i-2}...w_1)\\\end{align}</script><p>这个式子使用条件概率定义了一个在无限空间上的联合分布，可以取的句子长度为任意值，但是我们仍然可以用条件概率来定义这个分布，每一个都有着有限的采样空间。</p><p>n-gram 模型是基于马尔科夫假设定义的语言模型：它假定在 i 处的单词的概率分布仅由之前的 n 个单词所决定：</p><script type="math/tex; mode=display">P(w_i|w_{i-1}, w_{i-2} ... w_1) = P(w_i|w_{i-1}, w_{i-2} ... w_{i-n})</script><p>我们可以使用计数的方式来估计 n-gram 模型的分布：</p><script type="math/tex; mode=display">P(w_i | w_{i-1}, w_{i-2} ... w_{i-n}) = \frac{C(w_{i-n},...w_{i-1}, w_i)}{C(w_i)}</script><p>由于单词的分布往往有着长尾特性，我们使用加一平滑 (add-one smoothing) 或者加<script type="math/tex">\alpha</script>平滑 (add-alpha smooting) 来处理计数，来保证所有的样本空间上的概率都不为0:</p><script type="math/tex; mode=display">P(w_i | w_{i-1}, w_{i-2} ... w_{i-n}) = \frac{C(w_{i-n},...w_{i-1}, w_i) + \alpha}{C(w_i) + \alpha|V|}</script><p>其中当平滑系数<script type="math/tex">\alpha</script>取1的时候，为加一平滑。我们通常取一个相对数量级比较小的平滑系数来处理。其他的处理方法还有插值法等，暂时不讨论。</p><p>当我们有了一个语言模型后，我们可以根据之前的序列，使用最大似然(maximum likelihood)来推断下一个单词出现的概率:</p><script type="math/tex; mode=display">\hat{w}_{k+1} = \underset{w_{k+1}}{\operatorname{argmax}}P{w_{k+1}|w_1 ... w_k}</script><h3 id="Language-Model-in-translation"><a href="#Language-Model-in-translation" class="headerlink" title="Language Model in translation"></a>Language Model in translation</h3><p>n-gram 模型尽管是一种经典的语言模型，在翻译上，它不起作用:</p><p>给定英文序列<script type="math/tex">e =e_1,e_2...e_n</script>，预测中文序列<script type="math/tex">f=f_1,f_2...f_m</script>, 我们会发现 n-gram 模型会遗忘以前的内容，但是这些内容很显然对翻译是非常重要的。就算我们将序列穿插：<script type="math/tex">w = f_1 e_1 f_2 e_2 ...</script>， 仍然有两个问题：1. 语言的顺序不一定一一对应 2. 长度未必相等。为了解决这个问题，人们提出了<strong>对齐模型</strong> (word alignment model)</p><h4 id="IBM-Model-1"><a href="#IBM-Model-1" class="headerlink" title="IBM Model 1"></a>IBM Model 1</h4><script type="math/tex; mode=display">p(f, a|e) = p(I|J) \prod_{i=1}^{I} p(a_i | J) p(f_i |e_{a_i})</script><p>其中，f 表示目标语言，e 表示源语言， a 表示目标语言对应的对齐目标， I 表示目标语言句子的长度， J 表示源语言的句子长度。我们可以将其看做一个零阶的 HMM 模型:<script type="math/tex">p(a_i|J)</script> 可以看做是状态转换概率，<script type="math/tex">p(f_i|e_{a_i})</script> 可以看做是触发概率，由于模型的转换概率不基于历史记忆(前一个状态)，这个马尔科夫链是0阶的。这个模型基于如下假设：</p><ol><li>目标句中，每个单词都由原句的某个单词产生</li><li>对应关系作为隐变量 (latent variable)</li><li>给定对齐 a，翻译的决策是独立的</li></ol><p>注意这里我们对对齐的定义为：一个向量存储了位置的对应关系。对齐有如下几种关系：</p><ol><li>Reorder 顺序不同</li><li>Word dropping 某些单词不翻译</li><li>Word Insertion 原句中需要一个 null token 来表示目标句中不存在的对应单词的情况</li><li>one-to-many 一个单词对应多个目标单词</li><li>many-to-one 一个目标单词对应多个源单词</li></ol><p>举个例子：</p><blockquote><p>虽然 北 风 呼啸，但 天空 仍然 十分 清澈。<br>However, the sky remained clear under the strong north wind.</p></blockquote><p>(However-虽然， “,” - “,”， the-天空， sky-天空， remained-仍然, clear-(十分，清澈)， under-null， the-null， strong-呼啸， north-北， wind-风， “.”-“。”)</p><p>对齐向量为：(1, 5, 7, 7, 8, (9, 10), 0, 0, 4, 3, 2, 11)</p><p>我们模型的参数为：任意一组源语言-目标语言单词对的概率, 比如说<script type="math/tex">p(north|北)</script>，其期望一般用”北”与”north”对齐的次数与”北”与所有单词的对其次数的比值来表示。</p><p>我们可以用 EM 算法来训练对齐模型：</p><ol><li>随机初始化模型的参数</li><li>使用现有的参数计算每种对齐的概率 </li><li>使用期望，通过MLE计算新的参数</li><li>迭代以上两步直至收敛</li></ol><p>举例：</p><ol><li>大 房子 - big house</li><li>清理 房子 - cleaning house</li></ol><p>我们首先用均匀分布初始化参数，即源语言-目标语言对概率:</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>1/3</td><td>1/3</td><td>1/3</td></tr><tr><td>house</td><td>1/3</td><td>1/3</td><td>1/3</td></tr><tr><td>cleaning</td><td>1/3</td><td>1/3</td><td>1/3</td></tr></tbody></table></div><p>第一步E计算对齐的概率：</p><ol><li>大-big, 房子-house: 1/3 × 1/3 = 1/9</li><li>大-house, 房子-big： 1/3 × 1/3 = 1/9</li><li>清理-cleaning, 房子-house: 1/3 × 1/3  = 1/9</li><li>清理-house, 房子-cleaning: 1/3 × 1/3 = 1/9</li></ol><p>这一步，我们可以看出每种对齐的概率都是相等的1/2</p><p>第一步M, 我们需要根据给定的数据来计算参数矩阵。<br>首先，我们根据数据与对齐概率，计算每个单词对的期望</p><script type="math/tex; mode=display">\begin{align}p(f_i|e_i) &= \frac{\sum_a p(a|f, e) * C(f_i, e_i)}{\sum_a p(a|f, e) C(f_i, \cdot)} \\ &= \frac{\mathbb{E}[C(f_i, e_i)]}{\mathbb{E}[C(f_i, \cdot)]} \end{align}</script><p>归一化后我们得到</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>1/2</td><td>1/2</td><td>0</td></tr><tr><td>house</td><td>1/4</td><td>1/2</td><td>1/4</td></tr><tr><td>cleaning</td><td>0</td><td>1/2</td><td>1/2</td></tr></tbody></table></div><p>第二步，我们使用上一步得到的矩阵进行进一步的E:</p><ol><li>大-big, 房子-house: 1/2 × 1/2 = 1/4 =&gt; 2/3</li><li>大-house, 房子-big： 1/4 × 1/2 = 1/8 =&gt; 1/3</li><li>清理-cleaning, 房子-house: 1/2 × 1/2  = 1/4 =&gt; 2/3</li><li>清理-house, 房子-cleaning: 1/4 × 1/2 = 1/8 =&gt; 1/3</li></ol><p>然后进行M：</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>2/3</td><td>1/3</td><td>0</td></tr><tr><td>house</td><td>1/6</td><td>2/3</td><td>1/6</td></tr><tr><td>cleaning</td><td>0</td><td>1/3</td><td>2/3</td></tr></tbody></table></div><p>可以看出， 大-big， 房子-house, 清理-cleaning 的概率是递增的，也就是最后会收敛到三个1</p><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><p><a href="http://www.shuang0420.com/2017/05/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/" target="_blank" rel="noopener">http://www.shuang0420.com/2017/05/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Natural Language Understanding
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://MeowAlienOwO.github.io/1970/01/01/hello-world/"/>
    <id>http://MeowAlienOwO.github.io/1970/01/01/hello-world/</id>
    <published>1970-01-01T07:12:16.000Z</published>
    <updated>2019-10-23T04:00:00.897Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More <code>info</code>: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><pre><code class="lang-python">import randomfor i in range(0, 1, 0.1):  print(&quot;aaa&quot;)</code></pre><h3 id="Math-test"><a href="#Math-test" class="headerlink" title="Math test"></a>Math test</h3><p>inline <script type="math/tex">1 + 2</script></p><p>equation</p><script type="math/tex; mode=display">e = mc^2</script><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>即名路青细记但养<code>单行代码</code>运，是划从完究压军成，走因励精求器。 术志料后群龙史见带矿圆达，济处部安等一备实关节里，感命更影很计己育农后。 等放的采属建支值着写，复据青界接数越多，西越J少历蠢日便。</p><blockquote><blockquote><p>引用矿事受理么可及土极育，门响证议即完京区空，基了屈见邮葛豆旱。<br>带转山构毛九技电带风所求设已速议，准京走法族新A段美体除再次。<br>万干且月三需你空日油，处起飞完美入个直始，消这H用奇凝花本。<br>就位和动节直性战力观使音他，并市始步平建5护豆精识。 等各都于能口同家化公称很拉层使存，也律手反转分丽十条志都统个。<br>易清三要有成没计加社据即，当治参并西干组接六技总，质的霸以法持需受抢传。<br>大住身然干原常有条调，平京地声自论光整，适切建面确求抗器。</p></blockquote></blockquote><div class="table-container"><table><thead><tr><th>表头</th><th style="text-align:center">表头</th><th style="text-align:right">表头</th></tr></thead><tbody><tr><td>内容</td><td style="text-align:center">内容</td><td style="text-align:right">内容</td></tr><tr><td>内容</td><td style="text-align:center">内容</td><td style="text-align:right">内容</td></tr></tbody></table></div><ul><li>列表<ul><li>无序列表</li><li>无序列表</li><li>无序列表</li></ul></li></ul><ul><li>列表<br>1.有序<br>2.有序嵌套</li><li>无序列表<ul><li>无序列表</li></ul></li></ul><ol><li>列表</li><li>列表<br>1.有序<br>2.有序嵌套</li><li>有序嵌套<ul><li>无序嵌套</li><li>无序嵌套</li><li>无序嵌套</li></ul></li></ol><p>合事速电没争住毛平交，电<em>养员</em>社林所<strong>低一证</strong>，军实<strong>A天眼确县</strong>手。 真写<del>条的空</del>第开集，<em>花至</em>处和把必各，山部束事断求攻。 算置作难小提千并七品中展二六劳务器，整别非五表查音斯否钉断豆见凝行。 线决样处酸市书节安军去，么铁手政中京转什动，之义届于医志杠算毛。</p><p>就响争就办书适，些带包统工提，号杏第即派。 至力即出能八组石角何需，同达由意题红地去日品，自例豆长抗9题抗法。 造极革什权各或毛太，精难革验计条众，影认隶针利扮须。 过龙什太今油元发式圆响，层务图较三强低利小理口，事利刷近号述年共花。 类例片干九民众到验发，无二内改出S找知一。</p><p>她间路着任清众明，样或出标式史，前区I圆通水。 济取当位使体象引家，传全名矿自音年圆，农或T风当美任。 速九元向即存办统交，派米认准面来论，始机K基志据存。</p><p>多究话准眼层造量文目存特，总元品增习适Q任级制教。 此体着管情斗今，石了期政飞打物，集T却集高。 应放在周支社参心越所，劳众共华图命例业个率，着进丽XG光目许。 江六决价东素技员起复，议采交原切实信，提第录杨别克快攻。</p><p>厂算量众由议转车习，些京需据识百能活，和究U速政几处。 代性权机万意型太式需质，你法器以Q覆Y取。</p><p>算来作维百快调效去识，称也习备行收想作式，真方T院想至信体。 的个流解斗从需家行，者花己安改情想，区他孤做伸劳线。</p><p>边能前在条毛拉地月，个头复和不长主，放济H六张抗元。 是听利照离应选些，类学件便对管，变设6枝V级。 它片集地角采技子议业办流，资风花建她达准且展义解，生头录确把又坟员千身。</p><p>变四复合交音眼太三，同口下划部美，进内G极酸护打。 铁技元意指市劳收小主南于边先保成，位义近集六长做一6见志生习面照己。 议内和世新即看律相，名必体飞少铁确状，生称D何族陕长。 越山地系条院细，三大果严开决，同G飞即如。 据六开音流识今细变社，新全这率化或参化安，据划E命线得的为。 高算算究只图空里影下，有文天身正争治记，做没更角才系设苍。</p><p>器地对发算作实例，受此二青好通感济，这H佣无励政。</p><p>史都太定这对可类特，部说以每传安持进，大也医真第位公。 风低向包只身部中济，价们面度往别素毛解，第表杨战估积或建。 因本这战商长织，除住市近再，样Q方所杨。 出铁使办工给打具海派科，组压识存边9号本手邮。 老族为全便个展应领，示影面色消之身众少走，快位孤杨扭分干均。 约义开思自革局报分交深表中，现命形众少清Z油画油杜。 系话在界色，至本导，机辰造。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="test" scheme="http://MeowAlienOwO.github.io/categories/test/"/>
    
    
      <category term="test" scheme="http://MeowAlienOwO.github.io/tags/test/"/>
    
  </entry>
  
</feed>
