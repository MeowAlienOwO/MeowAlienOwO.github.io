<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>the Garden of Sinners</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://MeowAlienOwO.github.io/"/>
  <updated>2019-10-22T09:23:08.730Z</updated>
  <id>http://MeowAlienOwO.github.io/</id>
  
  <author>
    <name>MeowAlien喵星人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://MeowAlienOwO.github.io/21970/01/01/hello-world/"/>
    <id>http://MeowAlienOwO.github.io/21970/01/01/hello-world/</id>
    <published>+021970-01-01T07:12:16.000Z</published>
    <updated>2019-10-22T09:23:08.730Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More <code>info</code>: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><pre><code class="lang-python">import randomfor i in range(0, 1, 0.1):  print(&quot;aaa&quot;)</code></pre><h3 id="Math-test"><a href="#Math-test" class="headerlink" title="Math test"></a>Math test</h3><p>inline <script type="math/tex">1 + 2</script></p><p>equation</p><script type="math/tex; mode=display">e = mc^2</script><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>即名路青细记但养<code>单行代码</code>运，是划从完究压军成，走因励精求器。 术志料后群龙史见带矿圆达，济处部安等一备实关节里，感命更影很计己育农后。 等放的采属建支值着写，复据青界接数越多，西越J少历蠢日便。</p><blockquote><blockquote><p>引用矿事受理么可及土极育，门响证议即完京区空，基了屈见邮葛豆旱。<br>带转山构毛九技电带风所求设已速议，准京走法族新A段美体除再次。<br>万干且月三需你空日油，处起飞完美入个直始，消这H用奇凝花本。<br>就位和动节直性战力观使音他，并市始步平建5护豆精识。 等各都于能口同家化公称很拉层使存，也律手反转分丽十条志都统个。<br>易清三要有成没计加社据即，当治参并西干组接六技总，质的霸以法持需受抢传。<br>大住身然干原常有条调，平京地声自论光整，适切建面确求抗器。</p></blockquote></blockquote><div class="table-container"><table><thead><tr><th>表头</th><th style="text-align:center">表头</th><th style="text-align:right">表头</th></tr></thead><tbody><tr><td>内容</td><td style="text-align:center">内容</td><td style="text-align:right">内容</td></tr><tr><td>内容</td><td style="text-align:center">内容</td><td style="text-align:right">内容</td></tr></tbody></table></div><ul><li>列表<ul><li>无序列表</li><li>无序列表</li><li>无序列表</li></ul></li></ul><ul><li>列表<br>1.有序<br>2.有序嵌套</li><li>无序列表<ul><li>无序列表</li></ul></li></ul><ol><li>列表</li><li>列表<br>1.有序<br>2.有序嵌套</li><li>有序嵌套<ul><li>无序嵌套</li><li>无序嵌套</li><li>无序嵌套</li></ul></li></ol><p>合事速电没争住毛平交，电<em>养员</em>社林所<strong>低一证</strong>，军实<strong>A天眼确县</strong>手。 真写<del>条的空</del>第开集，<em>花至</em>处和把必各，山部束事断求攻。 算置作难小提千并七品中展二六劳务器，整别非五表查音斯否钉断豆见凝行。 线决样处酸市书节安军去，么铁手政中京转什动，之义届于医志杠算毛。</p><p>就响争就办书适，些带包统工提，号杏第即派。 至力即出能八组石角何需，同达由意题红地去日品，自例豆长抗9题抗法。 造极革什权各或毛太，精难革验计条众，影认隶针利扮须。 过龙什太今油元发式圆响，层务图较三强低利小理口，事利刷近号述年共花。 类例片干九民众到验发，无二内改出S找知一。</p><p>她间路着任清众明，样或出标式史，前区I圆通水。 济取当位使体象引家，传全名矿自音年圆，农或T风当美任。 速九元向即存办统交，派米认准面来论，始机K基志据存。</p><p>多究话准眼层造量文目存特，总元品增习适Q任级制教。 此体着管情斗今，石了期政飞打物，集T却集高。 应放在周支社参心越所，劳众共华图命例业个率，着进丽XG光目许。 江六决价东素技员起复，议采交原切实信，提第录杨别克快攻。</p><p>厂算量众由议转车习，些京需据识百能活，和究U速政几处。 代性权机万意型太式需质，你法器以Q覆Y取。</p><p>算来作维百快调效去识，称也习备行收想作式，真方T院想至信体。 的个流解斗从需家行，者花己安改情想，区他孤做伸劳线。</p><p>边能前在条毛拉地月，个头复和不长主，放济H六张抗元。 是听利照离应选些，类学件便对管，变设6枝V级。 它片集地角采技子议业办流，资风花建她达准且展义解，生头录确把又坟员千身。</p><p>变四复合交音眼太三，同口下划部美，进内G极酸护打。 铁技元意指市劳收小主南于边先保成，位义近集六长做一6见志生习面照己。 议内和世新即看律相，名必体飞少铁确状，生称D何族陕长。 越山地系条院细，三大果严开决，同G飞即如。 据六开音流识今细变社，新全这率化或参化安，据划E命线得的为。 高算算究只图空里影下，有文天身正争治记，做没更角才系设苍。</p><p>器地对发算作实例，受此二青好通感济，这H佣无励政。</p><p>史都太定这对可类特，部说以每传安持进，大也医真第位公。 风低向包只身部中济，价们面度往别素毛解，第表杨战估积或建。 因本这战商长织，除住市近再，样Q方所杨。 出铁使办工给打具海派科，组压识存边9号本手邮。 老族为全便个展应领，示影面色消之身众少走，快位孤杨扭分干均。 约义开思自革局报分交深表中，现命形众少清Z油画油杜。 系话在界色，至本导，机辰造。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="test" scheme="http://MeowAlienOwO.github.io/categories/test/"/>
    
    
      <category term="test" scheme="http://MeowAlienOwO.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU：Open-vocabulary Models</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/08/nlu-5/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/08/nlu-5/</id>
    <published>2019-05-08T12:52:40.000Z</published>
    <updated>2019-10-22T09:22:09.766Z</updated>
    
    <content type="html"><![CDATA[<p>本文对 open-vocabulary model 进行一些复习跟梳理。<br><a id="more"></a></p><h1 id="Open-vocabulary-models"><a href="#Open-vocabulary-models" class="headerlink" title="Open-vocabulary models"></a>Open-vocabulary models</h1><p>一般而言，传统的 NLP 使用 one-hot 编码来对词汇进行处理。但是，通常而言自然语言的词汇表会非常庞大，这会直接导致我们的网络结构过于复杂，训练速度变慢。在翻译领域，这一表现尤其明显：首先，翻译的语料库往往包含大量的单词种类；而且在生成单词过程中，我们需要处理未见过的单词；另外名字，数字等通常有着简单的形式，但是他们自身是开放的单词类(open word classes)。因此，我们需要 open-vocabulary model 来解决庞大乃至无限大的词汇空间。</p><p>我们可以简单地将不在词汇表里的单词用一个 out of vocabulary (unk) 符号来表示。如果我们单从单词覆盖率的角度来看，需要用 unk 代替的稀有词汇其实不多 (5% for 50000 words)。但是这些词汇往往包含很多信息，比如说人名，地名，专有名词等等，而这些信息的缺失在翻译中是致命的。</p><hr><h2 id="Approximative-softmax"><a href="#Approximative-softmax" class="headerlink" title="Approximative softmax"></a>Approximative softmax</h2><p>在一个“激活”的词汇表子集上做 softmax 处理。在训练时，神经网络的输出端是原有的目标词汇表的一个子集，更新梯度的时候只处理在子集中正确的结果，然后在使用的时候在整个目标词汇表上计算单词的概率。这个方法能够使得使用更大规模的目标词汇表成为可能，但是有两个问题：1. 这个方法实际上不是 open-vocabulary 的；2. 对比较稀有的词汇，这个网络难以学习到正确的表示。</p><h2 id="Back-off-model"><a href="#Back-off-model" class="headerlink" title="Back-off model"></a>Back-off model</h2><p>我们在训练的时候依然使用 unk 来替代稀有的词汇，但是在系统生成 unk 后，对 unk 符号做对齐以映射到源语言的单词，然后用 back-off 方法(根据词汇表逐词翻译等)来对映射的单词进行翻译。这个方法同样有很多的问题：1. 很难处理一对多的关系；2. 难以处理词形(inflection)；3. 如果目标语言与源语言的字母表不同（中文，英文），需要一套转写名字的系统;4. 对齐方面，attention 不一定总是可靠的。</p><h2 id="Subword-NMT"><a href="#Subword-NMT" class="headerlink" title="Subword-NMT"></a>Subword-NMT</h2><p>Subword-NMT 的思路是，将单词拆开来成为 subword， 然后使用一些 NMT 的手段处理 sub-word。比如，我们如果要对特朗普做翻译：</p><ol><li>英文：<span style="color:red">T</span><span style="color:blue">rum</span><span style="color:green">p</span></li><li>中文：<span style="color:red">特</span><span style="color:blue">朗</span><span style="color:green">普</span></li><li>日文：<span style="color:red">ト</span><span style="color:blue">ラン</span><span style="color:green">プ</span></li></ol><p>而对于数字，专有名词等，我们在人工翻译的时候往往也是逐 subword 翻译。</p><p>我们希望 open-vocabulary 系统能够：</p><ul><li>用一个相对小的词汇表编码所有的词</li><li>能够在未出现过的单词上泛化良好</li><li>需要比较小的训练文本</li><li>相对的，有较好的翻译质量</li></ul><h3 id="Byte-pair-encoding-for-word-segmentation-字节串编码"><a href="#Byte-pair-encoding-for-word-segmentation-字节串编码" class="headerlink" title="Byte-pair encoding for word segmentation 字节串编码"></a>Byte-pair encoding for word segmentation 字节串编码</h3><p><a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">论文翻译</a><br><a href="https://www.aclweb.org/anthology/P16-1162" target="_blank" rel="noopener">论文</a></p><p>字节串编码的基本思路是使用一个新的、不在词汇表里的符号来替代原来字符串里经常出现的符号。<br>我们首先将单词表示为字母组成的序列，然后使用字节串编码对其进行压缩，压缩后的子字符就是我们的新的词汇表，我们可以使用超参数来控制词汇表的大小。</p><p>BPE 的算法简述如下：</p><ol><li>初始化符号表，先将所有的字母放到符号表中，然后将每个单词表示为符号序列。每个单词的结尾符号使用特殊的符号来标注，使得我们可以重建单词(e.g. 词尾的 w 与 词中的 w 不一样)</li><li>迭代地计算所有的符号对，将出现次数最多的符号对替换成为一个单独的符号，比如 <code>(&#39;A&#39;, &#39;B&#39;)</code> 会被换成 <code>(&#39;AB&#39;)</code> 这个操作每次相当于创建了一个 n-gram 的编码。同理，<code>(&#39;AB&#39;, &#39;C&#39;)</code> 会被换成<code>(&#39;ABC&#39;)</code>。迭代多次，可以将出现频率最高的 n-gram 编码成为新的符号</li><li>我们将新合并的符号放入原有的词汇表，最终的词汇表大小为原有的大小+合并操作次数</li></ol><p>举例，我们在原有的词汇表里面有如下单词，分别出现若干次:</p><blockquote><p>low:5 lower:2 newest:6 widest:3</p></blockquote><p>现有的词汇表如下：</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d</p></blockquote><p>观察我们的训练数据，我们可以发现 <code>(&#39;e&#39;, &#39;s&#39;)</code> 是出现次数最多的（9+3），于是我们使用 <code>(&#39;es&#39;)</code> 代替。迭代地，我们发现<code>(&#39;es&#39;, &#39;t&lt;/w&gt;&#39;)</code> 出现次数最多，于是我们也将其编码为 <code>(&#39;est&lt;w/&gt;&#39;)</code>，词汇表变为：</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d es est&lt;\w&gt;</p></blockquote><p>再一次迭代，最多的符号对变成了 <code>(&#39;l&#39;, &#39;o&#39;)</code>, 而不是 <code>(&#39;w&#39;, &#39;est&lt;\w&gt;&#39;)/(&#39;d&#39;, &#39;est&lt;\w&gt;&#39;)</code>。于是符号表变为:</p><blockquote><p>l o w&lt;\w&gt; w e r&lt;\w&gt; n s t&lt;\w&gt; i d es est&lt;\w&gt; lo</p></blockquote><p>使用 BPE 的编码方法比 back-off 会有大概 5% 左右的提升(BLEU分数), 而且在稀有词的表现更好。</p><p>如果我们将源语言与目标语言的 BPE 合并处理会有更好的一致性。而分开处理，有可能导致同样的名字在不同的语言中被不同地分割，从而影响一致性。</p><p>If we apply BPE independently, the same name may be segmented<br>differently in the two languages, which makes it<br>harder for the neural models to learn a mapping<br>between the subword units. </p><h2 id="Character-level-models"><a href="#Character-level-models" class="headerlink" title="Character-level models"></a>Character-level models</h2><ul><li>优点：<ul><li>open-vocabulary</li><li>不需要启发性/language specific 分割</li><li>神经网络可以从字符串序列学习</li></ul></li><li>缺点：<ul><li>序列长度增加导致训练与解码的耗时增加(2-8倍)</li></ul></li><li>open-questions:<ul><li>on which level represent meaning?</li><li>on which level attention operate?</li></ul></li></ul><h3 id="Hierarchical-model-back-off-revisited"><a href="#Hierarchical-model-back-off-revisited" class="headerlink" title="Hierarchical model: back-off revisited"></a>Hierarchical model: back-off revisited</h3><p>在单词层面，使用 UNK 替代， 对于每一个 UNK, 使用 character level model预测单词，基于单词的 hidden state。</p><ol><li>比查字典更灵活</li><li>比纯 character-level 翻译有更好的准确性</li><li>main model 与 back-off model 之间有独立的假设(Markov?)</li></ol><h3 id="Character-level-output"><a href="#Character-level-output" class="headerlink" title="Character-level output"></a>Character-level output</h3><ul><li>目标语言单词不需要分割，使用 character-level 表示(i.e. 字符序列)</li><li>encoder 使用 BPE-level 词汇表</li><li>EN -&gt; {DE, CS, RU, FI} 有较好表现</li><li>训练时间长</li></ul><h3 id="Character-level-input"><a href="#Character-level-input" class="headerlink" title="Character-level input"></a>Character-level input</h3><ul><li>输入层面不做分割，使用 character-level 的表示</li><li>使用 character-level lstm 来计算字符序列的向量形式</li></ul><h3 id="Fully-Character-level-NMT"><a href="#Fully-Character-level-NMT" class="headerlink" title="Fully Character-level NMT"></a>Fully Character-level NMT</h3><ul><li>跨单词处理</li><li>目标语言端使用 character-level RNN</li><li>源语言端：CNN + max-pooling</li></ul><p><img src="/images/fully-character-level-NMT.png" alt></p><h3 id="Large-capacity-character-level-NMT"><a href="#Large-capacity-character-level-NMT" class="headerlink" title="Large-capacity character-level NMT"></a>Large-capacity character-level NMT</h3><ul><li>训练深度 attentional LSTM encoder-decoder</li><li>浅层模型：BPE</li><li>深度模型：character-level 模型更好</li><li>主要的问题是训练时间过长</li><li>主要的挑战：在保证质量的前提下压缩表示方法</li></ul><h1 id="Morphology"><a href="#Morphology" class="headerlink" title="Morphology"></a>Morphology</h1><p>单词根据词法，会有不同的变化：</p><ul><li>词形(inflection)</li><li>case (格)</li><li>数量(number)</li><li>agreement </li></ul><p>对于英语来说:</p><ul><li>case</li><li>number </li><li>person</li><li>tense</li></ul><p>… 总之 词法很复杂</p><p>在书写系统中，我们定义 morpheme 为最小的对意义造成影响的单位</p><ul><li>free morpheme: 独立地作用, dog/house</li><li>bound morpheme: 只作为单词的一部分出现： un-, -ed, -ing</li></ul><p>对于中文而言，偏旁部首也作为 phoneme 存在</p><p>原则上来说，subword/character-level 模型能够学习到词法生成的规则，但是在实际上从文本中学习到词法是很困难的：</p><ul><li>subword 不一定是 morphoneme</li><li>有关联的词的形态不一定相似: stand-stood</li><li>词法上，语言所表示的含义可能不同</li></ul><p>另外，语言学的研究提供了很多词法规则可以直接拿来使用。一种最基本的方法是直接用字典形式+前后缀来替代输入中的一些变化的词(lemmatize)</p><h2 id="Morphology-on-Source-side"><a href="#Morphology-on-Source-side" class="headerlink" title="Morphology on Source side"></a>Morphology on Source side</h2><p>对于输入而言，我们可以将原本的单词向量替换成单词向量+字典形式向量(lemma)拼接后的向量。</p><h2 id="Morphology-on-Target-side"><a href="#Morphology-on-Target-side" class="headerlink" title="Morphology on Target side"></a>Morphology on Target side</h2><p>2-step 翻译: 首先用主系统预测字典形式的单词，然后将字典形式的单词使用基于统计的词形转换系统进行预测</p><p>2-step NMT：首先交叉地预测字典形式单词与词法类别，然后使用有限状态转换器(finite state transducer)来处理词形变换</p><h2 id="Neural-Inflection-Generation"><a href="#Neural-Inflection-Generation" class="headerlink" title="Neural Inflection Generation"></a>Neural Inflection Generation</h2><p>输入为单词的字典形与词法参数(时态，数量，格等)，使用 encoder-decoder 模型来预测转换后的单词</p><h2 id="Information-missing-in-source-case-study-Politeness"><a href="#Information-missing-in-source-case-study-Politeness" class="headerlink" title="Information missing in source, case study: Politeness"></a>Information missing in source, case study: Politeness</h2><p>在英语里，you 没有普通用法与尊敬用法的区别，但是在很多其他语言中，这样的区别存在。将英语翻译成别的语言的时候，我们需要判断何时应当用普通形，何时应当用敬体形。</p><p>解决问题的主要思路如下：<br>我们需要根据目标端的一些额外信息：在目标语言的语境中是否礼貌，将其作为一个额外的标注拼接在源语言序列后面。在测试的时候，我们可以控制输入是否礼貌。i.e. 对输入增加一些源语言中不存在的信息的标注。</p><p>同样的思路可以应用到时态，evidentiality，领域适配，对输出语言的控制等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文对 open-vocabulary model 进行一些复习跟梳理。&lt;br&gt;
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
      <category term="ml" scheme="http://MeowAlienOwO.github.io/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU：Dependency Parsing, RNNG, Semantic Role Labelling</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/07/nlu-4/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/07/nlu-4/</id>
    <published>2019-05-07T14:48:59.000Z</published>
    <updated>2019-10-22T09:22:15.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h1><p>传统的语法模型通常是成分式 (constituent structure) 的：把句子看作是不同成分的组合。</p><p>成分解析法通常将一个句子组合成一棵树，树的叶节点是终结符(单词)，其上所有的非叶节点表示语法结构。举例:</p><blockquote><pre><code>             Sentence                |  +-------------+------------+  |                          |</code></pre><p> Noun Phrase                Verb Phrase<br>      |                          |<br>    John                 +———-+————+<br>                         |                |<br>                       Verb          Noun Phrase<br>                         |                |<br>                       sees              Bill</p><p> 来源： <a href="https://zhuanlan.zhihu.com/p/31766972" target="_blank" rel="noopener">知乎</a></p></blockquote><p>但是成分解析在语义的方面比较弱。以上面的解析树为例，对于动词 sees， 我们关心的是它的主语跟宾语，但是在成分解析树中，主语 John 跟宾语 Bill 都被当成 NP 处理，语义上的关联性需要去处理许多的语法成分节点。因此，我们引入依赖(dependency)。</p><p>在 dependency parsing 中，每个单词都将根据它们之间的关系连接起来形成一张有向图，图的每一个节点都是每一条边都表<br>示依赖关系。比如上例的 dependency parsing 树可以画为：</p><blockquote><p>Root —-&gt; sees —- (obj)  —-&gt; Bill<br>           |</p><pre><code>       + ----- (subj) ---&gt; John</code></pre></blockquote><p>一棵 dependency parsign 树由如下部分组成：</p><ol><li>Head</li><li>Dependent</li><li>Label identifying H and D</li></ol><h2 id="Parsing-Techniques"><a href="#Parsing-Techniques" class="headerlink" title="Parsing Techniques"></a>Parsing Techniques</h2><p>对于 consistuent parsing 来说，通常使用一些诸如 CKY，shift-reduce parsing 的算法来进行 parsing。 但<br>是，这些方法往往以来临近的单词，而 dependency parsing 通常会依赖于非邻接的单词。我们通常用两种算法来进行这<br>种 parsing: maximum-spanning trees(MST) 以及 transition-based dependency parsing (MALT) 。</p><p>我们的目标是：在所有可能的依赖树(dependency parsing)空间内，寻找到一棵最优的树。<br>令<script type="math/tex">\mathbf{x} = x_1 ... x_n</script> 为输入的单词序列，<script type="math/tex">\mathbf{y}</script>为边(dependency edge)的集合。令<script type="math/tex">(i,j)\in\mathbf{y}</script>定义为单词 <script type="math/tex">x_i</script> 到 <script type="math/tex">x_j</script> 的边。<br>由于每个词都有唯一一个父节点，这个问题同 tagging problem 很类似：每一个词都用句子中的另一个词来做 tagging。如果我们进行对整棵树进行边分解(edge factorize)，定义树的得分为所有边的乘积，我们可以简单地直接选取每个词有着最高分数的边，只要我们满足所有的边最后能够组成一棵树的约束。</p><p>我们定义一条边的分数为一个函数 <script type="math/tex">s(i,j)</script>，依赖树的分数为：<script type="math/tex">s(\mathbf{x}, \mathbf{y})=  \sum_{(i,j)\in\mathbf{y}} s(i,j)</script>。我们 parsing 的目标是，给定句子 x, 选取有最高分数的树 y 。</p><h3 id="MST-与-Chu-Liu-Edmonds-CLE-算法"><a href="#MST-与-Chu-Liu-Edmonds-CLE-算法" class="headerlink" title="MST 与 Chu-Liu-Edmonds(CLE) 算法"></a>MST 与 Chu-Liu-Edmonds(CLE) 算法</h3><p>MST 算法可以表述成如下过程：</p><ol><li>我们假定一个全连接图 G，使得句子中每个单词都与其他单词互联，任意单词对都有一条边建立。</li><li>定义评分函数 s</li><li>在 G 中，找到一棵 MST，使得树有着最高的分数且包含了所有的 node，这棵树即为最佳的 parsing tree</li></ol><p>使用 Chu-liu-Edmonds 算法，能够在 <script type="math/tex">O(n^2)</script> 的时间内找到一棵 MST, 但是不保证这棵树是 projective 的。</p><p><img src="/images/MST_full_connect.png" alt></p><p>如上图，这是一张已经评完分的全连接图。<br>首先，对于图中的所有节点，选择一个有着最高分的入边(incoming edge)，形成新的图</p><p><img src="/images/MST_greedy.png" alt></p><p>如果这一步生成了一棵树，那么这棵树就是 MST，如果存在不连通的点则无解。第三种情况必然有环的存在。</p><p>对于环而言，我们将每个环“缩”成一个等效的点，并且重新计算这个等效点入边与出边的权重。</p><p>根据<a href="https://www.dawxy.com/article/%E6%9C%80%E5%B0%8F%E6%A0%91%E5%BD%A2%E5%9B%BE%EF%BC%8C%E6%9C%B1%E5%88%98%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3%E8%BD%AC/" target="_blank" rel="noopener">这篇文章</a><br>的解释，我们需要寻找环内的一条边打断。这个打断的过程等于消除了环上一个节点的入边，以次优的入边（最大或者最小）取代之,取<strong>打断后最优</strong>的选项，进行打断，从而消除环。如上图, John 与 saw 形成了一个环，我们要将这个环替换成一个等效的点，可以打断的选项有 (John, saw) 与 (saw, John)两条边。对于每一种打断方式而言，环会形成一条链。由于我们将环缩成了等效的点，外部的入边到环的权重变为：从该边指向的节点作为链的起始，直到链的结尾的所有入边的权重之和。</p><p>我们重复以上步骤直到只有一个点，然后按照我们的打断法展开成树，就可以得到 MST。</p><p>对于打分函数而言，我们可以选用设计好的SVM MIRA 等，最近使用神经网络的比较多。对于神经网络打分函数，我们定义函数值为给定句子<script type="math/tex">\mathbf{x}</script>与父节点<script type="math/tex">w_i</script>，连通子节点<script type="math/tex">w_j</script>的概率</p><script type="math/tex; mode=display">s(i,j) = p(w_j|w_i, \mathbf{x}) = \frac{exp(g(\mathbf{a}_j, \mathbf{a}_i))}{\sum_k exp(g(\mathbf{a}_k,\mathbf{a}_i))}</script><p>其中，向量<script type="math/tex">a</script>是通过前向-后向 RNN 生成的。函数 g 计算相关性分数，一个简单的实现为：</p><script type="math/tex; mode=display">g(a_j, a_i) = v_a^T tanh(U_a a_j + W_a a_i)</script><p>我们可以看出，相关性分数的形式与之前提到过的 Attention 的计算方式很像。</p><h1 id="RNN-based-Grammar"><a href="#RNN-based-Grammar" class="headerlink" title="RNN-based Grammar"></a>RNN-based Grammar</h1><p>自然语言的语法很大程度上是有层级的(hierarchial)。如果要使用 RNN 来对语法进行建模，我们需要对此进行考虑。</p><p>RNN语法(RNNG)的一般操作如下：</p><ol><li>使用 RNN 生成序列的符号(generate symbols sequencially)</li><li>周期性的使用一些“控制符号”(control symbols)来重写历史<ol><li>周期性的将序列“压缩”(compress)成一个单独的成分(constituent)</li><li>增强(augment?) RNN 使其能够将最近的历史压缩成为一个单独的向量(reduce)</li><li>RNN 通过历史（包含压缩后与非压缩的单词）来预测新的符号</li><li>RNN 同样要预测控制符号，来决定压缩成分的大小(size of constituent)</li></ol></li></ol><p>与普通的 RNN 相比， RNNG 会生成一棵语法树。</p><p><img src="/images/RNNG-order.png" alt></p><p>实际上 RNNG 相当于 RNN-based 的 probablistic pushdown automata(下推自动机)。<br>从这个角度考虑，RNN 需要对如下内容进行建模：</p><ol><li>之前的 terminate symbols（单词，标点等)</li><li>之前的操作</li><li>当前的 stack 状态。</li></ol><p>RNN 根据以上信息来预测下一步的操作(nt, gen, reduce 等)，如下图<br><img src="/images/RNNG-stack.png" alt></p><p>当最后整个句子被处理完毕后，最后在 stack 中剩下的内容就是整个句子语法树的向量形式。</p><p>这里我们比较关注的是 reduce 的处理方式。假设我们要对下句进行处理:</p><blockquote><p>(NP <em>The hungry cat</em>)</p></blockquote><p><img src="/images/RNNG-birnn.png" alt></p><p>如图，我们使用双向 RNN/LSTM 来处理。NP 定义为语法符号的向量，括号定义为当前序列的终结符，每一个符号都用向量进行表示。BI-RNN 从左到右与从右到左两个方向进行编码，然后将两个方向的信息综合起来输出成一个向量，用来表示部分语法元素。同样的，这个输出后的向量同样可以当成普通的符号进行处理，这样我们就可以递归的解析整棵语法树。</p><p>对于 terminal 而言，我们可以发现较前面的序列是后面的序列的前缀，所以可以很轻松地使用 LSTM 来处理。但是对于 stack 而言并没有这层关系。我们使用 stack RNN 来处理这样的栈：我们给 RNN 添加一个栈指示器，这样的 RNN 可以进行两步操作：</p><ol><li>push: 读取输入，将其放在栈最上面，与当前的栈指示器的位置拼接起来</li><li>pop: 将栈指针指向当前的父元素</li></ol><p><img src="/images/stacked-rnn.gif" alt></p><p>上图展示了 stack RNN 的运作方式。（问题：这里的连接是指的时间序列还是单纯的连接？我觉得应该是时间序列，在时间轴上可以将RNN看做一个链表）下面是 stack RNN 的实际运用。</p><p><img src="/images/stacked-rnn-example.gif" alt></p><p>到现在为止我们有 3 个 RNN：第一个 RNN 用于对句子进行建模，第二个 RNN 用于对语法符号序列进行建模，第三个 stack RNN 用于对 pushdown automata 的栈进行建模。每一次预测下一个符号的概率都是由三个 RNN 的信息进行综合。</p><p>具体的模型如下：</p><script type="math/tex; mode=display">p(\mathbf{x}, \mathbf{y}) = \prod_{t=1}^{|\mathbf{a(\mathbf{x}, \mathbf{y})}|}p(a_t|\mathbf{a}_{<t})\\\mathbf{y} = \prod_{t=1}^{|\mathbf{a(\mathbf{x}, \mathbf{y})}|} softmax(\mathbf{r}_{a_t}+b_{a_t})\\\mathbf{u} = tanh(\mathbf{W}[o_t; s_t; h_t]+c)</script><p>其中，r是动作嵌入向量，u是历史摘要向量，x是句子，y是树的向量表示。历史向量由三个 RNN 向量拼接而成。</p><h1 id="Semantic-Role-Labeling-语义角色标注"><a href="#Semantic-Role-Labeling-语义角色标注" class="headerlink" title="Semantic Role Labeling 语义角色标注"></a>Semantic Role Labeling 语义角色标注</h1><p>语义角色标注是对句子中的语义成分进行有逻辑性的标注的技术。举例，对于“发送”这个动作而言，有如下要素：</p><ol><li>发送者</li><li>接收者</li><li>发送的东西</li><li>来源(位置)</li><li>目标(位置)</li></ol><p>SRL 的目标是将这些逻辑要素进行标注，以进行进一步处理。</p><p>一个语义可以有多种表述方式，比如说如下句子：</p><blockquote><p>The marmable was shipped by Sam<br>Sam shipped the marmable</p></blockquote><p>他们在语义上的表述是相同的，但是拥有不同的语法结构。</p><p>我们用帧语义来定义语法结构：</p><ol><li>帧用来描述原型状况(prototypical situation?)</li><li>使用帧唤起元素(frame evoking element)来唤起帧</li><li>帧包含若干帧元素，包括语义角色(sem roles)与参数(arguments)</li></ol><p>我的理解是，帧就是将句子分割成有机的语义元素的东西。</p><p>帧语义有如下性质：</p><ol><li>提供了浅层语义的分析(非情态，范围)</li><li>在 universal 与 specific 之间的粒度</li><li>能够在大多数语言中泛化良好</li><li>能够作为许多其他应用的基础</li></ol><p>我们通常使用 proposition bank 里提供的分类来标注。</p><p>一般而言，传统 SRL 包含如下步骤：</p><ol><li>训练语料的解析</li><li>将帧元素与句子成分作匹配</li><li>从解析树中抽取特征</li><li>根据特征，训练概率模型</li></ol><h2 id="Deep-bi-RNN-LSTM-方案"><a href="#Deep-bi-RNN-LSTM-方案" class="headerlink" title="Deep bi-RNN/LSTM 方案"></a>Deep bi-RNN/LSTM 方案</h2><p>输入：已经标记了谓词的句子</p><p>输出：对句子的帧语义标注</p><p>由于 SRL 是一个序列标注任务，我们同样可以使用 RNN/LSTM。<br>目前应用的比较广的技术是直接使用深度双向RNN/LSTM进行端到端的语义标注。</p><ul><li>不使用外部语法信息</li><li>不需要额外的帧对应步骤</li><li>不需要特征工程</li></ul><p>给定输入句子，我们设计一个输出序列的概率模型：</p><script type="math/tex; mode=display">P(y|x) =  \prod_{i=1}^{|x|} P(y_i|x)</script><p>y 是标注序列， x 是输入序列，输入的特征为当前的单词与谓词的位置。</p><p>网络架构如下图：</p><p><img src="/images/SRL-bi-rnn.png" alt></p><p>该图的 DB-LSTM 是 LSTM 的一种变种。 bidirectional LSTM 一般包含两层，同相同的输入与输出层连接，对同一句子进行双向处理。这里的 bi-LSTM 有所不同，第一层 LSTM 的输出是第二层反向 LSTM 的输入，这实际上将多重 LSTM 堆叠起来形成了深度学习网络。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Dependency-Parsing&quot;&gt;&lt;a href=&quot;#Dependency-Parsing&quot; class=&quot;headerlink&quot; title=&quot;Dependency Parsing&quot;&gt;&lt;/a&gt;Dependency Parsing&lt;/h1&gt;&lt;p&gt;传统的语法模
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU: Seq2seq模型, Evaluation, Attention</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/07/nlu-3/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/07/nlu-3/</id>
    <published>2019-05-06T23:45:30.000Z</published>
    <updated>2019-10-22T09:22:20.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sequence-to-Sequence-Model"><a href="#Sequence-to-Sequence-Model" class="headerlink" title="Sequence-to-Sequence Model"></a>Sequence-to-Sequence Model</h1><p>首先我们来看一下语言模型与翻译模型的区别：语言模型是给定前面的序列，求解当前单词的概率分布；而翻译模型则不然，翻译模型是给定源语言序列，求解目标语言序列。如果用语言模型的方法对翻译模型建模如下：</p><script type="math/tex; mode=display">p(T|S) = \prod_{i=1}^T p(y_i|y_1, ... y_{i-1},x_1, ... x_m)</script><p>会碰到这些问题：</p><ol><li>我们不关心源语言单词的概率</li><li>源语言与目标语言的词汇表不同，或者有不同的结构</li></ol><p>因此，比起用一个 RNN 建模源语言-目标语言序列，我们通常选用两个 RNN 来对源语言与目标语言分别建模。</p><h2 id="Encoder-decoder-模型"><a href="#Encoder-decoder-模型" class="headerlink" title="Encoder-decoder 模型"></a>Encoder-decoder 模型</h2><p><img src="/images/encoder-decoder.png" alt></p><p>模型分成 encoder 与 decoder 两个部分。首先是 encoder, 他的目的是对源语言句子进行建模。encoder 的输出可以看做是源语言句子的 “摘要”，这个“摘要”我们使用一个中间向量来进行表示。在对多语言进行学习的情况下，这个向量代表着语言无关的语义。在 decoder 端，中间向量作为decoder的第一个输入，用于生成第一个单词；之后使用前面单元生成的词汇与历史信息作为输入，递归的生成整个句子。</p><p>使用中间向量来连接 encoder 与 decoder 会出现问题：随着句子长度的增加，固定长度的中间向量表现会变差，一种解决方法是将句子倒过来处理；但是更加有效的优化方法是引入注意力机制（attention）。</p><p>注意力机制的作用方式如下：我们使用一层前馈网络来计算源句中每一个单词的权重，然后将每一步 encoder 的输出加权形成一个新的输入向量。对于 decoder 的每一个单词，注意力权重都不同，这样一来我们就可以将源句中的信息分配给比较适合的单词。</p><p>注意力机制带来了一个副产物：通过注意力，我们可以某种程度上完成词对齐的工作，但是由于信息流同样递归地流动，实际上不保证注意力机制能够完全替代词对齐。</p><p>注意力模型不仅限于处理 NLP 任务，对于视觉任务，e.g. 语义识别等也有一席之地。</p><p>Encoder-decoder 模型常见的应用有：</p><ol><li>给翻译打分: 计算目标句的概率</li><li>机器翻译</li></ol><h2 id="decoding-的一些技术"><a href="#decoding-的一些技术" class="headerlink" title="decoding 的一些技术"></a>decoding 的一些技术</h2><p>机器翻译实际上可以看做如下行为：在所有可能的目标语言句空间中，寻找出一个最合适，即概率最高的句子。但是，实际上我们不可能对如此庞大的空间进行搜索，所以我们在 decoding 的时候要采取若干近似手段。</p><p>其一，我们可以用从概率分布中采样，或者贪婪的选择当前概率最高的单词，直到生成完结符号。这种方法一般而言不会是最优解。<br>其二，我们使用束搜索 (beam search)来提升准确率。给定一个 beam size，我们每次搜索的时候，保留一定数量的候选单词，这些单词有着较高的评分或者概率，然后对于每一个候选词汇，我们都往后生成一系列的搜索束，直到搜索序列的长度到达 beam size， 贪婪地选择总体概率最高的序列，重复该过程直到结束。<br>其三，我们可以用 ensemble 的方式：训练若干个模型，然后在 decoding 的时候，通过投票的方式来选出最佳的翻译。基本的方法是平均(log)概率。一般而言，这些模型共享目标词汇表，以及历史的解码信息，但是训练的方式与网络架构都可以不同。</p><h1 id="NMT-的评价方法"><a href="#NMT-的评价方法" class="headerlink" title="NMT 的评价方法"></a>NMT 的评价方法</h1><p>NMT 质量的评价方法可以大致分为主观与客观两种。</p><p>Metrics:</p><ol><li>Adequacy: 翻译的句子是否与原句的意思相近？</li><li>Fluency: 翻译的句子在目标语言中是否通顺？</li></ol><p>Kappa 系数：</p><script type="math/tex; mode=display">K = \frac{p(A) - p(E)}{1 - p(E)}</script><p>p(A) 指评价者评价的概率， p(E)指随机评价的概率</p><p>提高一致性的方法还有：<br>将准确与流畅的评价分开，正则化分数，用一些 trick 将不靠谱的受试者剔除</p><p>其他需要评价的一些 criteria:</p><ol><li>speed </li><li>size</li><li>integration</li><li>customization </li></ol><h2 id="自动评价"><a href="#自动评价" class="headerlink" title="自动评价"></a>自动评价</h2><ul><li>目标：程序来评价翻译的质量</li><li>好处：成本低，易于调整，一致性好</li><li>基本操作：<ul><li>给定机翻输出</li><li>给定人类翻译</li><li>目标：比较两者相似度</li></ul></li></ul><p>precision(correct/output-length), recall(correct/reference-length), f-measure</p><p>word error rate: 最小编辑距离除以总长度</p><p>BLEU:</p><p>n-gram 覆盖率，计算长度为 1-4 的 ngram 的 precision。<br>首先，计算一个对过短翻译的惩罚：</p><p>BP = min(1, exp(1 - ref-length/out-length))</p><p>然后计算BLEU：</p><script type="math/tex; mode=display">BLEU = BP(\prod_{i=1}^4 precision_i)^{1/4}</script><p>如果没有 4-gram 能够匹配， BLEU 的值为0。一般而言, BLEU 是计算整个语料库的。</p><p>BLEU 同样可以用于计算有多个 reference 的翻译来测试 variability (多样性？)<br>n-grams 可以匹配任意的 reference, 但是计算的时候用最接近的 ref-length 做计算。</p><p>Meteor: 给予词干，同义词一定的分数，paraphrase 的使用也有分。</p><p>对 Metrics 的一些批评：</p><ol><li>无视相关的词</li><li>通常在本地水平测试，对更加广义的语法不加以考虑</li><li>分数可解释性不强</li><li>有的时候人工翻译会得到一个较低的 BLEU 分数</li></ol><p>对 Metrics 的评价体系：<br>与人工判断作比较，计算相关性</p><h1 id="Attention-的一些变种"><a href="#Attention-的一些变种" class="headerlink" title="Attention 的一些变种"></a>Attention 的一些变种</h1><p>首先，我们看一下多种的计算 attention 分数的方法(Luong et al., 2015)：</p><ol><li><script type="math/tex">h_t^T h_s</script> 点乘法计算分数</li><li><script type="math/tex">h_t^T W_a h_s</script> 一般形式</li><li><script type="math/tex">v_a^T tanh(W_a[h_t;h_s])</script> 向量拼接的形式：将源语言的输出向量 <script type="math/tex">h_t</script> 与上一个翻译的单词拼接起来</li></ol><h2 id="Condition"><a href="#Condition" class="headerlink" title="Condition"></a>Condition</h2><p>我们还可以把之前的决策纳入考虑(dl4mt-tutorial)：</p><ul><li><script type="math/tex; mode=display">s' = GRU_A(s_{i-1}, y_{i-1})</script></li><li><script type="math/tex; mode=display">c_i = ATT(C, s')</script></li><li><script type="math/tex; mode=display">s_i = GRU_B(c_i, s')</script></li></ul><p>其主要思路是：之前的 Attention 模型只与隐层 s 相关，但是真正决策的单词也会影响之后的概率。</p><h2 id="Guided-Alignment-Training-chen-et-al-2016"><a href="#Guided-Alignment-Training-chen-et-al-2016" class="headerlink" title="Guided Alignment Training(chen et.al., 2016)"></a>Guided Alignment Training(chen et.al., 2016)</h2><p>基本思路:</p><ol><li>使用外部的工具创建词对齐向量(word alignment)</li><li>如果多个源单词对应一个目标单词，需要进行正则化，以使得<script type="math/tex">\sum_j A_{ij} = 1</script></li><li>对训练时的目标函数作如下更改：<script type="math/tex">H(A, \alpha)=-\frac{1}{T_y} \sum_{i=1}^{T_y}\sum_{j=1}^{T_x} A_{ij}log\alpha_{ij}</script>。该函数的目的有二：一，同之前一样最小化交叉熵；二，最小化注意力向量与外部的词对齐向量之间的差异</li></ol><h2 id="Incorporating-Structural-Alignment-Biases-Cohn-et-al-2016"><a href="#Incorporating-Structural-Alignment-Biases-Cohn-et-al-2016" class="headerlink" title="Incorporating Structural Alignment Biases(Cohn et.al.,2016)"></a>Incorporating Structural Alignment Biases(Cohn et.al.,2016)</h2><p>通过统计词对齐算法，我们发现对齐本身会有一定的偏差(bias)。</p><ol><li>position bias: 相关的位置对对齐提供了大量的信息</li><li>fertility/coverage: (生育/覆盖) 一些词会在目标句中生成多个词来保证所有的源单词都会被覆盖到</li><li>bilingual symmetry: 双语对称性, 互为源语言与目标语言的两种语言，词对齐是对称的</li></ol><p>针对 position bias, 我们需要在创建注意力模型的时候输入位置信息。对于非递归架构来说这种方法更加有效(CNN?)<br>position encoding 的方法也有各式各样的。</p><p>针对 fertility/coverage， 我们的想法是对于目标语言，同样有 <script type="math/tex">\sum_i^{T_y}\alpha_{ij} \simeq 1</script>，即同注意力的 softmax 类似，同一源单词的注意力权重在目标句上的和应当近似于1。因此，我们可以用这样的正则项来作为训练的目标函数:<script type="math/tex">\sum_j^{T_x}(1-\sum_i^{T_y}\alpha_{ij})^2</script>。这种优化方法由于有一个预先假设，但是在实际情况中这种假设不一定成立，一种更加 general 的方式是，使用一个神经网络来学习单词的”生育”: <script type="math/tex">f_j = N\delta(W_jh_j)</script>，然后用 fertility 项来替代原式中的 1。</p><p>针对 bilingual symmetry, 我们在训练的时候可以添加一项路径奖励，来奖励那些能够生成具有对称性的注意力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sequence-to-Sequence-Model&quot;&gt;&lt;a href=&quot;#Sequence-to-Sequence-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequence-to-Sequence Model&quot;&gt;&lt;/a&gt;Sequence
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU: Perceptron, Neural Networks, RNN/LSTM</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/05/nlu-2/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/05/nlu-2/</id>
    <published>2019-05-05T07:12:16.000Z</published>
    <updated>2019-10-22T09:22:25.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Perceptron"><a href="#Perceptron" class="headerlink" title="Perceptron"></a>Perceptron</h1><p>感知机是神经网络最基本的元件之一， 是一个加权分类器。<br>基本的感知机算法如下：</p><script type="math/tex; mode=display">u(\mathbf{x}) = \sum_{i=1}^{n} w_i x_i</script><p>当<script type="math/tex">u(x)</script>大于某个阈值时，输出1，反之输出0。</p><h2 id="逻辑门实现"><a href="#逻辑门实现" class="headerlink" title="逻辑门实现"></a>逻辑门实现</h2><p>我们可以用最基本的感知机来实现一些基本的逻辑门。</p><p>首先我们来看一下 AND 函数的真值表：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 AND x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table></div><p>AND 函数是线性可分的：在一个二维空间里，横坐标为x1, 纵坐标为 x2， 我们可以用一条直线来将空间分为两部分，分别表示激活状态与非激活状态。同理，OR 函数也是线性可分的。</p><p>我们使用如下函数来定义我们的感知机加权计算u(x)：</p><script type="math/tex; mode=display">u(x) = 0.5 x_1 + 0.5 x_2</script><p>激活的阈值<script type="math/tex">\theta=1</script>,当函数值小于1时，不激活；反之大于等于1时激活。</p><p>同理，我们可以根据如下的真值表写出 OR 的定义：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 OR x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>1</td></tr></tbody></table></div><p>感知机权重：</p><script type="math/tex; mode=display">u(x) = 0.5 x_1 + 0.5 x_2</script><p>阈值为大于0， 小于等于0.5的任意实数。实际上，我们可以将阈值从不等式<script type="math/tex">u(x)\leq \theta</script>的右边拿到左边，这样一来我们就可以使用一个截距项<script type="math/tex">b=-\theta</script>来处理不同情况下的阈值。</p><p>XOR 是另外一种情况：我们在空间中没有办法使用线性的分割线将空间分为两个部分。用感知机的解决方案是：复合多层感知机(MLP)，来应对非线性的问题。首先，我们写出 XOR 的真值表：</p><div class="table-container"><table><thead><tr><th>x1</th><th>x2</th><th>x1 XOR x2</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table></div><p>注意到如下事实：x1 XOR x2 = ((NOT x1) AND x2) OR (x1 AND (NOT x2))，可以写真值表验算一下。<br>我们实际上可以将 XOR 操作分解为两步： 第一步计算两个 AND， 第二部计算上一步的OR。NOT操作，我们可以用负的权重来实现。</p><p>由此，我们可以写出如下的感知机分解式子：</p><script type="math/tex; mode=display">u_1(\mathbf{x}) = -0.5 x_1 + 0.5 x_2 -1 > 0u_2(\mathbf{x}) = 0.5 x_1 - 0.5 x_2 -1 > 0v(\mathbf{u}) = 0.5 u_1 + 0.5 u_2 - 0.4 > 0</script><p>这是一个两层的感知机，其结构如下图：</p><p><img src alt="感知机XOR"></p><p>另一种分解方式为： x1 XOR x2 = (x1 NAND x2) AND (x1 OR x2)</p><h2 id="参数的学习"><a href="#参数的学习" class="headerlink" title="参数的学习"></a>参数的学习</h2><p>我们可以通过梯度下降法来学习参数：</p><script type="math/tex; mode=display">\begin{align}w_i &= w_i + \eta \delta w_i \\  &= w_i + \eta(t - o)x_i\end{align}</script><p>即：损失函数相对于参数<script type="math/tex">w_i</script>的导数。</p><p>MLP 的一般使用思路如下：</p><ol><li>初始化 MLP 参数</li><li>给定训练特征x，然后计算 MLP 的输出 y</li><li>将 y 与正确的目标 t 作对比，得到误差量 (error quantity)</li><li>根据误差量，使用梯度下降法对 MLP 的参数进行修正</li><li>重复2-5，直至收敛</li></ol><p>我们的学习目标一般是最小化损失函数，常用的损失函数有均方根误差(Mean Square Error, MSE)</p><script type="math/tex; mode=display">E(\mathbf{w}) = \frac{1}{2N} \sum_{p=1}^{N}(t - o)^2</script><h1 id="基于-MLP-的语言模型"><a href="#基于-MLP-的语言模型" class="headerlink" title="基于 MLP 的语言模型"></a>基于 MLP 的语言模型</h1><p>n-gram 角度的语言模型：输入为<script type="math/tex">w_{i-n+1}..w_i</script>，输出为单词<script type="math/tex">w_i</script>的概率，我们用一个函数来描述:<script type="math/tex">f:V^n \rightarrow \mathbb{R}_{+}</script>。<br>函数角度的语言模型：输入为 prefix 字符串，输出为一个概率分布: <script type="math/tex">f:V^{n-1} \rightarrow (V \rightarrow \mathbb{R}_{+})</script>。</p><p>我们使用 one-hot 编码来编码单词，再将单词按照顺序连接成一个大的向量，这样子我们就可以将单词表示为向量形式。同理，我们的概率分布也是向量形式。这样，我们在数学上就可以用 MLP 来处理，因为 MLP 在前向的运算实际上就是矩阵乘法的形式。在输出层，我们使用 softmax 函数来处理每个单词的概率分布。</p><p>对于 n-gram 模型， 我们通常可以使用如下的神经网络进行处理:</p><p><img src="/images/ngram-nn-lm.png" alt></p><p>首先，对每个 one-hot 向量，使用矩阵 C 进行压缩编码得到词嵌入(embedding)；然后使用矩阵 W 来计算 ngram 的完整表示向量，最后，矩阵 V 被用来计算最终的概率分布。</p><h1 id="Recurrent-Networks"><a href="#Recurrent-Networks" class="headerlink" title="Recurrent Networks"></a>Recurrent Networks</h1><p>引入递归神经网络的原因有以下几点：</p><ol><li>n-gram 模型对上下文信息利用不充分</li><li>上述的 MLP 模型虽然能够对比较广的上下文进行建模，但是其大小是固定的</li><li>与此相反，语言学的角度来说，需要的上下文范围实际上很大</li></ol><p>基于这些原因，人们提出了 RNN 模型：</p><p><img src="/images/rnn-simple.png" alt></p><p>这是一个最简单的，回看一步的 RNN 网络结构。向量 x 是时刻 t 的输入，这里我们可以理解为 one-hot 的单词。向量 y 是时刻 t 的输出向量，向量 s 是时刻 t 的隐层，对于每个时刻而言，输入都是上一时刻的 s 与 x 向量拼接后的向量。<br>我们可以看出，该网络递归的利用了所有以前的信息，从而，我们在使用 RNN 建立语言模型的时候不再需要马尔科夫假设。</p><p>训练网络的过程如下：</p><ol><li>首先，我们随机初始化参数矩阵 U, V, W, 隐层随机初始化。</li><li>对于任意的时刻 t, 我们都将其作为普通的 MLP进行处理，只不过我们的输入是 x 与 时刻t-1 的 s 的复合</li><li>我们定义训练的目标是给定历史序列，计算下一个单词的概率分布，所以错误信号定义为 <script type="math/tex">w_{t} - y(t)</script>，其中 <script type="math/tex">w_{t}</script> 是 one-hot 编码。</li><li>我们使用如上方法，来在给定的训练集上训练网络。</li></ol><h2 id="Backpropagation-through-time-BPTT"><a href="#Backpropagation-through-time-BPTT" class="headerlink" title="Backpropagation through time(BPTT)"></a>Backpropagation through time(BPTT)</h2><p>再进一步，我们需要将我们的反向传播前推至任意长度，以实现对历史上下文信息的利用。实际上，我们可以把这个过程看做是当前的单词的概率是之前 n 个单词的函数，所以反向传播需要对以前的时刻的参数进行更新。从另一个角度来看，我们可以将 BPTT 版本的 RNN 看作是一个沿着时间轴展开的大型 MLP。</p><p><img src="/images/bptt.png" alt></p><p>首先，我们定义一下公式：</p><script type="math/tex; mode=display">s_t = Vx_t + U s_{t-1}\\y_t = softmax(W s_t)</script><p>首先，令向量<script type="math/tex">a = W s_t</script>对于 softmax 函数而言，对第 i 分类的第 j 个输入我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = \frac{\frac{\partial e^{a_i}}{\sum_k e^{a_k}}}{\partial a_j}</script><p>注意，i 与 j 不一定相等。令<script type="math/tex">\Sigma = \sum_k e^{a_k}</script>使用除法求导法则我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = \frac{\frac{\partial e^{a_i}}{\partial e^{a_j}}\Sigma - e^{a_i} \frac{\partial \Sigma}{\partial e^{a_j}}}{\Sigma^2} =\frac{\frac{\partial e^{a_i}}{\partial e^{a_j}}}{\Sigma} - \frac{e^{a_i}}{\Sigma}\frac{e^{a_j}}{\Sigma}</script><p>当 <script type="math/tex">i=j</script>的时候，第一项偏导数为 <script type="math/tex">e^{a_i}</script>，从而我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_i} = y_i(1 - y_i)</script><p>反之，第一项偏导数为0，我们有</p><script type="math/tex; mode=display">\frac{\partial y_i}{\partial a_j} = -y_j y_i</script><p>使用复合函数求导法则，我们可以得到 W, s 关于 a 的偏导：</p><script type="math/tex; mode=display">\frac{d a}{dW} = \mathbf{s}^T \nabla \mathbf{a}\\\frac{d a}{ds} = \nabla \mathbf{a}^T \mathbf{W}</script><p>而对于 s 而言，它又可以传播到前一个 s’:</p><script type="math/tex; mode=display">\frac{d s'}{d s} = \nabla \mathbf{s}^T U</script><p>以此类推。</p><p>我们可以任意地回溯时间直至序列的开头，但是由于我们通常采用诸如 sigmoid, tanh 之类的激活函数，导数在数值上往往小于1，从而带来了梯度消失的风险：反向传播会以指数形式放大/缩小导数，由于反向传播过远，导致梯度在较长的时刻之前接近于0，从而无法传播。RNN 的中远时刻的信息，要么非常强，要么非常弱。</p><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>为了解决梯度消失的问题，LSTM 作为 RNN 的一个变种被提出。LSTM 的基本思路是：保持很长一段时间的信息不变，从而系统可以从较远的输入中获取信息。</p><p>与普通的 RNN 不同， LSTM 有一个额外的记忆层来存储长时间的记忆。LSTM 由如下四个元件组成：</p><ol><li>Input gate： 用于控制输入是否被存储到记忆中</li><li>Output gate: 用于控制当前激活的记忆向量是否传递至输出层</li><li>Forget gate: 控制记忆向量是否清零</li><li>Memory cell: 存储当前的记忆向量</li></ol><p><img src="/images/lstm.png" alt></p><p>图中，<script type="math/tex">\delta</script> 表示 sigmoid 函数， 加号表示向量拼接，乘号表示向量乘法。h 表示输出。<br>从左到右：</p><ol><li>首先，隐层向量与输入向量在遗忘门经过sigmoid函数转化为 0-1 区间的数，接近 0 则表示遗忘，接近 1 则表示记忆。<br>这一步是对记忆层进行遗忘，遗忘的依据是当前的输入向量与隐层</li><li>第二步是输入门，输入门由一个 sigmoid 与一个 tanh 层组成，对当前的输入(隐层向量与输入向量的拼接)进行筛选，tanh 与 sigmoid 分别做决策，然后 pointwise multiplication 综合两者，都重要的记忆留下，其余筛去。</li><li>从输入门往上，将经过遗忘的记忆向量与经过输入门筛选的向量进行 pointwise addition， 激活/强化当前的记忆</li><li>第三步是输入门右边的输出门，由一个 sigmoid 激活原来的隐层向量与输入向量的拼接，由一个 tanh 处理记忆，然后两者point-wise相乘，得到当前时刻的输出。这个输出同时会保存，作为下一时刻的隐层输入。</li></ol><p>LSTM 用于解决梯度消失的思路是：记忆层的传递本质是一个线性函数，所以在这个层面没有梯度消失的问题,导数可以一直传递下去。对于控制门而言，它们能够随着训练学习到何时应当开门激活，何时应当屏蔽门的输入。这种设计提升了网络的复杂度，也能更精细地控制历史信息。</p><p>若干数学推导参考<a href="https://www.zhihu.com/question/44895610" target="_blank" rel="noopener">https://www.zhihu.com/question/44895610</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Perceptron&quot;&gt;&lt;a href=&quot;#Perceptron&quot; class=&quot;headerlink&quot; title=&quot;Perceptron&quot;&gt;&lt;/a&gt;Perceptron&lt;/h1&gt;&lt;p&gt;感知机是神经网络最基本的元件之一， 是一个加权分类器。&lt;br&gt;基本的感知机算
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
  <entry>
    <title>复习NLU:Introduction, Language Model</title>
    <link href="http://MeowAlienOwO.github.io/2019/05/04/nlu-1/"/>
    <id>http://MeowAlienOwO.github.io/2019/05/04/nlu-1/</id>
    <published>2019-05-04T13:13:03.000Z</published>
    <updated>2019-10-22T09:22:28.220Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Natural Language Understanding:</p><blockquote><p>Any computational problem that input is natural language, output is structured information of a computer can store/execute</p></blockquote><h2 id="Computational-Linguists-with-Deep-Learning"><a href="#Computational-Linguists-with-Deep-Learning" class="headerlink" title="Computational Linguists with Deep Learning"></a>Computational Linguists with Deep Learning</h2><blockquote><p>“Although current deep learning research tends to claim to<br>encompass NLP, I’m (1) much less convinced about the strength of the results, compared<br>to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision,<br>the way to go is to couple huge amounts of data with black-box learning architectures.”<br>Michael Jordan</p></blockquote><p>目前而言，深度学习在比较 high-level 的 NLP 任务中，没有能够如同在视觉领域一样大幅度减小错误率(e.g. 25-50%)，而且似乎只有在相对纯粹的信号处理中，应用深度学习能够大幅度提高性能。    </p><h2 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model:"></a>Language Model:</h2><p>我们把语言模型认为是一种关于字符串的概率模型：</p><ol><li>语音识别：给定语音信号为条件</li><li>机器翻译：给定另一种语言</li><li>自动补全：给定一句话的前面若干词<br>…</li></ol><blockquote><p>给定一个有限的词汇表$V$, 我们定义一个概率分布：<script type="math/tex">V^*\rightarrow\mathbb{R}_{+}</script></p></blockquote><ol><li>What is sample space? <script type="math/tex">V*</script></li><li>What might be some useful random variables? sentence, i.e. sequence of words, each word is variable</li><li>What constrains must <script type="math/tex">P</script> satisfy? sequence of words &lt;= product of prob of each word ,non-negative,sum to 1</li></ol><h3 id="N-gram-Model"><a href="#N-gram-Model" class="headerlink" title="N-gram Model"></a>N-gram Model</h3><p>我们令<script type="math/tex">w</script>为一个单词序列（句子），<script type="math/tex">|w|=n</script>表示这个序列的长度，<script type="math/tex">w_i</script> 表示第 i 个单词。这个序列的概率可以被<br>定义为(<script type="math/tex">W_i</script>表示单词在位置i处的随机变量)：</p><script type="math/tex; mode=display">\begin{align}P(w) &= P(w_1, w_2, ... w_{n}) \\     &= P(W_1 =w_1) \times \\     &~~~P(W_2 = w_2|W_1 = w_1) \times\\     &~~~ ...\\     &~~~ P(W_n = w_n|W_{n-1}=w_{n-1}...)\\     &= \prod_{i=1}^{n}P(W_i = w_i|W_{i-1}=w_{i-1}, W_{i-2}=w_{i-2}...W_1=w_1)\\     &= \prod_{i=1}^{n}P(w_i|w_{i-1},w_{i-2}...w_1)\\\end{align}</script><p>这个式子使用条件概率定义了一个在无限空间上的联合分布，可以取的句子长度为任意值，但是我们仍然可以用条件概率来定义这个分布，每一个都有着有限的采样空间。</p><p>n-gram 模型是基于马尔科夫假设定义的语言模型：它假定在 i 处的单词的概率分布仅由之前的 n 个单词所决定：</p><script type="math/tex; mode=display">P(w_i|w_{i-1}, w_{i-2} ... w_1) = P(w_i|w_{i-1}, w_{i-2} ... w_{i-n})</script><p>我们可以使用计数的方式来估计 n-gram 模型的分布：</p><script type="math/tex; mode=display">P(w_i | w_{i-1}, w_{i-2} ... w_{i-n}) = \frac{C(w_{i-n},...w_{i-1}, w_i)}{C(w_i)}</script><p>由于单词的分布往往有着长尾特性，我们使用加一平滑 (add-one smoothing) 或者加<script type="math/tex">\alpha</script>平滑 (add-alpha smooting) 来处理计数，来保证所有的样本空间上的概率都不为0:</p><script type="math/tex; mode=display">P(w_i | w_{i-1}, w_{i-2} ... w_{i-n}) = \frac{C(w_{i-n},...w_{i-1}, w_i) + \alpha}{C(w_i) + \alpha|V|}</script><p>其中当平滑系数<script type="math/tex">\alpha</script>取1的时候，为加一平滑。我们通常取一个相对数量级比较小的平滑系数来处理。其他的处理方法还有插值法等，暂时不讨论。</p><p>当我们有了一个语言模型后，我们可以根据之前的序列，使用最大似然(maximum likelihood)来推断下一个单词出现的概率:</p><script type="math/tex; mode=display">\hat{w}_{k+1} = \underset{w_{k+1}}{\operatorname{argmax}}P{w_{k+1}|w_1 ... w_k}</script><h3 id="Language-Model-in-translation"><a href="#Language-Model-in-translation" class="headerlink" title="Language Model in translation"></a>Language Model in translation</h3><p>n-gram 模型尽管是一种经典的语言模型，在翻译上，它不起作用:</p><p>给定英文序列<script type="math/tex">e =e_1,e_2...e_n</script>，预测中文序列<script type="math/tex">f=f_1,f_2...f_m</script>, 我们会发现 n-gram 模型会遗忘以前的内容，但是这些内容很显然对翻译是非常重要的。就算我们将序列穿插：<script type="math/tex">w = f_1 e_1 f_2 e_2 ...</script>， 仍然有两个问题：1. 语言的顺序不一定一一对应 2. 长度未必相等。为了解决这个问题，人们提出了<strong>对齐模型</strong> (word alignment model)</p><h4 id="IBM-Model-1"><a href="#IBM-Model-1" class="headerlink" title="IBM Model 1"></a>IBM Model 1</h4><script type="math/tex; mode=display">p(f, a|e) = p(I|J) \prod_{i=1}^{I} p(a_i | J) p(f_i |e_{a_i})</script><p>其中，f 表示目标语言，e 表示源语言， a 表示目标语言对应的对齐目标， I 表示目标语言句子的长度， J 表示源语言的句子长度。我们可以将其看做一个零阶的 HMM 模型:<script type="math/tex">p(a_i|J)</script> 可以看做是状态转换概率，<script type="math/tex">p(f_i|e_{a_i})</script> 可以看做是触发概率，由于模型的转换概率不基于历史记忆(前一个状态)，这个马尔科夫链是0阶的。这个模型基于如下假设：</p><ol><li>目标句中，每个单词都由原句的某个单词产生</li><li>对应关系作为隐变量 (latent variable)</li><li>给定对齐 a，翻译的决策是独立的</li></ol><p>注意这里我们对对齐的定义为：一个向量存储了位置的对应关系。对齐有如下几种关系：</p><ol><li>Reorder 顺序不同</li><li>Word dropping 某些单词不翻译</li><li>Word Insertion 原句中需要一个 null token 来表示目标句中不存在的对应单词的情况</li><li>one-to-many 一个单词对应多个目标单词</li><li>many-to-one 一个目标单词对应多个源单词</li></ol><p>举个例子：</p><blockquote><p>虽然 北 风 呼啸，但 天空 仍然 十分 清澈。<br>However, the sky remained clear under the strong north wind.</p></blockquote><p>(However-虽然， “,” - “,”， the-天空， sky-天空， remained-仍然, clear-(十分，清澈)， under-null， the-null， strong-呼啸， north-北， wind-风， “.”-“。”)</p><p>对齐向量为：(1, 5, 7, 7, 8, (9, 10), 0, 0, 4, 3, 2, 11)</p><p>我们模型的参数为：任意一组源语言-目标语言单词对的概率, 比如说<script type="math/tex">p(north|北)</script>，其期望一般用”北”与”north”对齐的次数与”北”与所有单词的对其次数的比值来表示。</p><p>我们可以用 EM 算法来训练对齐模型：</p><ol><li>随机初始化模型的参数</li><li>使用现有的参数计算每种对齐的概率 </li><li>使用期望，通过MLE计算新的参数</li><li>迭代以上两步直至收敛</li></ol><p>举例：</p><ol><li>大 房子 - big house</li><li>清理 房子 - cleaning house</li></ol><p>我们首先用均匀分布初始化参数，即源语言-目标语言对概率:</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>1/3</td><td>1/3</td><td>1/3</td></tr><tr><td>house</td><td>1/3</td><td>1/3</td><td>1/3</td></tr><tr><td>cleaning</td><td>1/3</td><td>1/3</td><td>1/3</td></tr></tbody></table></div><p>第一步E计算对齐的概率：</p><ol><li>大-big, 房子-house: 1/3 × 1/3 = 1/9</li><li>大-house, 房子-big： 1/3 × 1/3 = 1/9</li><li>清理-cleaning, 房子-house: 1/3 × 1/3  = 1/9</li><li>清理-house, 房子-cleaning: 1/3 × 1/3 = 1/9</li></ol><p>这一步，我们可以看出每种对齐的概率都是相等的1/2</p><p>第一步M, 我们需要根据给定的数据来计算参数矩阵。<br>首先，我们根据数据与对齐概率，计算每个单词对的期望</p><script type="math/tex; mode=display">\begin{align}p(f_i|e_i) &= \frac{\sum_a p(a|f, e) * C(f_i, e_i)}{\sum_a p(a|f, e) C(f_i, \cdot)} \\ &= \frac{\mathbb{E}[C(f_i, e_i)]}{\mathbb{E}[C(f_i, \cdot)]} \end{align}</script><p>归一化后我们得到</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>1/2</td><td>1/2</td><td>0</td></tr><tr><td>house</td><td>1/4</td><td>1/2</td><td>1/4</td></tr><tr><td>cleaning</td><td>0</td><td>1/2</td><td>1/2</td></tr></tbody></table></div><p>第二步，我们使用上一步得到的矩阵进行进一步的E:</p><ol><li>大-big, 房子-house: 1/2 × 1/2 = 1/4 =&gt; 2/3</li><li>大-house, 房子-big： 1/4 × 1/2 = 1/8 =&gt; 1/3</li><li>清理-cleaning, 房子-house: 1/2 × 1/2  = 1/4 =&gt; 2/3</li><li>清理-house, 房子-cleaning: 1/4 × 1/2 = 1/8 =&gt; 1/3</li></ol><p>然后进行M：</p><div class="table-container"><table><thead><tr><th></th><th>大</th><th>房子</th><th>清理</th></tr></thead><tbody><tr><td>big</td><td>2/3</td><td>1/3</td><td>0</td></tr><tr><td>house</td><td>1/6</td><td>2/3</td><td>1/6</td></tr><tr><td>cleaning</td><td>0</td><td>1/3</td><td>2/3</td></tr></tbody></table></div><p>可以看出， 大-big， 房子-house, 清理-cleaning 的概率是递增的，也就是最后会收敛到三个1</p><h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><p><a href="http://www.shuang0420.com/2017/05/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/" target="_blank" rel="noopener">http://www.shuang0420.com/2017/05/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Natural Language Understanding
      
    
    </summary>
    
      <category term="note" scheme="http://MeowAlienOwO.github.io/categories/note/"/>
    
    
      <category term="nlu" scheme="http://MeowAlienOwO.github.io/tags/nlu/"/>
    
  </entry>
  
</feed>
