<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Machine Learning Review: Neural Network &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Machine Learning">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
     <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
     </script> -->

<!-- Share.js -->
<!-- <link href="/assets/css/share.min.css" rel="stylesheet"> -->
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js"></script> -->
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Review: Neural Network">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/machine learning/">
				    machine learning (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/RL/">
				    RL (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=60827595">2017 謹賀新年 | おたけやん [pixiv]</a></div><!-- /.image-credit -->
  <div class="entry-image">
    
    <img src="http://og78s5hbx.bkt.clouddn.com/60827595.jpg" alt="Machine Learning Review: Neural Network">
    

  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" rel="bookmark" title="Machine Learning Review: Neural Network">Machine Learning Review: Neural Network</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2017-01-09T18:36:10+08:00">January 09, 2017</time></span></h2>
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<p>呼……终于到最后一篇了，也是开学学的内容。</p>

<p>今年也请多多指教。</p>

<h1 id="神经网络neural-network">神经网络（Neural Network）</h1>

<p>神经网络是一种很古老，但是到现在依然有很广泛的用途的机器学习算法。包括现在火的深度学习，其基础也是神经网络。</p>

<p>神经网络的定义（来自西瓜书，引用T.Kohonen）：</p>

<blockquote>
  <p>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，其组织能够模拟生物神经系统对真实世界作出的交互反应。</p>
</blockquote>

<h2 id="神经元模型neuron-model">神经元模型(Neuron Model)</h2>

<p>如上述的简单单元就是神经网络的最基本组成部分–神经元。一个神经元可以接受若干个来自其他神经元的输入信号，这些信号通过带权重的连接进行传递，总输入值与神经元的阈值进行比较，然后通过激活函数处理，形成神经元的输出。</p>

<p>神经元的数学形式可以表示如下：</p>

<script type="math/tex; mode=display">y = f(\sum\limits_{i=1}^{n}w_ix_i - \theta)</script>

<p>其中，$f$表示激活函数，$w_i$表示权重，$x_i$表示属性值，$\theta$表示阈值。</p>

<p>一种比较基本的激活函数是<strong>阶跃函数</strong>（signal function），即等于或超过阈值则为1，反之为0。但是这个函数不方便求导与计算误差，我们在实际中比较常见的函数有sigmoid函数：</p>

<script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script>

<p>神经元具有学习功能，会根据输出值与真实值的误差对权重进行调整，从而达到泛化的目的。表示神经元的学习方式的函数叫做<strong>训练函数</strong>(training function)</p>

<h2 id="感知机perceptron">感知机（Perceptron）</h2>

<p>感知机是一种最基本的神经网络，其包含两层神经元，输入层接受外界的信号，传递给输出神经元。输出神经元对输入的数据作加权处理，然后与阈值（threshold）做比较，判断神经元是否被激活。</p>

<p>感知机可以被认为是一种线性分类器，即在样本空间中寻找一个超平面，使得样本能够分布在超平面的两端。这需要我们的样本是<strong>线性可分</strong>的。这也是我们对应用感知机场景的一个基本假设。例如，XOR问题就不是线性可分的，因此无法应用感知机。</p>

<p>感知机会根据输出值与目标分类值之间的误差更新权重，这就是感知机的**学习**过程。</p>

<p>基本的感知机表示如下：</p>

<p><script type="math/tex">R = \theta + \sum\limits_{i=1}{m}w_ix_i \\
o = sign(R) = \lbrace +1; if~R > 0 \\ -1; otherwise</script>其中，阈值$\theta$可以看做一个输入值恒为-1（或者-1）的”哑节点”（dummynode），那么感知机的表达方式就可以统一为权重的学习。</p>

<p>感知机的训练函数为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta (d - o) x_i, i = 1,2,...,n</script>

<p>其中，$w_i$表示权重，$\eta$表示学习率。学习率是一个常数，用于控制学习的速度。太低会导致学习过程缓慢，太高则有可能导致学习失败：感知机无法收敛到一个比较适当的区间。$d$表示类别的值，$o$表示感知机的输出，我们也可以将$d - o$统一成为误差。</p>

<p>我们不加证明地给出收敛性定理：</p>

<blockquote>
  <p>如果样本是线性可分的，那么感知机将会一定可以在有限的步骤内收敛到一个解。</p>
</blockquote>

<h2 id="自适应线性神经元adaptive-linear-elements">自适应线性神经元（Adaptive Linear Elements）</h2>

<p>感知机由于采用了阶跃函数作为激发函数，容易出现难以收敛的情况。同时在样本集不是线性可分的情况下，感知机难以找出一个恰当的近似。为此，我们引入自适应线性神经元。</p>

<p>简单地说，自适应线性神经元取消了阶跃函数，直接以输入的加权求和（包括阈值，或者说哑输入）作为输出值。</p>

<script type="math/tex; mode=display">o = \theta + \sum\limits_{i=1}{n} w_i x_i</script>

<p>我们的误差函数也需要从原来的简单相减进行改变：</p>

<script type="math/tex; mode=display">Err(W) = \frac{1}{2} \sum\limits_{k = 1}^{K}(d_k - o_k)^2</script>

<p>其中，k用于表示第k个训练样本，$d_k$表示样本的目标分类，$o_k$表示输出值。</p>

<p>这里常数1\2是用来处理求导后产生的常数。我们从误差函数中可以看出，如果误差函数越小，神经元就有更好的近似。</p>

<h2 id="梯度下降法gradient-decent">梯度下降法（Gradient Decent）</h2>

<p>对于自适应神经元，我们的训练目标是在样本空间中，对于给定的训练集，使其误差最小。为此，我们需要引入梯度下降法(Gradient Decent)。</p>

<p>在高维的情况下，同斜率等效的一阶导数称作<a href="https://en.wikipedia.org/wiki/Gradient">梯度</a>(gradient)。我们知道，二维的情况下函数的极小值是“山谷”的位置，即斜率为0且左右领域函数值皆大于极小值。推广至高维，高维函数的极小值同样是梯度为0的点。我们如果要使误差向极小值移动，我们需要判定其移动方向。在二维的情况下，我们选取<strong>斜率减小</strong>的方向，即函数值减小的防线。</p>

<p>同样的，我们在高维需要选取<strong>梯度减小</strong>的方向。对于权重的某个取值$\mathbf{w}$，我们可以求其梯度：</p>

<script type="math/tex; mode=display">\nabla F(w_1, w_2,...,w_m)</script>

<p>符号$\nabla$是一个用来表示向量微分的算子。为了更新权重，我们更加关注的是，权重向量在某一维度上的<strong>分量</strong>的改变趋势。为此，我们需要求取函数在某一维度上的导数，即为<a href="https://en.wikipedia.org/wiki/Partial_derivative">偏导数</a>：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial{w = w_i}} Err(w)</script>

<p>梯度可以写作:</p>

<script type="math/tex; mode=display">\nabla Err(\mathbf{w}) = [ \frac{\partial}{\partial{w_1}}, \frac{\partial}{\partial{w_2}},...,\frac{\partial}{\partial{w_m}}]</script>

<p>对于单个分量$w_i$而言，我们的训练函数改写如下：</p>

<script type="math/tex; mode=display">w_i \gets w_i - \eta \frac{\partial E}{\partial{w_j}}</script>

<p>因此，我们需要计算误差函数的偏导数。对于有K个样本的训练集，求总的偏导数：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \frac{\partial}{\partial w_i}\frac{1}{2}\sum\limits_{k=1}^{m}
 (\mathbf{d} - \mathbf{o})^2</script>

<p>提取常数项</p>

<script type="math/tex; mode=display">= \frac{1}{2} \frac{\partial}{\partial w_i} \sum\limits_{k=1}^K
(\mathbf{d} - \mathbf{o})^2</script>

<p>提取和式</p>

<script type="math/tex; mode=display">= \frac{1}{2} \sum\limits_{k=1}^K \frac{\partial}{\partial w_i}
(\mathbf{d} - \mathbf{o})^2\\</script>

<p>考虑到$(\mathbf{d} - \mathbf{o})^2$是关于$w_i$的函数，应用链式法则并消去常数项</p>

<script type="math/tex; mode=display">= \sum\limits{k=1}^K (\mathbf{d} - \mathbf{o})
\frac{\partial}{\partial{w_i}} (\mathbf{d} - \mathbf{o})</script>

<p>接下来，求$\mathbf{d} - \mathbf{o}$的偏导数。我们可以很容易地看出，这是一个线性函数，这意味着其他分量的导数为0（参考偏导数定义），有作用的只有$w_ix_i$这一项。保留符号，我们有</p>

<script type="math/tex; mode=display">= -\sum\limits{k=1}^K(\mathbf{d} - \mathbf{o})x_i(k)</script>

<p>于是我们的训练函数更改为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta \sum\limits_{k=1}{K}(\mathbf{d} - \mathbf{o}) x_i(k)</script>

<p>这被称为Delta法则(Delta Rule)</p>

<p>我们同样可以迭代求梯度，区别在于不是一次求取所有训练集的误差与更新值，而是每次计算一个训练样本。但是，这两者实际的结果会有所差异。每次训练的时候，我们迭代所有的训练样本一次，称作一个epoch。每次迭代样本的排序一般而言会改变。</p>

<p>在满足以下两个条件之一的时候，停止训练：</p>

<ol>
  <li>当预设的epoch数量运行完毕时</li>
  <li>当误差小于某个预设值</li>
</ol>

<h2 id="神经网络">神经网络</h2>
<p>由于线性神经元只能识别线性可分的情况，不能寻找非线性决策平面。于是，人们将若干神经元通过某些方式组合起来，形成神经网络。下图展示了神经网络的一种基本形式，其拓扑结构为单向向前输出的神经网络，后一层的输出是下一层的输入。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" alt=""></p>

<p>在神经网络中，激发函数一般使用sigmoid函数。它有一个很好的数学性质：</p>

<script type="math/tex; mode=display">f'(x) = f(x)(1 - f(x))</script>

<p>应用sigmoid函数计算偏导数如下：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} Err(w) = -\sum\limits_{k=1}^K (d_k - o_k)
\frac{\partial}{\partial w_i}sigmoid(net(\mathbf{w}\mathbf{x}))</script>

<p>其中$net(k)$是神经网络的对样本k的输出，即权重与输入的加权求和式。原来的函数无法一步求导到位，应用链式法则拆成两个变量相同的函数：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} sigmoid(net(k)) = \frac{\partial
o_k}{\partial net(k)} \frac{\partial sum(k)}{w_i}</script>

<p>根据sigmoid函数的特征，我们有</p>

<script type="math/tex; mode=display">\frac{\partial sigmoid(net(k)}{\partial sum(k)} = sigmoid(sum(k))(1 -
sigmoid(net(k))) = o_k(1 - o_k)</script>

<p>则我们的偏导数为：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \sum\limits_{k=1}^K x_i(k) o_k
(o_k - d_k)(1 - o_k)</script>

<h2 id="bp算法">BP算法</h2>

<p>我们以简单的三层神经网络（输入，隐层,输出）来讨论整个神经网络的权重更新算法：反向传播法(back propagation)</p>

<p>BP算法大致可以描述如下：</p>
<ol>
  <li>输入训练样本，计算其误差</li>
  <li>根据误差，计算梯度，更新权重</li>
  <li>根据上一步的梯度改变值，计算上一层的权重改变量，更新权重</li>
  <li>重复直到获得满意成果或者epoch用完</li>
</ol>

<p>考虑步骤2,3，我们不难发现，在应用上面的偏导数的前提下，需要知道如何**递归地**求取前面层级的权重更新量。</p>

<p>我们记</p>

<script type="math/tex; mode=display">\delta = - \frac{\partial Err}{\partial o} \frac{\partial o}{net}= o (o - d)(1 - o)</script>

<p>其中$net$表示该神经元的输入，即上一层的输出向量与权重的加权。</p>

<p>令输出层的某个神经元为l,隐层的某个神经元为m,输入层某个神经元为n。则，对于连接输出层与隐层的权重$w_{lm}$个，我们的更新公式可以写成:</p>

<script type="math/tex; mode=display">w_{lm} = w_{lm} + \eta \delta_l x_m</script>

<p>其中$\delta_l$是根据输出神经元l计算的值；$x_m$指的是输入向量的值。</p>

<p>连接输入层与隐层的权重的更新公式表示如下:</p>

<script type="math/tex; mode=display">w_{mn} = w_{mn} - \eta \Delta w_{mn}\\
 = w_{mn} - \eta \frac{\partial Err}{\partial w_{mn}}\\</script>

<p>注意到$x_m$是隐层的输入也是输入层的输出，用链式法则拆分偏导数：</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial w_{mn}} = - \frac{\partial
Err}{\partial x_m} \frac{\partial x_m}{\partial w_{mn}} \\
= - \frac{\partial Err}{\partial x_m} \frac{\partial x_m}{w_{mn}}</script>

<p>使用sigmoid函数的特性，有</p>

<script type="math/tex; mode=display">= -\frac{\partial Err}{\partial x_m} x_m(1 - x_m)</script>

<p>对于前面部分的偏导数，我们有</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial x_m} = -\sum\limits_{i = 1}^{l}
\frac{\partial Err}{\partial net} \frac{\partial net}{\partial x_m}\\
= \sum\limits_{i=1}{l} w_{im}g_{i}</script>

<p>于是输入层的更新函数就为：</p>

<script type="math/tex; mode=display">w_{mn} \gets w_mn - \eta x_m(1-x_m)\sum\limits_{i=1}^l w_{im}g_{i} x_n</script>

<p>注：此处限于本人数学水平，对为何要求和没有很好的理解，只能模糊地认为需要汇总所有的输出层的改变量，方向大致是全微分但是还是没能很好理解，待有识之士指导。</p>

<p>以下是一个演示图：</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/08135834-8e9b8ff2212545c0aeb1d68103ef3d64.gif" alt=""></p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong>
</li>
  <li>Lecture Material</li>
  <li>郭兰哲，BP算法，https://jlunevermore.github.io/2016/06/25/10.BP%E7%AE%97%E6%B3%95/</li>
  <li>daniel-D’s blog，BP算法之一种直观的解释 http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html</li>
  <li>
    <p>daniel-D’s blog，BP算法之向后传导 http://www.cnblogs.com/daniel-D/archive/2013/06/06/3121742.html</p>
  </li>
  <li>大量的网络资料（记不清了- -）</li>
</ol>
</body></html>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://MeowAlienOwO.github.io/tags/#Machine Learning" title="Pages tagged Machine Learning" class="tag"><span class="term">Machine Learning</span></a></span>
        
        <div class="social-share" >
  <!-- <ul class="socialcount socialcount-small inline-list"> -->
  <!--   <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li> -->
  <!--   <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li> -->
  <!--   <li class="googleplus"><a href="https://plus.google.com/share?url=http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li> -->
  <!-- </ul> -->
</div><!-- /.social-share -->

      </footer>
    </div><!-- /.entry-content -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://MeowAlienOwO.github.io/reinforcement-learning-3/" title="强化学习（三）">强化学习（三）</a></h3>
      <p>强化学习导论第三部分，planning, value function approximation, eligibility traces, policy gradient methods# Planning之前我们涉及到的强化学习方法中，DP是在$p$函数已知的情况下，其...&hellip; <a href="http://MeowAlienOwO.github.io/reinforcement-learning-3/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://MeowAlienOwO.github.io/reinforcement-learning-2/" title="强化学习（二）：动态规划，蒙特卡洛法，时间差分">强化学习（二）：动态规划，蒙特卡洛法，时间差分</a></h4>
        <span>Published on April 28, 2019</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/" title="强化学习导论（一）">强化学习导论（一）</a></h4>
        <span>Published on April 27, 2019</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->

  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js"></script> -->
<!-- <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
     inlineMath: [['$', '$'], [ '\\(', '\\)']],
     displayMath: [['$$', '$$']]
     }});
     </script>
-->
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'meowalienowogithubio'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



<script>
 var _config = {
     title: 'Machine Learning Review: Neural Network',
     image: 'http://og78s5hbx.bkt.clouddn.com/60827595.jpg'
 }

 socialShare('.social-share', _config)
</script>



</body>

</html>
