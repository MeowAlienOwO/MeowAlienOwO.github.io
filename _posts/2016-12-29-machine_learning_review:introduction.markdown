---
layout: post
title: "Machine Learning Review:Introduction"
date: "2016-12-29 19:20:50 +0800"
categories:[Machine Learning] 
tags: 
image: 
  feature: http://og78s5hbx.bkt.clouddn.com/60393964_p0.jpg 
  credit: スカサハ師匠 | 冬ゆき [pixiv] 
  creditlink: http://www.pixiv.net/member_illust.php?mode=medium&illust_id=60393964
comments: "General introduction on machine learning"
---



本学期机器学习的一个复习，还是跟大三下一样会写一个系列，就当为GRE跟师
匠攒人品了。

// 话说就课了一个礼拜的夜宵钱就出了总司什么情况……不会败人品吧……


# What is Machine Learning?
在西瓜书里，给出了这么一个定义：

> 机器学习是这样一门学科：它致力于研究如何通过计算的手段，利用**经验**来**改
> 善**系统自身的性能。

Mitchell's formal definition:

> A computer program is said to learn from experience $E$ with respect
> to some class of tasks $T$ and performance measure $P$, if its
> performance at tasks in $T$, as measured by $P$, improves with
> experience $E$.

我们可以总结出三条要点：
1. 有一个确定的*任务*
2. 根据过去的*经验*
3. *性能*需要提高

拿西瓜书里的西瓜做个例子（我也蛮喜欢吃西瓜的）：
- *任务*：(根据西瓜的特征)挑一个好瓜
- *经验*：过去挑西瓜的经验，比如说声音闷的是好瓜之类
- *性能*：给定一组西瓜的特征，正确地挑出好瓜的概率

## 没有免费的午餐定理(No Free Lunch Theorem)
在问题出现概率相同的情况下，总误差与学习算法无关，推理详见西瓜书。
在实际中，问题并不是*等概率*出现的，也就意味着我们可以针对*特定问题*可
以用机器学习方法找出一个很好的方案。

# How to design a learning system?

以神经网络识别手写数字为例：
0. 确定问题，没有确定问题学习什么呢- -
1. 寻找经验（训练集）
2. 将经验中的*特征数据*用向量表示，比如将手写数字识别的图片转换成向量：$X = transform(Image)$
3. 用向量表示经验中的*目标数据*$D$，然后将两者结合成为一个二元组来形式化
   地表示经验：$E = (X, D)$
4. 寻找一个学习算法$F(W, X)$, 其中$W$ 是学
   习算法的参数，或者说权重。用$M$来表示性能量度算法，$P$表示性能，我们有：
   $E = (X, D)$
   $F(W, X) = D'$
   $P = M(D, D')$
5. 调整$W$,使得获得更好的$P$
6. 调整好参数后，就可以系统拿去应用了。

# How to evaluate learning algorithm

在这一部分，我们会大致了解一下如何评价学习算法的表现。

## Data Split

一些较小的数据集，比如说我们用过的手写识
别[MNIST](http://yann.lecun.com/exdb/mnist/)集，会有测试集与训练集的划
分。但是，对于比较大的经验集，我们通常将其自身分成测试集(test set)
与训练集(training set)（以及可能的验证集(verification set)）

### 1.Hold-out
西瓜书里叫做留出法，是一种基础的分类方法：
1. 将数据*随机地*分成训练（，开发）与测试集
2. 在训练/测试的时候，不要用全部数据
3. 对很大的数据集来说比较有用

随机性的划分主要是为了保持数据分布的一致性，否则容易产生偏差。

留出法的问题在于，它对原始的数据集进行了一定的操作。如果测试集太大，会
导致训练集太小，得出的结果没有办法去近似在整个训练数据$D$上的结果；反
之，则会导致测试集不能很好地估计训练结果。目前而言没有完美解决方案，常
用的手法是训练集占比大约在三分之二至80%之间。

### 2.Bootstrapping
西瓜书里叫"自助法"。步骤如下：
1. 给定包含$m$个样本的数据集$D$
2. 构建测试集$D'$：每次从$D$中随机挑选一个数据，复制到$D'$中
3. 重复步骤2$m$次，我们就能得到集$D'$（注：这是极限情况）

对于某个特定的样本来说，在这$m$次采样中始终不被采到的概率为
$(1-\frac{1}{m})^m$，取极限有:

$\limits_{n \to \infty}(1 - \frac{1}{m}) = \frac{1}{e} \approx 36.8%$

在数据集足够大的情况下，初始数据集中大约有36.8%的数据是不会出现在集合
$D'$里的。于是，我们可以使用$D'$作为训练集，$D - D'$ 作为测试集。这样
做的坏处是，虽然我们的$D'$跟$D$有同样的大小，但是分布被改变了，也就会
导致训练误差。

### 3.Cross Validation
交叉验证法。其步骤如下：
1. 将数据集$D$划分成$k$个大小相等的*互斥*子集
2. 每次使用一个子集作为测试集，剩下的子集的合集作为训练集
3. 我们因而获得了$k$组测试/训练集，最终返回其结果的均值。

一般而言，我们在使用交叉验证的时候$k$值取10。在极限的情况下，$k$与数据
集的数据数量$m$相等，即每个子集有且仅有一个样本。这种情况叫做留一法
(Leave-One-Out), 好处是非常接近用整个数据集$D$训练的结果，坏处是计算开
销太过巨大，而且这还是不考虑“没有免费午餐定理”的情况下。

## Measuring Performance





### Overfitting & Underfitting

过拟合与欠拟合。

定义错误率为分类错误的样本数占总数的比例：$E = a / m$

我们同样可以定义精度：$(1 - a / m) * 100%$

更一般地，定义误差为“学习器的实际输出”与样本的真实输出之间的差异。学
习器在*训练集*上的误差称作"训练误差"(training error)，在*测试集*上的误
差称作"泛化误差"(generalization error)。
我们的目标是，获得一个*泛化误差*较小的学习器，但是我们并不能事先知道新
的样本长什么样。在实际操作中，我们采取的策略是使得经验误差最小化。但是，
我们很可能得到这样一种学习器：经验误差很小，但是在测试集上表现不佳。这
种情况，我们称作"过拟合"(overfitting)。与之相对，是“欠拟合”(underfitting).

![Stanford_Machine_Learning](http://og78s5hbx.bkt.clouddn.com/1341814477_1796.jpg)
*Overfitting* Source: Stanford Machine Learning

过拟合的本质，是学习器在训练集上学习得“太好了”，以至于没有办法对一个
更加泛化的数据集做出正确的反应。

由于我们通过学习器拟合的函数往往是多项式函数，而机器学习所解决的现实问
题往往是NP难或者以上，因此，只要我们认为$P \neq NP$，那么过拟合就*无法
避免*。我们所能做的，是尽量减少泛化误差。

通常有这么几个原因导致明显的过拟合：
1. 学习过程太长了！
2. 训练集不能较好地反映所有的情况，这是大多数场合出现过拟合的原因
3. 模型参数被调整到训练集中无效信息的部分--同目标函数根本没有正式关系
   (这段不太理解：Model parameters are adjusted to uninformative
   features in the training set that have no causal relation to the
   true underlying target function!)
   

### F1 Measure

对于二分类问题（分类只有正反两类），我们可以根据其真实类别与学习器预测的类别组合划分为：

1. 真正例(TP)：真实为正，预测为正
2. 假正例(FP)：真实为反，预测为正
3. 真反例(TN)：真实为反，预测为反
4. 假反例(FN): 真实为正，预测为反

显然，我们有：

1. $TP + FP + TN + FN$ = 样本总数
2. $TP + TN$ = 预测正确的数量
3. $TP + FP$ = 预测结果为正的数量
4. $TP + FN$ = 实际结果为正的数量

...等等。

定义*查准率*(precision, 或者也叫准确率):

$P = \frac{TP}{TP + FP}$

意义为正确地分类为正占预测分类为正的百分比。

定义*查全率*(recall, 或者也叫召回率)：

$P = \frac{TP}{TP + FN}$

意义为正确地分类为正占真实分类为正的百分比。

查准率跟查全率是一对相对的度量。我们拿挑瓜做一个直观的理解：

小明（你可以把小明看做学习器）去挑瓜，瓜有甜的跟不甜的两种类型。小明不
是天才，不能百分之百地挑出所有的甜瓜。那么，我们就有四种情况对应上面的
四个数据：
1. 小明觉得是甜的瓜，实际上是甜的瓜 -> TP
2. 小明觉得是甜的瓜，实际上是不甜的瓜 -> FP
3. 小明觉得是不甜的瓜（不挑），实际上是甜的瓜 -> FN
4. 小明觉得是不甜的瓜，实际上是不甜的瓜 -> TN

如果小明希望能够尽可能地挑出甜的瓜，即，尽量使挑到的瓜都是甜的，那么小
明可以做的事情是：尽量少挑瓜！在这种情况下，小明挑出来的瓜(TP + FP)会
很少，但是小明会对挑出瓜的甜度很有信心(极端情况下，小明只挑一个有把握
的甜瓜：查准率100%)。当然，另一个方面就是小明尽可能挑出更多的瓜，这种
情况下查全率会很高，但是甜的瓜有多少就是一个问题了。

我们衡量一个学习器的好坏，要综合查准率跟查重率两方面考虑。比如说，小刚
说他比小明更会挑瓜，理由是他挑的瓜是甜的概率大->你要考虑小刚是不是只挑
很少的有把握的瓜的那种类型。但是，如果小红说她比小明更会挑瓜，理由是小
红能够挑出更多的瓜，而且小红能挑中好瓜的概率还比小明高->那这毫无疑问是
小红比较强。

但是，我们不能保证相比较的情况都像小明跟小红那样无可争议。因此，我们需
要一个另外的度量来衡量两个人挑瓜的能力。我们可以使用平衡点（Break-Even
Point），定义为查全率与查准率相等时的值；当然我们还有更好的：F度量(F-measure)

$F_{\beta} = \frac{(1 + \beta^2) \times P \times R}{\beta^2 + R}$

这里，我们用$\beta$来控制查全率与查准率的权重。在某些时候我们希望查全
率高一点，某些时候希望查准率高一点。当$\beta$取1的时候，即为查全率与查
准率有同样的权重的时候。我们称这样的F度量为F1度量（F1-measure）。

在本质上，F度量的倒数是查全率与查准率的调和平均数：

$\frac{1}{F_{\beta}} = \frac{1}{1 + \beta^2} (\frac{1}{P} + \frac{\beta^2}{R})$

我们可以容易看出，当$\beta$值小于1的时候，查准率有更大的权重；而当
$\beta$大于1的时候，查全率有更大的权重。


### ROC(Receiver Operator Characteristic) Curve

很多学习器是为测试样本产生一个预测值，并且将之与一个分类阈值
(threshold)比较，如果大于则为正，反之则为反。

我们可以根据这个预测值将样本排序，例如将“最可能”为正的值排在前面，
“最不可能”为正的放在后面。我们可以根据情况来选择不同的阈值，但是如何
衡量排序本身的质量好坏，我们需要别的工具。

ROC曲线取“真正例率”与“假正例率”两个重要量的值，分别以他们为纵，横坐标作图。

定义真正例率（TPR）：

$TPR = \frac{TP}{TP + FN}$

定义假正例率（FPR）：

$FPR = \frac{FP}{FP + TN}$

ROC曲线为以TPR为纵轴，FPR为横轴的曲线

$TPR = ROC(FPR)$

坐标轴的对角线表示随机猜测的模型，点(0, 1)表示的是将所有正例排在反例之
前的理想模型。

ROC曲线下面包覆的面积即为AUC(Area Under ROC),经常被用作衡量学习器好坏
的一个度量。求AUC可以简单地对ROC进行加和或者求积分。

![ROC_curve](https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png)
*By BOR at the English language Wikipedia, CC BY-SA 3.0*

### Confusion Matrix
混淆矩阵。这是一个很好用的可视化工具来判断是不是正确分类，比如西瓜：

|真实情况\预测结果|正|反|
|:--:| :--: | :--: |
|正  |   TP |  FN  |
|反  |   FP |  TN  |


# References
1. 周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版
2. M.Bishop(2006), __Pattern Recognition and Machine Learning__
3. Lecture Material Topic 1, Topic 2, Topic 8




