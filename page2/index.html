<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Entry &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
</script>

<!-- Share.js -->
<link href="/assets/css/share.min.css" rel="stylesheet">
<script type="text/javascript" src="/assets/js/vendor/share.min.js"></script>
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Entry">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/page2/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/page2/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=22958285">ZERO | STAR影法師 [pixiv]</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://MeowAlienOwO.github.io/images/22958285.jpg" alt="Entry">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>喵窝[0]号机</h1>
      <h2>一个伪装成技术博客的吐槽网站。</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" title="Machine Learning Review: Decision Tree & Random Forest">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60741116.jpg" alt="Machine Learning Review: Decision Tree & Random Forest">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-06T21:08:28+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/">January 06, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" rel="bookmark" title="Machine Learning Review: Decision Tree & Random Forest" itemprop="url">Machine Learning Review: Decision Tree & Random Forest</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>

<p>死亡冲锋抽师匠！</p>

<h1 id="decision-tree">Decision Tree</h1>

<p>决策树是一个有层次(hierachical)的结构，可以通过分治法
（divide-and-conquer）来将数据分成多个类别。</p>

<p>决策树的一个特点是，可以用来处理不能用数字表示的数据，比如说西瓜的颜色
等等。
决策树学习的目标是，构建一棵树，使之能够表示决策过程。树的节点是决策点，
树的叶子是最后的分类。算法可以大致描述如下：</p>

<ol>
  <li>Input：
    <ul>
      <li>训练集$D$: ${(x_1, y_1), (x_2, y_2)….}$</li>
      <li>属性集$A$: ${a_1, a_2,…a_d}$，每个属性集有若干取值，比如说西瓜：
硬度有很硬，硬，普通，软；大小有很大，大，中，小，很小…这里的硬
度与大小就是不同的$a_n$</li>
    </ul>
  </li>
  <li>Function DecisionTreeGen($D$, $A$)</li>
  <li>生成节点Node</li>
  <li><code>if</code> $D$中所有元素属于属性$C$：
    <ol>
      <li>将Node标记为$C$类叶子，返回</li>
    </ol>
  </li>
  <li><code>if</code> $A$为空集，或者$D$的样本在$A$上均有相同取值:
    <ol>
      <li>将Node标记为叶子，其类别标记为$D$中样本数最多的类，返回</li>
    </ol>
  </li>
  <li><strong>从A中选择最优划分属性</strong>$a_m$</li>
  <li><code>for</code> $a_m$的每一个值$a_m^v$:
    <ol>
      <li>为Node生成一个分支SubNode。</li>
      <li>令$D_v$ 为训练集中在属性$a_m$上取值为$a_m^v$的样本集</li>
      <li><code>if</code> $D_v$为空集：
        <ol>
          <li>SubNode标记为叶子，类别取$D$中样本最多的类，返回</li>
        </ol>
      </li>
      <li><code>else</code>：
        <ol>
          <li>SubNode = TreeGenerate($D_v$, $A - a_m$)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>返回：以Node为根节点的决策树</li>
</ol>

<p>如上算法的3,4,6.3分别表示了决策树的三种递归停止的情形：</p>
<ol>
  <li>当前节点的样本都属于同一类别无需划分(最终划分到大小这一类，都是大的西瓜)</li>
  <li>当前属性集为空，或者所有的样本在所有属性上都有相同的取值（这里指某
一个属性没有办法作为划分属性，比如说如果有属性“是水果”–所有的西
瓜都是水果，这个属性就没有意义）</li>
  <li>当前节点包含的样本集合为空</li>
</ol>

<p>解决递归停止的情形2的方法称为“多数表决”法。</p>

<p>我们可以很明显地看出，步骤5是决定决策树算法性能的关键。</p>

<h2 id="信息熵">信息熵</h2>

<p>我们的目标是寻找一个较好的找出最优的划分属性的方法，使得其划分后的样本
能够<strong>最纯</strong>–换言之，尽可能相近的类别应该尽可能分在一起。为此，我们需
要了解信息熵。</p>

<p>信息熵是对不确定性的测量。一件事情发生的概率越低，描述其所需要的信息量
就越多。同样的，对于决策树而言，最容易划分的属性类别往往应该在高处，不
容易划分的在低处。信息熵的定义为：</p>

<script type="math/tex; mode=display">Ent(D) = - \sum\limits_{k=1}^{\|y\|}p_{k}log_{2}p_{k}</script>

<p>当p为0的时候，信息熵为0。</p>

<h2 id="id3算法与信息增益">ID3算法与信息增益</h2>

<p>但是，单单用信息熵我们是无法找到什么属性能够“最优”地划分样本集–样本
往往由多个属性复合而成，我们选取划分属性的时候应该怎么做呢？ID3算法
(iterative dichotomiser)引入了“信息增益”的概念。</p>

<p>离散属性$a$有$V$个可能的取值${a^1, a^2…a^V}$。如果使用$a$来对样本
进行划分的话，会有$V$个分支节点，每个分支节点下都包含若干属性为$a^v$的
样本，记作$D^v$。我们可以计算每个节点的信息熵，取$|D^v| / |D|$为权
重，然后对所有节点的信息熵进行加权求和。样本集$D$的信息熵与加权求和后
的结果定义为<strong>信息增益</strong>(gain)：</p>

<script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum\limits_{v = 1}^{V}\frac{\|D^v\|}{\|D\|}Ent(D^v)</script>

<p>ID3算法在每次选取属性的时候，计算所有属性的增益，然后选取增益最大的一
个属性进行划分，以此类推。</p>

<h2 id="c45算法与增益率">C4.5算法与增益率</h2>

<p>ID3算法的问题在于，它会天然地倾向于属性多的类别。比如说，如果把编号(id)作为
一类计入考虑，ID3会先划分编号–虽然划分非常纯净，但这毫无用处。为了克
服这一缺点，C4.5算法引入了“增益率”(gai ratio)的概念。增益率定义为：</p>

<script type="math/tex; mode=display">GainRatio(D, a) = \frac{Gain(D, a)}{IV(a)}</script>

<p>其中，固有值(intrinsic value)$IV$定义为：</p>

<script type="math/tex; mode=display">IV(a) = - \sum\limits_{v=1}{V} \frac{\|D^v\|}{\|D\|}log_{2}\frac{\|D^v\|}{\|D\|}</script>

<p>但是实际上增益率的处理会导致偏向取值少的类别。在实际应用中，会先用启发
式算法找出信息增益水平高于平均水平的属性，然后再选择高增益的属性。</p>

<h2 id="过拟合">过拟合</h2>

<p>决策树也会过拟合，主要的表现为分支过多，导致判断很多不必要的噪声。降低
过拟合的风险的主要手段是剪枝。剪枝的手段主要有预剪枝与后剪枝两种。预剪
枝指的是在构造树的过程中满足某些条件停止分支的构建，而后剪枝指构建完
整的决策树后再进行剪枝。</p>

<h1 id="随机森林">随机森林</h1>

<p>// 注：这是根据中文网络上比较常见的随机森林与西瓜书写的，可能跟PPT有出
// 入</p>

<h2 id="集成学习">集成学习</h2>

<p>集成学习(ensemble learning)是一种将多个学习器组合起来完成学习任务的方
法。其一般结构为，先生成一组基<span color="black">♂</span>学习器(base
learner)，然后用某种策略组合其输出。其学习算法称作基<span color="black">♂</span>学习算法(base learning algorithm)</p>

<p><img src="http://images.cnitblog.com/blog/633472/201410/181942114048093.png" alt="" /></p>

<p>集成学习的种类大致有两种：</p>
<ol>
  <li>个体间存在强依赖关系，必须串行执行。代表：Boosting</li>
  <li>个体间没有强依赖关系，可以并行执行。代表：Bagging, 随机森林
    <h2 id="baggingbootstrap-aggregating">Bagging（Bootstrap Aggregating）</h2>
  </li>
</ol>

<p>并行学习器的基本思想是，通过使基学习器之间尽量互相独立（有较大差异），
从而提高总体的泛化性能。在此，我们了解一下随机森林的基本型：Bagging算
法。</p>

<p>我们首先对样本进行Bootstrap采样：给定包含$m$个样本的样本集，每次随机抽
选一个，维持原有的采样集不变，重复$m$次，构建新的样本集。正如我们之前
所讨论的，这个样本集大约会包含原有样本集中63.2%的样本。同样的，我们可
以构建任意多个Bootstrap样本集，根据需求，我们构建$T$个样本集，每个样本
集都会包含原有样本集的63.2%。我们对每个采样集训练一个学习器，然后将这
些学习器进行结合。当结论不一致的时候，Bagging使用简单投票法。当得票数
相等时，我们可以随机选一个，或者考察学习器投票的置信率。</p>

<h2 id="随机森林算法">随机森林算法</h2>

<p>随机森林是Bagging的一个变种。随机森林以决策树作为基学习算法，在Bagging
基础上引入随机属性选择。</p>

<p>传统的决策树在选择划分属性的时候，使用一个最优化算法。但是在随机森林中，
对于基决策树的每一个节点，从原有属性集中随机选择一个包含$k$个属性的子
集，然后在这个子集中选取最优属性。$k$的取值决定了随机性的程度，当$k$与
属性集的大小相等时，就为传统决策树；当$k$为1的时候，就是随机选取属性。
一般情况使用的值为：$k=log_{2}d$</p>

<p>随机森林的特性是，起始性能比较差，但是当学习器数量增加时，随机森林的泛
化误差往往比较低。</p>

<h1 id="reference">Reference</h1>

<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>CodingLabs, 算法杂货铺——分类算法之决策树(Decision tree),
http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</li>
</ol>
 -->
    
        <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/" title="Machine Learning Review: Decision Tree & Random Forest">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60741116.jpg" alt="Machine Learning Review: Decision Tree & Random Forest">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-06T21:08:28+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/">January 06, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/" rel="bookmark" title="Machine Learning Review: Decision Tree & Random Forest" itemprop="url">Machine Learning Review: Decision Tree & Random Forest</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>

<p>死亡冲锋抽师匠！</p>

<h1 id="decision-tree">Decision Tree</h1>

<p>决策树是一个有层次(hierachical)的结构，可以通过分治法
（divide-and-conquer）来将数据分成多个类别。</p>

<p>决策树的一个特点是，可以用来处理不能用数字表示的数据，比如说西瓜的颜色
等等。
决策树学习的目标是，构建一棵树，使之能够表示决策过程。树的节点是决策点，
树的叶子是最后的分类。算法可以大致描述如下：</p>

<ol>
  <li>Input：
    <ul>
      <li>训练集$D$: ${(x_1, y_1), (x_2, y_2)….}$</li>
      <li>属性集$A$: ${a_1, a_2,…a_d}$，每个属性集有若干取值，比如说西瓜：
硬度有很硬，硬，普通，软；大小有很大，大，中，小，很小…这里的硬
度与大小就是不同的$a_n$</li>
    </ul>
  </li>
  <li>Function DecisionTreeGen($D$, $A$)</li>
  <li>生成节点Node</li>
  <li><code>if</code> $D$中所有元素属于属性$C$：
    <ol>
      <li>将Node标记为$C$类叶子，返回</li>
    </ol>
  </li>
  <li><code>if</code> $A$为空集，或者$D$的样本在$A$上均有相同取值:
    <ol>
      <li>将Node标记为叶子，其类别标记为$D$中样本数最多的类，返回</li>
    </ol>
  </li>
  <li><strong>从A中选择最优划分属性</strong>$a_m$</li>
  <li><code>for</code> $a_m$的每一个值$a_m^v$:
    <ol>
      <li>为Node生成一个分支SubNode。</li>
      <li>令$D_v$ 为训练集中在属性$a_m$上取值为$a_m^v$的样本集</li>
      <li><code>if</code> $D_v$为空集：
        <ol>
          <li>SubNode标记为叶子，类别取$D$中样本最多的类，返回</li>
        </ol>
      </li>
      <li><code>else</code>：
        <ol>
          <li>SubNode = TreeGenerate($D_v$, $A - a_m$)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>返回：以Node为根节点的决策树</li>
</ol>

<p>如上算法的3,4,6.3分别表示了决策树的三种递归停止的情形：</p>
<ol>
  <li>当前节点的样本都属于同一类别无需划分(最终划分到大小这一类，都是大的西瓜)</li>
  <li>当前属性集为空，或者所有的样本在所有属性上都有相同的取值（这里指某
一个属性没有办法作为划分属性，比如说如果有属性“是水果”–所有的西
瓜都是水果，这个属性就没有意义）</li>
  <li>当前节点包含的样本集合为空</li>
</ol>

<p>解决递归停止的情形2的方法称为“多数表决”法。</p>

<p>我们可以很明显地看出，步骤5是决定决策树算法性能的关键。</p>

<h2 id="信息熵">信息熵</h2>

<p>我们的目标是寻找一个较好的找出最优的划分属性的方法，使得其划分后的样本
能够<strong>最纯</strong>–换言之，尽可能相近的类别应该尽可能分在一起。为此，我们需
要了解信息熵。</p>

<p>信息熵是对不确定性的测量。一件事情发生的概率越低，描述其所需要的信息量
就越多。同样的，对于决策树而言，最容易划分的属性类别往往应该在高处，不
容易划分的在低处。信息熵的定义为：</p>

<script type="math/tex; mode=display">Ent(D) = - \sum\limits_{k=1}^{\|y\|}p_{k}log_{2}p_{k}</script>

<p>当p为0的时候，信息熵为0。</p>

<h2 id="id3算法与信息增益">ID3算法与信息增益</h2>

<p>但是，单单用信息熵我们是无法找到什么属性能够“最优”地划分样本集–样本
往往由多个属性复合而成，我们选取划分属性的时候应该怎么做呢？ID3算法
(iterative dichotomiser)引入了“信息增益”的概念。</p>

<p>离散属性$a$有$V$个可能的取值${a^1, a^2…a^V}$。如果使用$a$来对样本
进行划分的话，会有$V$个分支节点，每个分支节点下都包含若干属性为$a^v$的
样本，记作$D^v$。我们可以计算每个节点的信息熵，取$|D^v| / |D|$为权
重，然后对所有节点的信息熵进行加权求和。样本集$D$的信息熵与加权求和后
的结果定义为<strong>信息增益</strong>(gain)：</p>

<script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum\limits_{v = 1}^{V}\frac{\|D^v\|}{\|D\|}Ent(D^v)</script>

<p>ID3算法在每次选取属性的时候，计算所有属性的增益，然后选取增益最大的一
个属性进行划分，以此类推。</p>

<h2 id="c45算法与增益率">C4.5算法与增益率</h2>

<p>ID3算法的问题在于，它会天然地倾向于属性多的类别。比如说，如果把编号(id)作为
一类计入考虑，ID3会先划分编号–虽然划分非常纯净，但这毫无用处。为了克
服这一缺点，C4.5算法引入了“增益率”(gai ratio)的概念。增益率定义为：</p>

<script type="math/tex; mode=display">GainRatio(D, a) = \frac{Gain(D, a)}{IV(a)}</script>

<p>其中，固有值(intrinsic value)$IV$定义为：</p>

<script type="math/tex; mode=display">IV(a) = - \sum\limits_{v=1}{V} \frac{\|D^v\|}{\|D\|}log_{2}\frac{\|D^v\|}{\|D\|}</script>

<p>但是实际上增益率的处理会导致偏向取值少的类别。在实际应用中，会先用启发
式算法找出信息增益水平高于平均水平的属性，然后再选择高增益的属性。</p>

<h2 id="过拟合">过拟合</h2>

<p>决策树也会过拟合，主要的表现为分支过多，导致判断很多不必要的噪声。降低
过拟合的风险的主要手段是剪枝。剪枝的手段主要有预剪枝与后剪枝两种。预剪
枝指的是在构造树的过程中满足某些条件停止分支的构建，而后剪枝指构建完
整的决策树后再进行剪枝。</p>

<h1 id="随机森林">随机森林</h1>

<p>// 注：这是根据中文网络上比较常见的随机森林与西瓜书写的，可能跟PPT有出入</p>

<h2 id="集成学习">集成学习</h2>

<p>集成学习(ensemble learning)是一种将多个学习器组合起来完成学习任务的方
法。其一般结构为，先生成一组基<span color="black">♂</span>学习器(base
learner)，然后用某种策略组合其输出。其学习算法称作基<span color="black">♂</span>学习算法(base learning algorithm)</p>

<p><img src="http://images.cnitblog.com/blog/633472/201410/181942114048093.png" alt="" /></p>

<p>集成学习的种类大致有两种：</p>
<ol>
  <li>个体间存在强依赖关系，必须串行执行。代表：Boosting</li>
  <li>个体间没有强依赖关系，可以并行执行。代表：Bagging, 随机森林
    <h2 id="baggingbootstrap-aggregating">Bagging（Bootstrap Aggregating）</h2>
  </li>
</ol>

<p>并行学习器的基本思想是，通过使基学习器之间尽量互相独立（有较大差异），
从而提高总体的泛化性能。在此，我们了解一下随机森林的基本型：Bagging算
法。</p>

<p>我们首先对样本进行Bootstrap采样：给定包含$m$个样本的样本集，每次随机抽
选一个，维持原有的采样集不变，重复$m$次，构建新的样本集。正如我们之前
所讨论的，这个样本集大约会包含原有样本集中63.2%的样本。同样的，我们可
以构建任意多个Bootstrap样本集，根据需求，我们构建$T$个样本集，每个样本
集都会包含原有样本集的63.2%。我们对每个采样集训练一个学习器，然后将这
些学习器进行结合。当结论不一致的时候，Bagging使用简单投票法。当得票数
相等时，我们可以随机选一个，或者考察学习器投票的置信率。</p>

<h2 id="随机森林算法">随机森林算法</h2>

<p>随机森林是Bagging的一个变种。随机森林以决策树作为基学习算法，在Bagging
基础上引入随机属性选择。</p>

<p>传统的决策树在选择划分属性的时候，使用一个最优化算法。但是在随机森林中，
对于基决策树的每一个节点，从原有属性集中随机选择一个包含$k$个属性的子
集，然后在这个子集中选取最优属性。$k$的取值决定了随机性的程度，当$k$与
属性集的大小相等时，就为传统决策树；当$k$为1的时候，就是随机选取属性。
一般情况使用的值为：$k=log_{2}d$</p>

<p>随机森林的特性是，起始性能比较差，但是当学习器数量增加时，随机森林的泛
化误差往往比较低。</p>

<h1 id="reference">Reference</h1>

<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>CodingLabs, 算法杂货铺——分类算法之决策树(Decision tree),
http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</li>
</ol>
 -->
    
        <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/" title="Machine Learning Review: SVM & PCA">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60095827_p0.jpg" alt="Machine Learning Review: SVM & PCA">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2016-12-31T14:23:05+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/">December 31, 2016</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/" rel="bookmark" title="Machine Learning Review: SVM & PCA" itemprop="url">Machine Learning Review: SVM & PCA</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>新年快乐，跨年也是元气满满地在机房～</p>

<p>就不按照上课顺序了，从自己搞不清楚的部分写起。在考SQM之前姑且看看能不
能再复习一遍决策树吧。</p>

<h1 id="支持向量机-support-vector-machine">支持向量机 Support Vector Machine</h1>

<h2 id="线性分类器">线性分类器</h2>

<p>分类学习的基本想法是，在训练集$D$的样本空间中，寻找到一个超平面
(hyperplane)，从而将数据分成两类。感知机(perceptron)就是一个典型的线性
分类器。</p>

<p>线性分类器的一般方程可以表示如下：</p>

<p>$\mathbf{w}^T\mathbf{x} + b = 0$</p>

<p>其中，$\mathbf{w}$是权重向量，$\mathbf{x}$是特征向量。另外，在数学意义上，
$\mathbf{w}$是由方程$\mathbf{w}^T\mathbf{x} + b = 0$所表示的超平面的法向量，
$b$为该超平面的截距。</p>

<h2 id="支持向量机">支持向量机</h2>

<p>应用线性分类器的时候，我们常常会碰到这样一种情况：</p>

<p><img src="https://computersciencesource.files.wordpress.com/2010/01/svmafter_thumb.png?w=230&amp;h=240" alt="best-svm" /></p>

<p>线性分类器可能的取值在<strong>训练</strong>集上不止一个，那么我们如何寻找__最优__的
线性分类器呢？</p>

<p>注意到样本空间中任意一点$x$到超平面的距离为：</p>

<table>
  <tbody>
    <tr>
      <td>$r = \frac{</td>
      <td>\mathbf{w}^T\mathbf{x} + b</td>
      <td>}{</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>复习一下解析几何：</p>

<p>在二维情况下的情形是点到直线的距离:</p>

<p>$r = \frac{ax + by +c}{\sqrt{a^2 + b^2}}$</p>

<p>三维情况下点到平面的距离：</p>

<p>$r = \frac{Ax +By + Cz + D}{\sqrt{A^2 + B^2 + C^2}}$</p>

<table>
  <tbody>
    <tr>
      <td>我们不难推广到高维，其中$</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>$表示向量的模。</td>
    </tr>
  </tbody>
</table>

<p>另外，我们还有一个关于距离的定义：</p>

<blockquote>
  <p>两个非空集合间的距离为两者内各自的点之间的距离之下确界。</p>
</blockquote>

<p>将直线看做点集，点看做只有一个元素的点集，就是点与直线的距离公式。同样，
我们可以将其应用于直线与直线，直线与平面…
这个定义在测试中的基于距离的自适应测试（DART）以及KNN算法里都有应用。</p>

<p>距离超平面最近的样本点为__支持向量__(support vector)。假定超平面可以将训练样本正确分类，
那么我们可以得到：对分类结果有明显影响的是__支持向量__，而样本中其他的
样本点可以忽略不计。定义__间距__(margin)为两个分别在超平面不同边的支持向量到超
平面的距离之和，那么，为了寻找一个__最好__的超平面，我们需要找到有__最大__间
距的超平面。</p>

<p>如果我们令样本分成两类$y = {-1, 1}$,我们可以得到：</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \geq \rho / 2, if y_i \geq 1$</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \leq - \rho / 2, if y_i \leq -1$</p>

<p>其中$\rho$为间距。由于在这种情况下，支持向量到超平面的距离可以表示为：</p>

<table>
  <tbody>
    <tr>
      <td>$r = \frac{</td>
      <td>(\mathbf{w}^T\mathbf{x}) + b</td>
      <td>}{</td>
      <td> </td>
      <td>\mathbf{w}</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>我们可以得到：</p>

<table>
  <tbody>
    <tr>
      <td>$\rho = 2r = \frac{2}{</td>
      <td> </td>
      <td>\mathbf{w}</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>于是我们的问题便可以转化为：求取超平面的法向量$\mathbf{w}$的模
的最小值，且使得所有的样本$(\mathbf{x}_i, y_i)$有
$y_i{\mathbf{w}^T\mathbf{x}_i + b &gt; 1}$。这就是支持向量机的基本模式:</p>

<table>
  <tbody>
    <tr>
      <td>$min_{w,b}\frac{1}{2}</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>^2$</td>
    </tr>
  </tbody>
</table>

<p>$s.t. y_i(\mathbf{w}^T\mathbf{x}_i+b) \geq 1, i=1,2,…,m$</p>

<p>于是，我们的问题转换成了一个二次规划(quadratic optimization)问题。</p>

<h2 id="有约束的二次规划最优化问题拉格朗日乘子">有约束的二次规划最优化问题，拉格朗日乘子</h2>

<p>我们有许多程序包求解二次规划问题，通常而言不需要自己动手。但是了解一下
其中的数学意义还是非常有帮助的。</p>

<p>常用的解二次规划的方法之一，是应
用<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">拉格朗日乘子</a>
(Lagrange Multiplier)。</p>

<p>首先我们来看一下二次规划问题的几何意义。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Lagrange_multiplier.png" alt="" /></p>

<p>我们有约束条件$g(x,y) = c$，在图中表示成一条曲线；二次函数$f(x,y)$如
图，描述了一个曲面。假设这个曲面是一座山，约束条件的曲线是我们可以沿着
走的路，我们的目的就变成了：在这条路上，我们要走到最高的地方。图中的圆
圈表示二次函数的<a href="https://en.wikipedia.org/wiki/Level_set">水平集</a>（可
以直观地理解为等高线）。在__有解__的情况下，二次曲面与约束曲线会有交点。</p>

<p>我们可以容易看出，在（局部）最优的情况下,$f(x, y)$与$g(x,y)$的交点有且
仅有一个，即二者的__切线__相互平行。由于切线与梯度是垂直的，我们可以得
到：在最优点的情况下，二者的__梯度__相互平行。</p>

<p>我们引入一个未知标量–拉格朗日乘子$\alpha$。构建函数：</p>

<script type="math/tex; mode=display">L(\mathbf{w}, b, \mathbf{a}) = 
\frac{1}{2}||\mathbf{w}||^2 +\sum\limits_{i=1}{m}\alpha_i(1 -
y_i(\mathbf{w}^T\mathbf{x}_i + b))</script>

<p>令函数$L$对$\mathbf{w}$与$b$的偏导数为0：</p>

<script type="math/tex; mode=display"></script>

<p>$$</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>知乎 “拉格朗日乘子法如何理解？”，https://www.zhihu.com/question/38586401</li>
  <li>July(2012) 支持向量机通俗导论，http://blog.csdn.net/v_july_v/article/details/7624837</li>
</ol>

 -->
    
        <p>新年快乐，跨年也是元气满满地在机房～</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_svm_&_pca/" title="Machine Learning Review: SVM & PCA">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60319234_p0.jpg" alt="Machine Learning Review: SVM & PCA">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2016-12-31T14:23:05+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_svm_&_pca/">December 31, 2016</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_svm_&_pca/" rel="bookmark" title="Machine Learning Review: SVM & PCA" itemprop="url">Machine Learning Review: SVM & PCA</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>新年快乐，跨年也是元气满满地在机房～</p>

<p>就不按照上课顺序了，从自己搞不清楚的部分写起。在考SQM之前姑且看看能不
能再复习一遍决策树吧。</p>

<h1 id="支持向量机-support-vector-machine">支持向量机 Support Vector Machine</h1>

<h2 id="线性分类器">线性分类器</h2>

<p>分类学习的基本想法是，在训练集$D$的样本空间中，寻找到一个超平面
(hyperplane)，从而将数据分成两类。感知机(perceptron)就是一个典型的线性
分类器。</p>

<p>线性分类器的一般方程可以表示如下：</p>

<p>$\mathbf{w}^T\mathbf{x} + b = 0$</p>

<p>其中，$\mathbf{w}$是权重向量，$\mathbf{x}$是特征向量。另外，在数学意义上，
$\mathbf{w}$是由方程$\mathbf{w}^T\mathbf{x} + b = 0$所表示的超平面的法向量，
$b$为该超平面的截距。</p>

<h2 id="支持向量机">支持向量机</h2>

<p>应用线性分类器的时候，我们常常会碰到这样一种情况：</p>

<p><img src="https://computersciencesource.files.wordpress.com/2010/01/svmafter_thumb.png?w=230&amp;h=240" alt="best-svm" /></p>

<p>线性分类器可能的取值在<strong>训练</strong>集上不止一个，那么我们如何寻找<strong>最优</strong>的
线性分类器呢？</p>

<p>注意到样本空间中任意一点$x$到超平面的距离为：</p>

<p>$r = \frac{|\mathbf{w}^T\mathbf{x} + b|}{||w||}$</p>

<p>复习一下解析几何：</p>

<p>在二维情况下的情形是点到直线的距离:</p>

<p>$r = \frac{ax + by +c}{\sqrt{a^2 + b^2}}$</p>

<p>三维情况下点到平面的距离：</p>

<p>$r = \frac{Ax +By + Cz + D}{\sqrt{A^2 + B^2 + C^2}}$</p>

<p>我们不难推广到高维，其中$||w||$表示向量的模。</p>

<p>另外，我们还有一个关于距离的定义：</p>

<blockquote>
  <p>两个非空集合间的距离为两者内各自的点之间的距离之下确界。</p>
</blockquote>

<p>将直线看做点集，点看做只有一个元素的点集，就是点与直线的距离公式。同样，
我们可以将其应用于直线与直线，直线与平面…
这个定义在测试中的基于距离的自适应测试（DART）以及KNN算法里都有应用。</p>

<p>距离超平面最近的样本点为<strong>支持向量</strong>(support vector)。假定超平面可以将训练样本正确分类，
那么我们可以得到：对分类结果有明显影响的是<strong>支持向量</strong>，而样本中其他的
样本点可以忽略不计。定义<strong>间距</strong>(margin)为两个分别在超平面不同边的支持向量到超
平面的距离之和，那么，为了寻找一个<strong>最好</strong>的超平面，我们需要找到有<strong>最大</strong>间
距的超平面。</p>

<p>如果我们令样本分成两类$y = {-1, 1}$,我们可以得到：</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \geq \rho / 2, if y_i \geq 1$</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \leq - \rho / 2, if y_i \leq -1$</p>

<p>其中$\rho$为间距。由于在这种情况下，支持向量到超平面的距离可以表示为：</p>

<p>$r = \frac{|(\mathbf{w}^T\mathbf{x}) + b|}{||\mathbf{w}||}$</p>

<p>我们可以得到：</p>

<p>$\rho = 2r = \frac{2}{||\mathbf{w}||}$</p>

<p>于是我们的问题便可以转化为：求取超平面的法向量$\mathbf{w}$的模
的最小值，且使得所有的样本$(\mathbf{x}_i, y_i)$有
$y_i{\mathbf{w}^T\mathbf{x}_i + b &gt; 1}$。这就是支持向量机的基本模式:</p>

<p>$min_{w,b}\frac{1}{2}||w||^2$</p>

<p>$s.t. y_i(\mathbf{w}^T\mathbf{x}_i+b) \geq 1, i=1,2,…,m$</p>

<p>于是，我们的问题转换成了一个二次规划(quadratic optimization)问题。</p>

<h2 id="有约束的二次规划最优化问题拉格朗日乘子">有约束的二次规划最优化问题，拉格朗日乘子</h2>

<p>我们有许多程序包求解二次规划问题，通常而言不需要自己动手。但是了解一下
其中的数学意义还是非常有帮助的。</p>

<p>常用的解二次规划的方法之一，是应
用<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">拉格朗日乘子</a>
(Lagrange Multiplier)。</p>

<p>首先我们来看一下二次规划问题的几何意义。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Lagrange_multiplier.png" alt="" /></p>

<p>我们有约束条件$g(x,y) = c$，在图中表示成一条曲线；二次函数$f(x,y)$如
图，描述了一个曲面。假设这个曲面是一座山，约束条件的曲线是我们可以沿着
走的路，我们的目的就变成了：在这条路上，我们要走到最高的地方。图中的圆
圈表示二次函数的<a href="https://en.wikipedia.org/wiki/Level_set">水平集</a>（可
以直观地理解为等高线）。在<strong>有解</strong>的情况下，二次曲面与约束曲线会有交点。</p>

<p>我们可以容易看出，在（局部）最优的情况下,$f(x, y)$与$g(x,y)$的交点有且
仅有一个，即二者的<strong>切线</strong>相互平行。由于切线与梯度是垂直的，我们可以得
到：在最优点的情况下，二者的<strong>梯度</strong>相互平行。</p>

<p>我们引入一个未知标量–拉格朗日乘子$\alpha$。构建函数：</p>

<script type="math/tex; mode=display">L(\mathbf{w}, b, \mathbf{a}) = 
\frac{1}{2}\|\mathbf{w}\|^2 +\sum\limits_{i=1}^{m}\alpha_i(1 -
y_i(\mathbf{w}^T\mathbf{x}_i + b))</script>

<p>令函数$L$对$\mathbf{w}$与$b$的偏导数为0：</p>

<script type="math/tex; mode=display">\mathbf{w} = \sum\limits_{i=1}{m}\alpha_i y_i \mathbf{x}_i\\
0 = \sum\limits_{i=1}^{m}\alpha_i y_i</script>

<p>将上述两式代入原来的函数$L$得到函数$L’$:</p>

<script type="math/tex; mode=display">L' = \sum\limits_{i=1}^{m} - \frac{1}{2}
\sum\limits_{i=1}^{m}\sum\limits_{j=1}{m} \alpha_i \alpha_j y_i y_j
\mathbf{x}_i \mathbf{x}_j</script>

<p>从而，问题也转换成了原问题的拉格朗日对偶问题(Lagrange dual problem)：
求$\alpha$,使得函数$L’$有最大值，并且符合约束条件$\sum\alpha_i y_i = 0$
另外，我们还需要该函数满足KKT条件：</p>

<script type="math/tex; mode=display">\alpha_i \geq 0; \\
y_i f(\mathbf{x}_i) - 1 \geq 0;\\
\alpha_i(y_i f(x_i) -1) = 0.</script>

<p>解出该二次规划后，我们可以得到模型：</p>

<script type="math/tex; mode=display">f(x) = \mathbf{w}^T\mathbf{x} + b \\
= \sum\limits_{i=1}^{m}\alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b</script>

<p>其中，每一个非零的$\alpha_i$都对应一个支持向量$x_i$。</p>

<p>对于支持向量机的开销，我们要记住以下几点：</p>
<ol>
  <li>我们要计算支持向量与所有测试点$x$的内积</li>
  <li>我们要计算所有的样本点$x$的模</li>
</ol>

<p>这两点使得支持向量机的开销在面对大量的数据点时会变得比较糟糕。</p>

<h2 id="软间隔与正则化">软间隔与正则化</h2>

<p>在理想情况下，支持向量机应当能够对所有的样本正确分类；但实际上这并不一
定能够做到。解决的方法是，我们“容忍”一些样本点不被正确分类，即一些样
本点不满足：</p>

<script type="math/tex; mode=display">y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1</script>

<p>将原来的优化目标重写为：</p>

<script type="math/tex; mode=display">\min\limits_{\mathbf{x}, b} \frac{1}{2} \|\mathbf{w}\|^2 +
C\sum\limits_{i=1}^{m}lost(y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1)</script>

<p>其中，$lost$ 是一个损失函数，有多种定义。如果采用hinge损失，即将损
失函数定义为：
<script type="math/tex">lost_{hinge}(z) = max(0, 1-z)</script>
<img src="http://www.csuldw.com/assets/articleImg/4DFDU.png" alt="多种损失函数" /></p>

<p>引入松弛变量$\xi$(slack vairable),我们可以将其改写为：</p>

<script type="math/tex; mode=display">L(\mathbf{w}) = \mathbf{w}^T\mathbf{w} + C\sum\xi_{i} \\
y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi{i}, \xi_i \geq 0</script>

<p>其中，常数C可以被认为是一个用于控制过拟合与正确率之间的参数。由于每个
样本都有一二对应的松弛变量，我们同样需要类似地引入另一个拉格朗日算子来
化简变量。</p>

<h2 id="非线性svm核函数与升维方法">非线性SVM，核函数与升维方法</h2>

<p>我们之前讨论的SVM有一个前提：样本空间是线性可分的。但是，我们存在很多
问题是<strong>线性不可分</strong>的，比如说异或。对于这种问题，我们通常将其映射到一
个更高维的特征空间，使其线性可分。如果原始空间是有限维，那么一定存在一
个高维特征空间，使得样本可分。</p>

<p><img src="http://img.my.csdn.net/uploads/201304/03/1364952814_3505.gif" alt="" /></p>

<p>注意到线性分类器依赖于向量的内积：$K(\mathbf{x}_i, \mathbf{x}_j)$，我
们有升维变换
$\varphi\mathbf{x}\rightarrow\varphi(\mathbf{x}_i)^T\varphi(\mathbf{x}_j)$
。由于升维后的特征空间维数可能很高，我们直接计算这个升维变换会变得比较
困难。</p>

<p><strong>核函数</strong>(kernel function)是一个等价于
$\varphi(\mathbf{x_i})^T\varphi(\mathbf{x_j})$ 的函数。通过直接应用核
函数，我们就不需要去计算高维乃至无穷维特征的内积。核函数隐式地将低维空
间的点映射到高维空间，就不需要显式地计算高维空间的内积了。</p>

<p>如果我们知道映射$\varphi$的具体形式，我们就可以很容易地找到核函数。但
是，我们常常不知道具体的映射方式。同时，单纯地用原始的映射构造高维空间
有可能导致维数爆炸（比如July在他的文章里写的，两个同心圆分布+噪声的数
据的映射有可能是2维到5维，维度更高的会更加糟糕）。</p>

<p>我们有如下定理：</p>

<blockquote>
  <p>令$\chi$为输入空间，$k$是定义在$\chi * \chi$上的对称函数，则函数$k$
是核函数，当且仅当对于任意数据$D = {x_1, x_2,…,x_m}$, 核矩阵
(kernel matrix)总是半正定(semi-positive)的。</p>
</blockquote>

<p>(矩阵见PPT)</p>

<p>也就是说，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。</p>

<p>一些常见的核函数：</p>
<ol>
  <li>线性核：$k(x_i, x_j) = x_i^T x_j$</li>
  <li>多项式核：$k(x_i, x_j) = (x_i^Tx_j)^d$</li>
  <li>高斯核：$k(x_i, x_j) = exp(-\frac{|x_i - x_j |^2}{2\sigma})$</li>
</ol>

<h1 id="主成分分析pca">主成分分析(PCA)</h1>

<p>同升维方法相反，PCA是一种降维方法<del>二向箔</del>。比如在面部识别的时
候，常用的方法是用一个遮罩(mask)对当前图像的像素进行逐一扫描。这时特征
空间的维度会非常高，以至于难以处理。在这种情况下，我们需要缩减特征的数
量，也就是进行降维处理。</p>

<p>复习一下矩阵，我们要将一个高维向量(N维)缩减成低维(n维)：</p>

<script type="math/tex; mode=display">Y = AX</script>

<p>其中$Y$是转换后的低维向量，$A$是n*N转换矩阵。</p>

<p>PCA的数学定义：</p>

<blockquote>
  <p>一个正交化线性变换，将数据变换到一个新的坐标系统中，使得这个数据的任
何投影的第一大方差在第一个坐标（第一主成分）上，第二大方差在第二个坐
标（第二主成分）上…</p>
</blockquote>

<p>我们可以想象一个超平面，在这个超平面上，样本的投影能分开，这样，我们就
只需要关心超平面上的样本就行了–这意味着，我们不需要处理更高维的样本空
间了。于是，我们关心两个性质：</p>
<ol>
  <li>最近重构性：所有的样本点尽量靠近超平面</li>
  <li>最大可分性：样本点在这个超平面上的投影尽可能分开</li>
</ol>

<p>两种方式都可以推出主成分分析，但是我们主要从最大可分性的角度入手。我们
要使得投影点分得最开，就要让方差最大。</p>

<h2 id="协方差矩阵">协方差矩阵</h2>

<p>在统计学上，计算不同变量之间的方差叫做<strong>协方差</strong>。假设我们有两个变量
$x$，$y$，我们希望探索他们之间的相关程度：</p>

<script type="math/tex; mode=display">cov(x, y) = \frac{\sum\limits_{i=1}^{n}(x_i - \overline{x})(y_i -
\overline{y})}{n - 1}</script>

<p>容易看出，方差就是协方差的一种特殊形式。</p>

<p>对于一个m*n的样本矩阵$M$，每行为一个样本，每列为一个维度，我们定义协方差矩
阵$C$，使得矩阵中一个元素$C_{ij}$有：</p>

<script type="math/tex; mode=display">C_{ij} = cov(M_{:,i}, M_{:,j}) \\
= \frac{\sum(M_{:,i} - \overline{M_{:,i}})(M_{:,j} - \overline{M_{:,j}})}{m - 1}</script>

<p>特别注明一下，协方差矩阵计算的是<strong>维度</strong>之间的协方差，不是单个数据的。</p>

<p>在实际应用的时候，我们会先对样本矩阵进行处理：对于每一个数据，减去该
列的均值。我们称这种方法为中心化处理。经过中心化处理后，不难看出，协方
差矩阵是样本矩阵及其转置的乘积，再除以标量$m-1$，即样本数量。</p>

<h2 id="求解转换矩阵">求解转换矩阵</h2>

<p>// 这一部分主要翻译PPT,西瓜书，网上博客都各种各样的推导方法，我很好奇</p>

<p>// <del>好骑</del></p>

<p>我们可以看出，如果我们想尽量的让维度之间无关（分得更开），我们应该将矩
阵的协方差部分（非对角线部分）尽量为0，为此，我们需要将协方差矩阵<strong>对
角化</strong>（注，这里对为什么要对角化以及特征值的意义很模糊……有懂的人教教
我）</p>

<p>令$P$表示转换矩阵，推导如下：</p>

<script type="math/tex; mode=display">S_y = \frac{1}{n - 1} YY^T = \frac{1}{n-1}(PX)(PX)^T \\
    = \frac{1}{n-1}P X X^T P^T \\
    = \frac{1}{n-1}P (X X^T) P^T \\
    = \frac{1}{n-1}P A P^T \\
    where A = XX^T</script>

<p>这里，我们要让最终转换后的协方差矩阵成为<strong>对角矩阵</strong>，即各个维度之
间协方差为0。</p>

<p>考
虑<a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">特征值分解</a>矩
阵：$A = P’DP’^T$，我们可以发现，中间的$D$显然是一个对角矩阵。将
$A=XX^T$进行特征值分解，我们得到：</p>

<script type="math/tex; mode=display">S_y = \frac{1}{n-1} PP'DP'^TP^T</script>

<p>显然，当$P = P’^T$的时候，我们有$S_y = \frac{1}{n-1}D$。鉴于我们在做PCA的时候认为所有的基础向量都是正交的
(PCA assumes all basis vectors are orthonormal)，我们的结果是，问题转换
成了求矩阵$P’^T$。于是，PCA的步骤如下：</p>

<ol>
  <li>将样本矩阵中心化</li>
  <li>计算样本的协方差矩阵$XX^T$</li>
  <li>对$XX^T$做特征值分解</li>
  <li>取若干个最大的特征值所对应的特征向量，组成转换矩阵。其他特征值小的
被认为不是“主成分”，被丢弃。</li>
</ol>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>知乎 “拉格朗日乘子法如何理解？”，https://www.zhihu.com/question/38586401</li>
  <li>July(2012) 支持向量机通俗导论，http://blog.csdn.net/v_july_v/article/details/7624837</li>
  <li>D.W’s Notes, 机器学习-损失函数，http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</li>
  <li>古剑寒 PCA详解，https://my.oschina.net/gujianhan/blog/225241</li>
  <li>进击的马斯特 再谈协方差矩阵之主成分分析http://pinkyjie.com/2011/02/24/covariance-pca/</li>
</ol>
 -->
    
        <p>新年快乐，跨年也是元气满满地在机房～</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-introduction/" title="Machine Learning Review:Introduction">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60095827_p0.jpg" alt="Machine Learning Review:Introduction">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2016-12-29T19:20:50+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-introduction/">December 29, 2016</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-introduction/" rel="bookmark" title="Machine Learning Review:Introduction" itemprop="url">Machine Learning Review:Introduction</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>本学期机器学习的一个复习，还是跟大三下一样会写一个系列，就当为GRE跟师
匠攒人品了。</p>

<p>// 话说就课了一个礼拜的夜宵钱就出了总司什么情况……不会败人品吧……</p>

<h1 id="what-is-machine-learning">What is Machine Learning?</h1>
<p>在西瓜书里，给出了这么一个定义：</p>

<blockquote>
  <p>机器学习是这样一门学科：它致力于研究如何通过计算的手段，利用<strong>经验</strong>来<strong>改
善</strong>系统自身的性能。</p>
</blockquote>

<p>Mitchell’s formal definition:</p>

<blockquote>
  <p>A computer program is said to learn from experience $E$ with respect
to some class of tasks $T$ and performance measure $P$, if its
performance at tasks in $T$, as measured by $P$, improves with
experience $E$.</p>
</blockquote>

<p>我们可以总结出三条要点：</p>
<ol>
  <li>有一个确定的<em>任务</em></li>
  <li>根据过去的<em>经验</em></li>
  <li><em>性能</em>需要提高</li>
</ol>

<p>拿西瓜书里的西瓜做个例子（我也蛮喜欢吃西瓜的）：</p>
<ul>
  <li><em>任务</em>：(根据西瓜的特征)挑一个好瓜</li>
  <li><em>经验</em>：过去挑西瓜的经验，比如说声音闷的是好瓜之类</li>
  <li><em>性能</em>：给定一组西瓜的特征，正确地挑出好瓜的概率</li>
</ul>

<h2 id="没有免费的午餐定理no-free-lunch-theorem">没有免费的午餐定理(No Free Lunch Theorem)</h2>
<p>在问题出现概率相同的情况下，总误差与学习算法无关，推理详见西瓜书。
在实际中，问题并不是<em>等概率</em>出现的，也就意味着我们可以针对<em>特定问题</em>可
以用机器学习方法找出一个很好的方案。</p>

<h1 id="how-to-design-a-learning-system">How to design a learning system?</h1>

<p>以神经网络识别手写数字为例：</p>
<ol>
  <li>确定问题，没有确定问题学习什么呢- -</li>
  <li>寻找经验（训练集）</li>
  <li>将经验中的<em>特征数据</em>用向量表示，比如将手写数字识别的图片转换成向量：$X = transform(Image)$</li>
  <li>用向量表示经验中的<em>目标数据</em>$D$，然后将两者结合成为一个二元组来形式化
地表示经验：$E = (X, D)$</li>
  <li>寻找一个学习算法$F(W, X)$, 其中$W$ 是学
习算法的参数，或者说权重。用$M$来表示性能量度算法，$P$表示性能，我们有：
$E = (X, D)$
$F(W, X) = D’$
$P = M(D, D’)$</li>
  <li>调整$W$,使得获得更好的$P$</li>
  <li>调整好参数后，就可以系统拿去应用了。</li>
</ol>

<h1 id="how-to-evaluate-learning-algorithm">How to evaluate learning algorithm</h1>

<p>在这一部分，我们会大致了解一下如何评价学习算法的表现。</p>

<h2 id="data-split">Data Split</h2>

<p>一些较小的数据集，比如说我们用过的手写识
别<a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>集，会有测试集与训练集的划
分。但是，对于比较大的经验集，我们通常将其自身分成测试集(test set)
与训练集(training set)（以及可能的验证集(verification set)）</p>

<h3 id="1hold-out">1.Hold-out</h3>
<p>西瓜书里叫做留出法，是一种基础的分类方法：</p>
<ol>
  <li>将数据<em>随机地</em>分成训练（，开发）与测试集</li>
  <li>在训练/测试的时候，不要用全部数据</li>
  <li>对很大的数据集来说比较有用</li>
</ol>

<p>随机性的划分主要是为了保持数据分布的一致性，否则容易产生偏差。</p>

<p>留出法的问题在于，它对原始的数据集进行了一定的操作。如果测试集太大，会
导致训练集太小，得出的结果没有办法去近似在整个训练数据$D$上的结果；反
之，则会导致测试集不能很好地估计训练结果。目前而言没有完美解决方案，常
用的手法是训练集占比大约在三分之二至80%之间。</p>

<h3 id="2bootstrapping">2.Bootstrapping</h3>
<p>西瓜书里叫”自助法”。步骤如下：</p>
<ol>
  <li>给定包含$m$个样本的数据集$D$</li>
  <li>构建测试集$D’$：每次从$D$中随机挑选一个数据，复制到$D’$中</li>
  <li>重复步骤2$m$次，我们就能得到集$D’$（注：这是极限情况）</li>
</ol>

<p>对于某个特定的样本来说，在这$m$次采样中始终不被采到的概率为
$(1-\frac{1}{m})^m$，取极限有:</p>

<p>$\lim_{n \to \infty}(1 - \frac{1}{m}) = \frac{1}{e} = 36.8%$</p>

<p>在数据集足够大的情况下，初始数据集中大约有36.8%的数据是不会出现在集合
$D’$里的。于是，我们可以使用$D’$作为训练集，$D - D’$ 作为测试集。这样
做的坏处是，虽然我们的$D’$跟$D$有同样的大小，但是分布被改变了，也就会
导致训练误差。</p>

<h3 id="3cross-validation">3.Cross Validation</h3>
<p>交叉验证法。其步骤如下：</p>
<ol>
  <li>将数据集$D$划分成$k$个大小相等的<em>互斥</em>子集</li>
  <li>每次使用一个子集作为测试集，剩下的子集的合集作为训练集</li>
  <li>我们因而获得了$k$组测试/训练集，最终返回其结果的均值。</li>
</ol>

<p>一般而言，我们在使用交叉验证的时候$k$值取10。在极限的情况下，$k$与数据
集的数据数量$m$相等，即每个子集有且仅有一个样本。这种情况叫做留一法
(Leave-One-Out), 好处是非常接近用整个数据集$D$训练的结果，坏处是计算开
销太过巨大，而且这还是不考虑“没有免费午餐定理”的情况下。</p>

<h2 id="measuring-performance">Measuring Performance</h2>

<h3 id="overfitting--underfitting">Overfitting &amp; Underfitting</h3>

<p>过拟合与欠拟合。</p>

<p>定义错误率为分类错误的样本数占总数的比例：$E = a / m$</p>

<p>我们同样可以定义精度：$(1 - a / m) * 100%$</p>

<p>更一般地，定义误差为“学习器的实际输出”与样本的真实输出之间的差异。学
习器在<em>训练集</em>上的误差称作”训练误差”(training error)，在<em>测试集</em>上的误
差称作”泛化误差”(generalization error)。
我们的目标是，获得一个<em>泛化误差</em>较小的学习器，但是我们并不能事先知道新
的样本长什么样。在实际操作中，我们采取的策略是使得经验误差最小化。但是，
我们很可能得到这样一种学习器：经验误差很小，但是在测试集上表现不佳。这
种情况，我们称作”过拟合”(overfitting)。与之相对，是“欠拟合”(underfitting).</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/1341814477_1796.jpg" alt="Stanford_Machine_Learning" />
<em>Overfitting</em> Source: Stanford Machine Learning</p>

<p>过拟合的本质，是学习器在训练集上学习得“太好了”，以至于没有办法对一个
更加泛化的数据集做出正确的反应。</p>

<p>由于我们通过学习器拟合的函数往往是多项式函数，而机器学习所解决的现实问
题往往是NP难或者以上，因此，只要我们认为$P \neq NP$，那么过拟合就<em>无法
避免</em>。我们所能做的，是尽量减少泛化误差。</p>

<p>通常有这么几个原因导致明显的过拟合：</p>
<ol>
  <li>学习过程太长了！</li>
  <li>训练集不能较好地反映所有的情况，这是大多数场合出现过拟合的原因</li>
  <li>模型参数被调整到训练集中无效信息的部分–同目标函数根本没有正式关系
(这段不太理解：Model parameters are adjusted to uninformative
features in the training set that have no causal relation to the
true underlying target function!)</li>
</ol>

<h3 id="f1-measure">F1 Measure</h3>

<p>对于二分类问题（分类只有正反两类），我们可以根据其真实类别与学习器预测的类别组合划分为：</p>

<ol>
  <li>真正例(TP)：真实为正，预测为正</li>
  <li>假正例(FP)：真实为反，预测为正</li>
  <li>真反例(TN)：真实为反，预测为反</li>
  <li>假反例(FN): 真实为正，预测为反</li>
</ol>

<p>显然，我们有：</p>

<ol>
  <li>$TP + FP + TN + FN$ = 样本总数</li>
  <li>$TP + TN$ = 预测正确的数量</li>
  <li>$TP + FP$ = 预测结果为正的数量</li>
  <li>$TP + FN$ = 实际结果为正的数量</li>
</ol>

<p>…等等。</p>

<p>定义<em>查准率</em>(precision, 或者也叫准确率):</p>

<p>$P = \frac{TP}{TP + FP}$</p>

<p>意义为正确地分类为正占预测分类为正的百分比。</p>

<p>定义<em>查全率</em>(recall, 或者也叫召回率)：</p>

<p>$P = \frac{TP}{TP + FN}$</p>

<p>意义为正确地分类为正占真实分类为正的百分比。</p>

<p>查准率跟查全率是一对相对的度量。我们拿挑瓜做一个直观的理解：</p>

<p>小明（你可以把小明看做学习器）去挑瓜，瓜有甜的跟不甜的两种类型。小明不
是天才，不能百分之百地挑出所有的甜瓜。那么，我们就有四种情况对应上面的
四个数据：</p>
<ol>
  <li>小明觉得是甜的瓜，实际上是甜的瓜 -&gt; TP</li>
  <li>小明觉得是甜的瓜，实际上是不甜的瓜 -&gt; FP</li>
  <li>小明觉得是不甜的瓜（不挑），实际上是甜的瓜 -&gt; FN</li>
  <li>小明觉得是不甜的瓜，实际上是不甜的瓜 -&gt; TN</li>
</ol>

<p>如果小明希望能够尽可能地挑出甜的瓜，即，尽量使挑到的瓜都是甜的，那么小
明可以做的事情是：尽量少挑瓜！在这种情况下，小明挑出来的瓜(TP + FP)会
很少，但是小明会对挑出瓜的甜度很有信心(极端情况下，小明只挑一个有把握
的甜瓜：查准率100%)。当然，另一个方面就是小明尽可能挑出更多的瓜，这种
情况下查全率会很高，但是甜的瓜有多少就是一个问题了。</p>

<p>我们衡量一个学习器的好坏，要综合查准率跟查重率两方面考虑。比如说，小刚
说他比小明更会挑瓜，理由是他挑的瓜是甜的概率大-&gt;你要考虑小刚是不是只挑
很少的有把握的瓜的那种类型。但是，如果小红说她比小明更会挑瓜，理由是小
红能够挑出更多的瓜，而且小红能挑中好瓜的概率还比小明高-&gt;那这毫无疑问是
小红比较强。</p>

<p>但是，我们不能保证相比较的情况都像小明跟小红那样无可争议。因此，我们需
要一个另外的度量来衡量两个人挑瓜的能力。我们可以使用平衡点（Break-Even
Point），定义为查全率与查准率相等时的值；当然我们还有更好的：F度量(F-measure)</p>

<p>$F_{\beta} = \frac{(1 + \beta^2) \times P \times R}{\beta^2 + R}$</p>

<p>这里，我们用$\beta$来控制查全率与查准率的权重。在某些时候我们希望查全
率高一点，某些时候希望查准率高一点。当$\beta$取1的时候，即为查全率与查
准率有同样的权重的时候。我们称这样的F度量为F1度量（F1-measure）。</p>

<p>在本质上，F度量的倒数是查全率与查准率的调和平均数：</p>

<p>$\frac{1}{F_{\beta}} = \frac{1}{1 + \beta^2} (\frac{1}{P} + \frac{\beta^2}{R})$</p>

<p>我们可以容易看出，当$\beta$值小于1的时候，查准率有更大的权重；而当
$\beta$大于1的时候，查全率有更大的权重。</p>

<h3 id="rocreceiver-operator-characteristic-curve">ROC(Receiver Operator Characteristic) Curve</h3>

<p>很多学习器是为测试样本产生一个预测值，并且将之与一个分类阈值
(threshold)比较，如果大于则为正，反之则为反。</p>

<p>我们可以根据这个预测值将样本排序，例如将“最可能”为正的值排在前面，
“最不可能”为正的放在后面。我们可以根据情况来选择不同的阈值，但是如何
衡量排序本身的质量好坏，我们需要别的工具。</p>

<p>ROC曲线取“真正例率”与“假正例率”两个重要量的值，分别以他们为纵，横坐标作图。</p>

<p>定义真正例率（TPR）：</p>

<p>$TPR = \frac{TP}{TP + FN}$</p>

<p>定义假正例率（FPR）：</p>

<p>$FPR = \frac{FP}{FP + TN}$</p>

<p>ROC曲线为以TPR为纵轴，FPR为横轴的曲线</p>

<p>$TPR = ROC(FPR)$</p>

<p>坐标轴的对角线表示随机猜测的模型，点(0, 1)表示的是将所有正例排在反例之
前的理想模型。</p>

<p>ROC曲线下面包覆的面积即为AUC(Area Under ROC),经常被用作衡量学习器好坏
的一个度量。求AUC可以简单地对ROC进行加和或者求积分。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png" alt="ROC_curve" />
<em>By BOR at the English language Wikipedia, CC BY-SA 3.0</em></p>

<h3 id="confusion-matrix">Confusion Matrix</h3>
<p>混淆矩阵。这是一个很好用的可视化工具来判断是不是正确分类，比如西瓜：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">真实情况\预测结果</th>
      <th style="text-align: center">正</th>
      <th style="text-align: center">反</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">正</td>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">FN</td>
    </tr>
    <tr>
      <td style="text-align: center">反</td>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">TN</td>
    </tr>
  </tbody>
</table>

<h1 id="references">References</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material Topic 1, Topic 2, Topic 8</li>
</ol>

 -->
    
        <p>本学期机器学习的一个复习，还是跟大三下一样会写一个系列，就当为GRE跟师
匠攒人品了。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://MeowAlienOwO.github.io" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://MeowAlienOwO.github.io">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page7">7</a>
        
      </li>
    
  </ul>
  
    <a href="http://MeowAlienOwO.github.io/page3" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2018 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->





<script>
 var _config = {
     title: 'Entry',
     image: '22958285.jpg'
 }

 socialShare('.social-share', _config)
</script>

          

</body>
</html>
