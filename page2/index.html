<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Entry &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
     <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
     </script> -->

<!-- Share.js -->
<!-- <link href="/assets/css/share.min.css" rel="stylesheet"> -->
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js"></script> -->
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Entry">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/page2/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/page2/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/machine learning/">
				    machine learning (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/RL/">
				    RL (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=22958285">ZERO | STAR影法師 [pixiv]</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://MeowAlienOwO.github.io/images/22958285.jpg" alt="Entry">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>喵窝[0]号机</h1>
      <h2>一个伪装成技术博客的吐槽网站。</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" title="Machine Learning Review: K-means & KNN">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/56118124.jpg" alt="Machine Learning Review: K-means & KNN">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-08T23:09:34+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/">January 08, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" rel="bookmark" title="Machine Learning Review: K-means & KNN" itemprop="url">Machine Learning Review: K-means & KNN</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>啊……感觉要加快点节奏，不然来不及……</p>

<h1 id="聚类学习">聚类学习</h1>
<p>聚类学习(clustering)是无监督学习(unsupervised learning)的一种。训练样
本的标记信息是未知的，目的是通过无监督学习的算法来揭示数据的内部联系。
聚类学习的目的是，将原有的数据集合划分成若干个不相交的子集。每个子集被
称为一个簇(cluster)。但是，这些簇对于聚类算法而言具体代表什么意义是未
知的，需要使用者去标记。</p>

<h2 id="k-means">K-Means</h2>

<p>K-Means是一个典型的聚类学习算法。给定样本集$D = {x_1, x_2,…,x_m}$，
K-Means的目标是产生k个簇$C = {C_1, C_2,…,C_k}$，使得其平方误差最小：</p>

<script type="math/tex; mode=display">E = \sum\limits_{i=1}^{k}\sum\limits_{x \in C_i} \|\mathbf{x} - \mu_i\|^2</script>

<p>其中，$\mu$是划分为该簇的向量的均值，类似物理学中的质心。</p>

<p>K-means的算法可以大致描述如下：</p>

<ol>
  <li>随机选取k个初始均值向量$\mu_1,\mu_2,…,\mu_k$，用来表示簇的中心</li>
  <li>对于所有样本，根据样本数据与均值向量之间的距离$dst = $|\mathbf{x} - \mu$|$,将样
本划分至最近的簇中</li>
  <li>划分完后，对于所有的簇，根据现有的样本，重新计算其均值向量并且更新。</li>
  <li>重复步骤2,3，直到所有的均值向量不变或者变动范围小于某个阈值</li>
</ol>

<p>K-Means可以看做是不断地更新质心的过程。</p>

<h2 id="对距离的处理">对距离的处理</h2>

<p>K-Means处理距离的时候常使用闵可夫斯基距离(Minkowski distance)。</p>

<script type="math/tex; mode=display">dst_mk(\mathbf{x_i}, \mathbf{x_j}) = (\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^p)^{\frac{1}{p}}</script>

<p>在p值取2的时候，闵可夫斯基距离就是普通的欧几里得距离(Euclidean
distance)：
<script type="math/tex">dst_eu(\mathbf{x_i}, \mathbf{x_j}) =
\sqrt{\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^2}</script></p>

<p>p值取1的时候，闵可夫斯基距离变成曼哈顿距离。</p>

<h1 id="竞争学习">竞争学习</h1>
<h2 id="竞争神经网络competitive-network">竞争神经网络(Competitive Network)</h2>

<p>竞争神经网络是无监督学习的一种，即训练集中的样本没有给出最终的分类结果。
神经网络同样也可以用于无监督学习。其处理方式为增加了一道”竞争”机制。</p>

<p>竞争学习的神经网络通常只有两层：输入层与单层输出节点，输出神经元与输入
节点完全相连。</p>

<p>对于每次输入，只有一个输出神经元会被激活并更新权重。竞争神经网络采取
“赢家通吃”的方法来决定，即只有输出值最大的神经元得到激活并且更新权重。如果下一次
有相近的输入，那么该神经元也会有更大的概率被激活。</p>

<h2 id="最小距离分类minimun-distance-classifier">最小距离分类(minimun distance classifier)</h2>

<p>最小距离首先计算每一个已知类别的平均点，然后对于新的样本，分别计算其到
类别平均点的距离，选取距离最近的那个类别进行分类并且更新平均点。</p>

<h1 id="k邻近算法">K邻近算法</h1>

<p>K邻近算法(K-nearest neighbour)是一个简单的分类算法。其具体思想是，将样
本同训练集中的元素进行一一比较，取前k个最近的元素，选取这$k$个中最多的
类别进行分类。</p>

<p>K邻近算法没有学习过程，被称之为懒惰学习。</p>

<p>具体算法如下：</p>
<ol>
  <li>将数据集划分为测试集与训练集，并且保证其均匀分布</li>
  <li>保存训练集数据</li>
  <li>对每一个测试集中数据，计算其与所有的训练集之间的距离，取前K个。K值
预先指定</li>
  <li>取K个最近距离中类别最多的一项，标记类别</li>
</ol>

<p>K邻近算法虽然简单，但是泛化效率很高。但是，对大的数据集而言，它的效率
不太让人满意。</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>liangdas,最小距离分类法介绍, http://blog.csdn.net/liangdas/article/details/17039583</li>
  <li>51to.com, 7.6 竞争网络和竞争学习（1）, http://book.51cto.com/art/201302/380106.htm</li>
</ol>

 -->
    
        <p>啊……感觉要加快点节奏，不然来不及……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/" title="Machine Learning Review: Bayesian Learning">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60680252.jpg" alt="Machine Learning Review: Bayesian Learning">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-08T15:02:38+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/">January 08, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/" rel="bookmark" title="Machine Learning Review: Bayesian Learning" itemprop="url">Machine Learning Review: Bayesian Learning</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>死亡冲师匠失败(艸□`..)但是还是姑且把这个当成攒人品吧……</p>

<h1 id="贝叶斯分类器">贝叶斯分类器</h1>
<h2 id="数学准备">数学准备</h2>
<p>主要用到概率论的一些知识，简要地复习一下。</p>
<h3 id="条件概率">条件概率</h3>
<p>条件概率指的是在事件$B$发生的情况下，事件$A$发生的概率。数学符号记为
$P(A | B)$</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/bg2011082502.jpg" alt="" /></p>

<p>显然，如果要计算在事件$B$发生的情况下$A$发生的概率，我们有：</p>

<script type="math/tex; mode=display">P(A \| B) = \frac{P(A \cap B)}{P(B)}</script>

<p>那么，我们可以很容易得出：</p>

<script type="math/tex; mode=display">P(A \cap B) = P(A \| B)P(B)</script>

<p>同理，将事件换个位置我们有：</p>

<script type="math/tex; mode=display">P(A\cap B) = P(B \| A)P(A)</script>

<p>那么，我们有：</p>

<script type="math/tex; mode=display">P(A \| B)P(B) = P(B \| A )P(A)</script>

<p>这就是条件概率公式。</p>
<h3 id="全概率公式">全概率公式</h3>

<p><img src="http://og78s5hbx.bkt.clouddn.com/bg2011082504.jpg" alt="" /></p>

<p>假定我们有样本空间$S$与事件$A$，事件$A$与事件$\not A$构成了整个样本空
间。我们引入事件$B$，那么有：</p>

<script type="math/tex; mode=display">P(B) = P(B\|A)P(A) + P(B\| \not A)P(\not A)</script>

<p>证明：
由于$A$与$\not A$之和为整个样本空间，对于事件B我们有:</p>

<script type="math/tex; mode=display">P(B) = P(B \cap A) + P(B \cap \not A)</script>

<p>根据条件概率，我们有：</p>

<script type="math/tex; mode=display">P(B) = P(B \| A)P(A) + P(B \| \not A) P(\not A)</script>

<h3 id="贝叶斯推断">贝叶斯推断</h3>

<p>对条件概率公式变形，我们有如下形式:</p>

<script type="math/tex; mode=display">P(A \| B) = P(A)\frac{P(B \| A)}{P(B)}</script>

<p>我们把$P(A)$看作是<strong>先验概率</strong>（即我们在观察样本之前对事件$A$发生概率的估
计），$P(A|B)$是<strong>后验概率</strong>（即事件$B$发生的情况下，事件$A$发生的概率），
而$P(B |A) / P(B)$被称作<strong>似然度</strong>（likelyhood）。那么，我们的条件概
率公式就可以被认为是：</p>

<blockquote>
  <p>后验概率 = 先验概率 × 似然度</p>
</blockquote>

<p>这就是贝叶斯推断的意义。</p>

<p>用西瓜做比喻。假设我们从一堆西瓜中挑出了若干个，
那么根据贝叶斯公式，我们认为：</p>

<blockquote>
  <p>P(我们预计是甜的情况下真的是甜的) = P(我们预计是甜的) × P(真的是甜
的情况下且我们预计是甜的) / P(真的是甜的)</p>
</blockquote>

<h3 id="例子假阳性问题">例子：假阳性问题</h3>
<p>医学上的假阳性问题是一个很经典的例子，可以用来解释条件概率与贝叶斯定理。
假定某种疾病发生的概率是1%，现在有一种准确率为99%的检查手段可以检测病
情，即患者有病的情况下，检测为阳性的概率为99%。但是，这种手段的误报率
是5%，即患者如果没有病，它有5%的概率呈现阳性。现在有一个病人的检验结果
为阳性，求其得病的概率？</p>

<p>我们的先验概率，即疾病发生的概率是1%。我们要计算的是：“在检测为阳性的
情况下患者患病的概率”，即后验概率。根据贝叶斯定理，我们有：</p>

<script type="math/tex; mode=display">P(A \| B) = P(A)\frac{P(B \| A)}{P(B)}</script>

<p>其中$P(A)$为先验概率，$P(B | A)$是在患者患病的情况下的检测成功率。
$P(B)$是患者被检测出阳性的总概率。对于未知的$P(B)$，我们可以用全概率公
式求解：</p>

<script type="math/tex; mode=display">P(B) = P(B \| A) * P(A) + P(B \| \not A) * P(\not A)</script>

<p>则我们可以计算出，患者被检测出阳性的总概率大致为5.94%，从而，患者检测
为阳性的情况下患病率大致为16.7%。这是一个反直觉的结论，即检测结果为阳
性也只能说明检测对象有大约百分之十七的概率患病。如果发病率更低，检测结
果的效果更差，大家不妨试一试。</p>

<h2 id="贝叶斯分类器的原理">贝叶斯分类器的原理</h2>

<p>将贝叶斯定理与分类器的原理进行结合，我们就得到了贝叶斯分类器。其原理可
以简单地叙述如下：</p>

<blockquote>
  <p>给定一个包含若干个属性${x_1, x_2,…,x_m}$的样本，在已知所有相关概率的情况下，在类别空间
$D = {d_1,d_2,…,d_n}$上寻找最大可能的概率。</p>
</blockquote>

<p>我们先根据某些原则（过去的经验，某些性质比较好的点，等等）给出一个概率
的先验分布，我们的目的是最大化我们的后验概率(即，在样本发生的情况下，
其可以被分为某类的概率)。因此，贝叶斯分类器的数学形式可以表达如下：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(d_i \| \mathbf{x}) \\
~~= argmax_{d_i \in D} P(d_i) \frac{P(\mathbf{x} \| d_i)}{P(\mathbf{x})}</script>

<h2 id="极大似然估计mle与极大后验概率map">极大似然估计（MLE）与极大后验概率（MAP）</h2>

<p>由上式我们可以看出，样本的发生概率$P(\mathbf{x})$对于给定的数据集是一
个常数。但是，对于如何处理分子项$P(d_i)P(\mathbf{x} | d_i)$而言，根据
不同的视角而言会有不同的结论。</p>

<p>目前，概率学界大致有两种看法：频率主义认为，虽然频率的分布是未知的，但
是在实际上是一个确定的值；但是贝叶斯学派认为，频率的分布也是一个未知的
随机变量。</p>

<p>根据频率主义学派的观点，我们有极大似然估计(maximum likelihood
estimation)方法。频率学派认为，样本的分布是确定的，这意味着如果样本满
足独立同分布条件（虽然是什么我不清楚），先验概率
$P(d_i)$这一项在实际上是常数。我们在做计算的时候，只需要考虑似然度
$P(\mathbf{x} | d_i)$这一项即可。根据极大似然估计，我们可以将贝叶斯分
类器转化为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(\mathbf{x} \| d_i)</script>

<p>贝叶斯学派认为，样本的分布是不确定的，先验概率也是随机变量之一，于是，
$P(d_i)$项不为常数，在求最大值时无法约去。我们把这种处理方式称为极大后
验概率(maximum posteriori)。这时，贝叶斯分类器转化为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(d_i)P(\mathbf{x} \| d_i)</script>

<p>根据贝叶斯学派的观点，先验概率的选取会对结果造成影响，在现实世界中，这
也意味着之前的认知会对之后的估计造成影响，这里不展开。</p>

<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>

<p>基于贝叶斯公式估计后验概率的时候，由于样本的属性向量$\mathbf{x}$会随着
属性的不同而不同，在属性很多的情况下会是一个天文数字。这导致了两个问题：
在计算的时候出现组合爆炸；在泛化的时候又过于稀疏，难以估计广泛情况。为
了避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”(attribute
conditional independence assumption)，即假设所有的属性对结果的影响是独
立的。</p>

<p>根据属性条件独立性假设，我们可以将贝叶斯分类器重写为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} \frac{P(d_i)}{P(\mathbf{x})}
\prod_{j=1}^{m} P(x_j \| d_i)</script>

<p>由于对于所有的属性$P(\mathbf{x})$均为常数，我们可以将其约去。先验概率
的选取需要考虑问题的性质，如果我们可以很有信心地得出先验概率，则用已经
确定的先验概率。否则，我们可以对样本集进行统计，或者使用正态分布等。
特征是连续值的情况下，通常假设其服从于高斯分布（正态分布）。</p>

<p>当连乘式中出现某个属性没有出现，即其概率为0的情况，无论其他属性有多高
的可能性，其总概率为0，而这是我们不想看到的。其解决方法是m-估计(m-estimation):</p>

<script type="math/tex; mode=display">\frac{\|D_c\| + m\|D_c\|p}{\|D\| + m}</script>

<p>其中，$|D_c|$指的是该类别的样本总量，$|D|$是样本总量，$p$是<strong>该类
别</strong>的先验概率，$m$是等效样本的大小(equivalent sample size)。这里，$m$
可以取正的任意值。在样本量足够大的情况下，取一个较小的m不会对最终结果
有很大的影响。</p>

<h1 id="思想及其应用">思想及其应用</h1>

<p>在Paul Graham的《黑客与画家》一书中，使用了朴素贝叶斯分类器来分辨垃圾
邮件，具体操作是抽取关键词，并且计算关键词出现与邮件为垃圾邮件的概率。
朴素贝叶斯分类器的效率极高，在这方面的应用往往有99%以上的正确率。</p>

<p>另外，朴素贝叶斯的基本思想–将属性分开来独立地看待，也是在自然语言处理
方面很有用的隐马尔科夫模型的基本假设。</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>阮一峰，贝叶斯推断及其互联网应用（一）：定理简介，http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html</li>
  <li>阮一峰，贝叶斯推断及其互联网应用（二）：过滤垃圾邮件，http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_two.html</li>
  <li>冰枫的随笔， 贝叶斯方法的m-估计，http://blog.csdn.net/cyningsun/article/details/8671975</li>
  <li>张洋,算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification),http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html</li>
</ol>

 -->
    
        <p>死亡冲师匠失败(艸□`..)但是还是姑且把这个当成攒人品吧……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" title="Machine Learning Review: Decision Tree & Random Forest">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60741116.jpg" alt="Machine Learning Review: Decision Tree & Random Forest">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-06T21:08:28+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/">January 06, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" rel="bookmark" title="Machine Learning Review: Decision Tree & Random Forest" itemprop="url">Machine Learning Review: Decision Tree & Random Forest</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>

<p>死亡冲锋抽师匠！</p>

<h1 id="decision-tree">Decision Tree</h1>

<p>决策树是一个有层次(hierachical)的结构，可以通过分治法
（divide-and-conquer）来将数据分成多个类别。</p>

<p>决策树的一个特点是，可以用来处理不能用数字表示的数据，比如说西瓜的颜色
等等。
决策树学习的目标是，构建一棵树，使之能够表示决策过程。树的节点是决策点，
树的叶子是最后的分类。算法可以大致描述如下：</p>

<ol>
  <li>Input：
    <ul>
      <li>训练集$D$: ${(x_1, y_1), (x_2, y_2)….}$</li>
      <li>属性集$A$: ${a_1, a_2,…a_d}$，每个属性集有若干取值，比如说西瓜：
硬度有很硬，硬，普通，软；大小有很大，大，中，小，很小…这里的硬
度与大小就是不同的$a_n$</li>
    </ul>
  </li>
  <li>Function DecisionTreeGen($D$, $A$)</li>
  <li>生成节点Node</li>
  <li><code>if</code> $D$中所有元素属于属性$C$：
    <ol>
      <li>将Node标记为$C$类叶子，返回</li>
    </ol>
  </li>
  <li><code>if</code> $A$为空集，或者$D$的样本在$A$上均有相同取值:
    <ol>
      <li>将Node标记为叶子，其类别标记为$D$中样本数最多的类，返回</li>
    </ol>
  </li>
  <li><strong>从A中选择最优划分属性</strong>$a_m$</li>
  <li><code>for</code> $a_m$的每一个值$a_m^v$:
    <ol>
      <li>为Node生成一个分支SubNode。</li>
      <li>令$D_v$ 为训练集中在属性$a_m$上取值为$a_m^v$的样本集</li>
      <li><code>if</code> $D_v$为空集：
        <ol>
          <li>SubNode标记为叶子，类别取$D$中样本最多的类，返回</li>
        </ol>
      </li>
      <li><code>else</code>：
        <ol>
          <li>SubNode = TreeGenerate($D_v$, $A - a_m$)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>返回：以Node为根节点的决策树</li>
</ol>

<p>如上算法的3,4,6.3分别表示了决策树的三种递归停止的情形：</p>
<ol>
  <li>当前节点的样本都属于同一类别无需划分(最终划分到大小这一类，都是大的西瓜)</li>
  <li>当前属性集为空，或者所有的样本在所有属性上都有相同的取值（这里指某
一个属性没有办法作为划分属性，比如说如果有属性“是水果”–所有的西
瓜都是水果，这个属性就没有意义）</li>
  <li>当前节点包含的样本集合为空</li>
</ol>

<p>解决递归停止的情形2的方法称为“多数表决”法。</p>

<p>我们可以很明显地看出，步骤5是决定决策树算法性能的关键。</p>

<h2 id="信息熵">信息熵</h2>

<p>我们的目标是寻找一个较好的找出最优的划分属性的方法，使得其划分后的样本
能够<strong>最纯</strong>–换言之，尽可能相近的类别应该尽可能分在一起。为此，我们需
要了解信息熵。</p>

<p>信息熵是对不确定性的测量。一件事情发生的概率越低，描述其所需要的信息量
就越多。同样的，对于决策树而言，最容易划分的属性类别往往应该在高处，不
容易划分的在低处。信息熵的定义为：</p>

<script type="math/tex; mode=display">Ent(D) = - \sum\limits_{k=1}^{\|y\|}p_{k}log_{2}p_{k}</script>

<p>当p为0的时候，信息熵为0。</p>

<h2 id="id3算法与信息增益">ID3算法与信息增益</h2>

<p>但是，单单用信息熵我们是无法找到什么属性能够“最优”地划分样本集–样本
往往由多个属性复合而成，我们选取划分属性的时候应该怎么做呢？ID3算法
(iterative dichotomiser)引入了“信息增益”的概念。</p>

<p>离散属性$a$有$V$个可能的取值${a^1, a^2…a^V}$。如果使用$a$来对样本
进行划分的话，会有$V$个分支节点，每个分支节点下都包含若干属性为$a^v$的
样本，记作$D^v$。我们可以计算每个节点的信息熵，取$|D^v| / |D|$为权
重，然后对所有节点的信息熵进行加权求和。样本集$D$的信息熵与加权求和后
的结果定义为<strong>信息增益</strong>(gain)：</p>

<script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum\limits_{v = 1}^{V}\frac{\|D^v\|}{\|D\|}Ent(D^v)</script>

<p>ID3算法在每次选取属性的时候，计算所有属性的增益，然后选取增益最大的一
个属性进行划分，以此类推。</p>

<h2 id="c45算法与增益率">C4.5算法与增益率</h2>

<p>ID3算法的问题在于，它会天然地倾向于属性多的类别。比如说，如果把编号(id)作为
一类计入考虑，ID3会先划分编号–虽然划分非常纯净，但这毫无用处。为了克
服这一缺点，C4.5算法引入了“增益率”(gai ratio)的概念。增益率定义为：</p>

<script type="math/tex; mode=display">GainRatio(D, a) = \frac{Gain(D, a)}{IV(a)}</script>

<p>其中，固有值(intrinsic value)$IV$定义为：</p>

<script type="math/tex; mode=display">IV(a) = - \sum\limits_{v=1}{V} \frac{\|D^v\|}{\|D\|}log_{2}\frac{\|D^v\|}{\|D\|}</script>

<p>但是实际上增益率的处理会导致偏向取值少的类别。在实际应用中，会先用启发
式算法找出信息增益水平高于平均水平的属性，然后再选择高增益的属性。</p>

<h2 id="过拟合">过拟合</h2>

<p>决策树也会过拟合，主要的表现为分支过多，导致判断很多不必要的噪声。降低
过拟合的风险的主要手段是剪枝。剪枝的手段主要有预剪枝与后剪枝两种。预剪
枝指的是在构造树的过程中满足某些条件停止分支的构建，而后剪枝指构建完
整的决策树后再进行剪枝。</p>

<h1 id="随机森林">随机森林</h1>

<p>// 注：这是根据中文网络上比较常见的随机森林与西瓜书写的，可能跟PPT有出
// 入</p>

<h2 id="集成学习">集成学习</h2>

<p>集成学习(ensemble learning)是一种将多个学习器组合起来完成学习任务的方
法。其一般结构为，先生成一组基<span color="black">♂</span>学习器(base
learner)，然后用某种策略组合其输出。其学习算法称作基<span color="black">♂</span>学习算法(base learning algorithm)</p>

<p><img src="http://images.cnitblog.com/blog/633472/201410/181942114048093.png" alt="" /></p>

<p>集成学习的种类大致有两种：</p>
<ol>
  <li>个体间存在强依赖关系，必须串行执行。代表：Boosting</li>
  <li>个体间没有强依赖关系，可以并行执行。代表：Bagging, 随机森林
    <h2 id="baggingbootstrap-aggregating">Bagging（Bootstrap Aggregating）</h2>
  </li>
</ol>

<p>并行学习器的基本思想是，通过使基学习器之间尽量互相独立（有较大差异），
从而提高总体的泛化性能。在此，我们了解一下随机森林的基本型：Bagging算
法。</p>

<p>我们首先对样本进行Bootstrap采样：给定包含$m$个样本的样本集，每次随机抽
选一个，维持原有的采样集不变，重复$m$次，构建新的样本集。正如我们之前
所讨论的，这个样本集大约会包含原有样本集中63.2%的样本。同样的，我们可
以构建任意多个Bootstrap样本集，根据需求，我们构建$T$个样本集，每个样本
集都会包含原有样本集的63.2%。我们对每个采样集训练一个学习器，然后将这
些学习器进行结合。当结论不一致的时候，Bagging使用简单投票法。当得票数
相等时，我们可以随机选一个，或者考察学习器投票的置信率。</p>

<h2 id="随机森林算法">随机森林算法</h2>

<p>随机森林是Bagging的一个变种。随机森林以决策树作为基学习算法，在Bagging
基础上引入随机属性选择。</p>

<p>传统的决策树在选择划分属性的时候，使用一个最优化算法。但是在随机森林中，
对于基决策树的每一个节点，从原有属性集中随机选择一个包含$k$个属性的子
集，然后在这个子集中选取最优属性。$k$的取值决定了随机性的程度，当$k$与
属性集的大小相等时，就为传统决策树；当$k$为1的时候，就是随机选取属性。
一般情况使用的值为：$k=log_{2}d$</p>

<p>随机森林的特性是，起始性能比较差，但是当学习器数量增加时，随机森林的泛
化误差往往比较低。</p>

<h1 id="reference">Reference</h1>

<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>CodingLabs, 算法杂货铺——分类算法之决策树(Decision tree),
http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</li>
</ol>
 -->
    
        <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/" title="Machine Learning Review: Decision Tree & Random Forest">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60741116.jpg" alt="Machine Learning Review: Decision Tree & Random Forest">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-06T21:08:28+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/">January 06, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_decision_tree_&_random_forest/" rel="bookmark" title="Machine Learning Review: Decision Tree & Random Forest" itemprop="url">Machine Learning Review: Decision Tree & Random Forest</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>

<p>死亡冲锋抽师匠！</p>

<h1 id="decision-tree">Decision Tree</h1>

<p>决策树是一个有层次(hierachical)的结构，可以通过分治法
（divide-and-conquer）来将数据分成多个类别。</p>

<p>决策树的一个特点是，可以用来处理不能用数字表示的数据，比如说西瓜的颜色
等等。
决策树学习的目标是，构建一棵树，使之能够表示决策过程。树的节点是决策点，
树的叶子是最后的分类。算法可以大致描述如下：</p>

<ol>
  <li>Input：
    <ul>
      <li>训练集$D$: ${(x_1, y_1), (x_2, y_2)….}$</li>
      <li>属性集$A$: ${a_1, a_2,…a_d}$，每个属性集有若干取值，比如说西瓜：
硬度有很硬，硬，普通，软；大小有很大，大，中，小，很小…这里的硬
度与大小就是不同的$a_n$</li>
    </ul>
  </li>
  <li>Function DecisionTreeGen($D$, $A$)</li>
  <li>生成节点Node</li>
  <li><code>if</code> $D$中所有元素属于属性$C$：
    <ol>
      <li>将Node标记为$C$类叶子，返回</li>
    </ol>
  </li>
  <li><code>if</code> $A$为空集，或者$D$的样本在$A$上均有相同取值:
    <ol>
      <li>将Node标记为叶子，其类别标记为$D$中样本数最多的类，返回</li>
    </ol>
  </li>
  <li><strong>从A中选择最优划分属性</strong>$a_m$</li>
  <li><code>for</code> $a_m$的每一个值$a_m^v$:
    <ol>
      <li>为Node生成一个分支SubNode。</li>
      <li>令$D_v$ 为训练集中在属性$a_m$上取值为$a_m^v$的样本集</li>
      <li><code>if</code> $D_v$为空集：
        <ol>
          <li>SubNode标记为叶子，类别取$D$中样本最多的类，返回</li>
        </ol>
      </li>
      <li><code>else</code>：
        <ol>
          <li>SubNode = TreeGenerate($D_v$, $A - a_m$)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>返回：以Node为根节点的决策树</li>
</ol>

<p>如上算法的3,4,6.3分别表示了决策树的三种递归停止的情形：</p>
<ol>
  <li>当前节点的样本都属于同一类别无需划分(最终划分到大小这一类，都是大的西瓜)</li>
  <li>当前属性集为空，或者所有的样本在所有属性上都有相同的取值（这里指某
一个属性没有办法作为划分属性，比如说如果有属性“是水果”–所有的西
瓜都是水果，这个属性就没有意义）</li>
  <li>当前节点包含的样本集合为空</li>
</ol>

<p>解决递归停止的情形2的方法称为“多数表决”法。</p>

<p>我们可以很明显地看出，步骤5是决定决策树算法性能的关键。</p>

<h2 id="信息熵">信息熵</h2>

<p>我们的目标是寻找一个较好的找出最优的划分属性的方法，使得其划分后的样本
能够<strong>最纯</strong>–换言之，尽可能相近的类别应该尽可能分在一起。为此，我们需
要了解信息熵。</p>

<p>信息熵是对不确定性的测量。一件事情发生的概率越低，描述其所需要的信息量
就越多。同样的，对于决策树而言，最容易划分的属性类别往往应该在高处，不
容易划分的在低处。信息熵的定义为：</p>

<script type="math/tex; mode=display">Ent(D) = - \sum\limits_{k=1}^{\|y\|}p_{k}log_{2}p_{k}</script>

<p>当p为0的时候，信息熵为0。</p>

<h2 id="id3算法与信息增益">ID3算法与信息增益</h2>

<p>但是，单单用信息熵我们是无法找到什么属性能够“最优”地划分样本集–样本
往往由多个属性复合而成，我们选取划分属性的时候应该怎么做呢？ID3算法
(iterative dichotomiser)引入了“信息增益”的概念。</p>

<p>离散属性$a$有$V$个可能的取值${a^1, a^2…a^V}$。如果使用$a$来对样本
进行划分的话，会有$V$个分支节点，每个分支节点下都包含若干属性为$a^v$的
样本，记作$D^v$。我们可以计算每个节点的信息熵，取$|D^v| / |D|$为权
重，然后对所有节点的信息熵进行加权求和。样本集$D$的信息熵与加权求和后
的结果定义为<strong>信息增益</strong>(gain)：</p>

<script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum\limits_{v = 1}^{V}\frac{\|D^v\|}{\|D\|}Ent(D^v)</script>

<p>ID3算法在每次选取属性的时候，计算所有属性的增益，然后选取增益最大的一
个属性进行划分，以此类推。</p>

<h2 id="c45算法与增益率">C4.5算法与增益率</h2>

<p>ID3算法的问题在于，它会天然地倾向于属性多的类别。比如说，如果把编号(id)作为
一类计入考虑，ID3会先划分编号–虽然划分非常纯净，但这毫无用处。为了克
服这一缺点，C4.5算法引入了“增益率”(gai ratio)的概念。增益率定义为：</p>

<script type="math/tex; mode=display">GainRatio(D, a) = \frac{Gain(D, a)}{IV(a)}</script>

<p>其中，固有值(intrinsic value)$IV$定义为：</p>

<script type="math/tex; mode=display">IV(a) = - \sum\limits_{v=1}{V} \frac{\|D^v\|}{\|D\|}log_{2}\frac{\|D^v\|}{\|D\|}</script>

<p>但是实际上增益率的处理会导致偏向取值少的类别。在实际应用中，会先用启发
式算法找出信息增益水平高于平均水平的属性，然后再选择高增益的属性。</p>

<h2 id="过拟合">过拟合</h2>

<p>决策树也会过拟合，主要的表现为分支过多，导致判断很多不必要的噪声。降低
过拟合的风险的主要手段是剪枝。剪枝的手段主要有预剪枝与后剪枝两种。预剪
枝指的是在构造树的过程中满足某些条件停止分支的构建，而后剪枝指构建完
整的决策树后再进行剪枝。</p>

<h1 id="随机森林">随机森林</h1>

<p>// 注：这是根据中文网络上比较常见的随机森林与西瓜书写的，可能跟PPT有出入</p>

<h2 id="集成学习">集成学习</h2>

<p>集成学习(ensemble learning)是一种将多个学习器组合起来完成学习任务的方
法。其一般结构为，先生成一组基<span color="black">♂</span>学习器(base
learner)，然后用某种策略组合其输出。其学习算法称作基<span color="black">♂</span>学习算法(base learning algorithm)</p>

<p><img src="http://images.cnitblog.com/blog/633472/201410/181942114048093.png" alt="" /></p>

<p>集成学习的种类大致有两种：</p>
<ol>
  <li>个体间存在强依赖关系，必须串行执行。代表：Boosting</li>
  <li>个体间没有强依赖关系，可以并行执行。代表：Bagging, 随机森林
    <h2 id="baggingbootstrap-aggregating">Bagging（Bootstrap Aggregating）</h2>
  </li>
</ol>

<p>并行学习器的基本思想是，通过使基学习器之间尽量互相独立（有较大差异），
从而提高总体的泛化性能。在此，我们了解一下随机森林的基本型：Bagging算
法。</p>

<p>我们首先对样本进行Bootstrap采样：给定包含$m$个样本的样本集，每次随机抽
选一个，维持原有的采样集不变，重复$m$次，构建新的样本集。正如我们之前
所讨论的，这个样本集大约会包含原有样本集中63.2%的样本。同样的，我们可
以构建任意多个Bootstrap样本集，根据需求，我们构建$T$个样本集，每个样本
集都会包含原有样本集的63.2%。我们对每个采样集训练一个学习器，然后将这
些学习器进行结合。当结论不一致的时候，Bagging使用简单投票法。当得票数
相等时，我们可以随机选一个，或者考察学习器投票的置信率。</p>

<h2 id="随机森林算法">随机森林算法</h2>

<p>随机森林是Bagging的一个变种。随机森林以决策树作为基学习算法，在Bagging
基础上引入随机属性选择。</p>

<p>传统的决策树在选择划分属性的时候，使用一个最优化算法。但是在随机森林中，
对于基决策树的每一个节点，从原有属性集中随机选择一个包含$k$个属性的子
集，然后在这个子集中选取最优属性。$k$的取值决定了随机性的程度，当$k$与
属性集的大小相等时，就为传统决策树；当$k$为1的时候，就是随机选取属性。
一般情况使用的值为：$k=log_{2}d$</p>

<p>随机森林的特性是，起始性能比较差，但是当学习器数量增加时，随机森林的泛
化误差往往比较低。</p>

<h1 id="reference">Reference</h1>

<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>CodingLabs, 算法杂货铺——分类算法之决策树(Decision tree),
http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</li>
</ol>
 -->
    
        <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/" title="Machine Learning Review: SVM & PCA">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60095827_p0.jpg" alt="Machine Learning Review: SVM & PCA">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2016-12-31T14:23:05+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/">December 31, 2016</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/2016-12-31-machine_learning_review-_svm_&_pca/" rel="bookmark" title="Machine Learning Review: SVM & PCA" itemprop="url">Machine Learning Review: SVM & PCA</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>新年快乐，跨年也是元气满满地在机房～</p>

<p>就不按照上课顺序了，从自己搞不清楚的部分写起。在考SQM之前姑且看看能不
能再复习一遍决策树吧。</p>

<h1 id="支持向量机-support-vector-machine">支持向量机 Support Vector Machine</h1>

<h2 id="线性分类器">线性分类器</h2>

<p>分类学习的基本想法是，在训练集$D$的样本空间中，寻找到一个超平面
(hyperplane)，从而将数据分成两类。感知机(perceptron)就是一个典型的线性
分类器。</p>

<p>线性分类器的一般方程可以表示如下：</p>

<p>$\mathbf{w}^T\mathbf{x} + b = 0$</p>

<p>其中，$\mathbf{w}$是权重向量，$\mathbf{x}$是特征向量。另外，在数学意义上，
$\mathbf{w}$是由方程$\mathbf{w}^T\mathbf{x} + b = 0$所表示的超平面的法向量，
$b$为该超平面的截距。</p>

<h2 id="支持向量机">支持向量机</h2>

<p>应用线性分类器的时候，我们常常会碰到这样一种情况：</p>

<p><img src="https://computersciencesource.files.wordpress.com/2010/01/svmafter_thumb.png?w=230&amp;h=240" alt="best-svm" /></p>

<p>线性分类器可能的取值在<strong>训练</strong>集上不止一个，那么我们如何寻找__最优__的
线性分类器呢？</p>

<p>注意到样本空间中任意一点$x$到超平面的距离为：</p>

<table>
  <tbody>
    <tr>
      <td>$r = \frac{</td>
      <td>\mathbf{w}^T\mathbf{x} + b</td>
      <td>}{</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>复习一下解析几何：</p>

<p>在二维情况下的情形是点到直线的距离:</p>

<p>$r = \frac{ax + by +c}{\sqrt{a^2 + b^2}}$</p>

<p>三维情况下点到平面的距离：</p>

<p>$r = \frac{Ax +By + Cz + D}{\sqrt{A^2 + B^2 + C^2}}$</p>

<table>
  <tbody>
    <tr>
      <td>我们不难推广到高维，其中$</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>$表示向量的模。</td>
    </tr>
  </tbody>
</table>

<p>另外，我们还有一个关于距离的定义：</p>

<blockquote>
  <p>两个非空集合间的距离为两者内各自的点之间的距离之下确界。</p>
</blockquote>

<p>将直线看做点集，点看做只有一个元素的点集，就是点与直线的距离公式。同样，
我们可以将其应用于直线与直线，直线与平面…
这个定义在测试中的基于距离的自适应测试（DART）以及KNN算法里都有应用。</p>

<p>距离超平面最近的样本点为__支持向量__(support vector)。假定超平面可以将训练样本正确分类，
那么我们可以得到：对分类结果有明显影响的是__支持向量__，而样本中其他的
样本点可以忽略不计。定义__间距__(margin)为两个分别在超平面不同边的支持向量到超
平面的距离之和，那么，为了寻找一个__最好__的超平面，我们需要找到有__最大__间
距的超平面。</p>

<p>如果我们令样本分成两类$y = {-1, 1}$,我们可以得到：</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \geq \rho / 2, if y_i \geq 1$</p>

<p>$\mathbf{w}^T\mathbf{x}_i + b \leq - \rho / 2, if y_i \leq -1$</p>

<p>其中$\rho$为间距。由于在这种情况下，支持向量到超平面的距离可以表示为：</p>

<table>
  <tbody>
    <tr>
      <td>$r = \frac{</td>
      <td>(\mathbf{w}^T\mathbf{x}) + b</td>
      <td>}{</td>
      <td> </td>
      <td>\mathbf{w}</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>我们可以得到：</p>

<table>
  <tbody>
    <tr>
      <td>$\rho = 2r = \frac{2}{</td>
      <td> </td>
      <td>\mathbf{w}</td>
      <td> </td>
      <td>}$</td>
    </tr>
  </tbody>
</table>

<p>于是我们的问题便可以转化为：求取超平面的法向量$\mathbf{w}$的模
的最小值，且使得所有的样本$(\mathbf{x}_i, y_i)$有
$y_i{\mathbf{w}^T\mathbf{x}_i + b &gt; 1}$。这就是支持向量机的基本模式:</p>

<table>
  <tbody>
    <tr>
      <td>$min_{w,b}\frac{1}{2}</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>^2$</td>
    </tr>
  </tbody>
</table>

<p>$s.t. y_i(\mathbf{w}^T\mathbf{x}_i+b) \geq 1, i=1,2,…,m$</p>

<p>于是，我们的问题转换成了一个二次规划(quadratic optimization)问题。</p>

<h2 id="有约束的二次规划最优化问题拉格朗日乘子">有约束的二次规划最优化问题，拉格朗日乘子</h2>

<p>我们有许多程序包求解二次规划问题，通常而言不需要自己动手。但是了解一下
其中的数学意义还是非常有帮助的。</p>

<p>常用的解二次规划的方法之一，是应
用<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">拉格朗日乘子</a>
(Lagrange Multiplier)。</p>

<p>首先我们来看一下二次规划问题的几何意义。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Lagrange_multiplier.png" alt="" /></p>

<p>我们有约束条件$g(x,y) = c$，在图中表示成一条曲线；二次函数$f(x,y)$如
图，描述了一个曲面。假设这个曲面是一座山，约束条件的曲线是我们可以沿着
走的路，我们的目的就变成了：在这条路上，我们要走到最高的地方。图中的圆
圈表示二次函数的<a href="https://en.wikipedia.org/wiki/Level_set">水平集</a>（可
以直观地理解为等高线）。在__有解__的情况下，二次曲面与约束曲线会有交点。</p>

<p>我们可以容易看出，在（局部）最优的情况下,$f(x, y)$与$g(x,y)$的交点有且
仅有一个，即二者的__切线__相互平行。由于切线与梯度是垂直的，我们可以得
到：在最优点的情况下，二者的__梯度__相互平行。</p>

<p>我们引入一个未知标量–拉格朗日乘子$\alpha$。构建函数：</p>

<script type="math/tex; mode=display">L(\mathbf{w}, b, \mathbf{a}) = 
\frac{1}{2}||\mathbf{w}||^2 +\sum\limits_{i=1}{m}\alpha_i(1 -
y_i(\mathbf{w}^T\mathbf{x}_i + b))</script>

<p>令函数$L$对$\mathbf{w}$与$b$的偏导数为0：</p>

<script type="math/tex; mode=display"></script>

<p>$$</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>知乎 “拉格朗日乘子法如何理解？”，https://www.zhihu.com/question/38586401</li>
  <li>July(2012) 支持向量机通俗导论，http://blog.csdn.net/v_july_v/article/details/7624837</li>
</ol>

 -->
    
        <p>新年快乐，跨年也是元气满满地在机房～</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://MeowAlienOwO.github.io" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://MeowAlienOwO.github.io">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page7">7</a>
        
      </li>
    
  </ul>
  
    <a href="http://MeowAlienOwO.github.io/page3" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js"></script> -->
<!-- <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
     inlineMath: [['$', '$'], [ '\\(', '\\)']],
     displayMath: [['$$', '$$']]
     }});
     </script>
-->
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->





<script>
 var _config = {
     title: 'Entry',
     image: '22958285.jpg'
 }

 socialShare('.social-share', _config)
</script>

          

</body>
</html>
