<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><title>强化学习导论（二） [ Garden of Sinners ]</title><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0,maximum-scale=1.0, user-scalable=no"><meta name="robot" content="index,follow"><meta name="format-detection" content="telephone=no"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="stylesheet" href="/css/sprinter.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1464257_drr710yuepc.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/5.10.2/css/all.min.css"><link rel="stylesheet" href="/thirdparty/highlightjs/styles/atelier-dune-light.css"><link rel="stylesheet" href="/thirdparty/sharejs/dist/css/share.min.css"></head><body><div class="wrap"><header class="mobile-header"><div class="menu-btn-container"><div class="menu-btn"><div class="menu-bar" id="menu-bar-up"></div><div class="menu-bar" id="menu-bar-middle"></div><div class="menu-bar" id="menu-bar-bottom"></div></div></div><a class="mobile-title" href="/">Garden of Sinners</a></header><aside><div class="avatar"><div class="avatar-black"></div><img class="avatar-img" src="/images/avatar.png"></div><div class="author"><h1 class="author-name">MeowAlien喵星人</h1><div class="author-keywords"><span>Machine Learning</span><span>ACG</span><span>Kyokushin</span></div></div><div class="motto"> <h3>——式。我一生都不会放开你</h3></div><nav><ul class="menu"><li><a class="index-menu-link" href="/about">About</a></li><li><a class="index-menu-link" href="https://github.com/MeowAlienOwO/CV">Resume</a></li><li><a class="index-menu-link" href="/archives">Archives</a></li></ul></nav><div class="social"><a class="social-icon social-icon-color-github" href="https://github.com/MeowAlienOwO"><i class="iconfont icon-github"></i></a><a class="social-icon social-icon-color-weibo" href="https://www.weibo.com/u/3027659753/home?wvr=5"><i class="iconfont icon-weibo"></i></a><a class="social-icon social-icon-color-mail" href="mailto:meowalienowo@outlook.com"><i class="iconfont icon-mail"></i></a><a class="social-icon social-icon-color-bilibili" href="https://space.bilibili.com/1374912"><i class="iconfont icon-bilibili"></i></a></div></aside><div class="content"><div class="card" id="post"><div class="post-title"><h1 id="title">强化学习导论（二）</h1><time id="date" datetime="2019-10-31T09:06:37.000Z">2019年10月31日</time></div><article class="md-article container"><p>强化学习第二部分</p>
<a id="more"></a>
<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>动态规划的基本思路是：将问题划分为可存储的子优化问题，通过解决子问题来最终解决父问题。<br>在强化学习中，由于MDP的贝尔曼方程的存在，我们可以很容易地将问题递归表示。</p>
<h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>基本的动态规划算法为策略迭代。策略迭代分为两大部分：</p>
<ol>
<li>对策略的价值估计</li>
<li>对策略的优化</li>
</ol>
<p>具体而言，就是先用当前的策略进行价值估计得到$v_t$, 然后根据估计的价值来更新$\pi_t$，在下一时刻，使用更新后的策略继续估算价值。<br>价值估计与优化问题合起来被称为控制问题，这两部分是强化学习所重点关注的地方。</p>
<h3 id="Iterative-Policy-Evaluation-迭代策略估计"><a href="#Iterative-Policy-Evaluation-迭代策略估计" class="headerlink" title="Iterative Policy Evaluation 迭代策略估计"></a>Iterative Policy Evaluation 迭代策略估计</h3><p>利用贝尔曼方程我们可以递归的计算$v_\pi$:</p>
<script type="math/tex; mode=display">
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s', r} p(s' r | s, a)[r + \gamma v_k(s')]</script><p>基本的算法为:</p>
<pre><code>input policy pi
init array V[s]=0 for s in S

Repeat 
  delta := 0
  for each s in S:
    v := V(s)
    V(s) := sum_a(pi(a|s) * sum_s&#39;,r(p(s&#39;, r| s,a)[r + gamma V[s&#39;]]))
    delta := max(delta, |v - V(s)|)
until delta &lt; theta
return V
</code></pre><p>TODO: 收敛性证明</p>
<h3 id="策略优化"><a href="#策略优化" class="headerlink" title="策略优化"></a>策略优化</h3><p>我们首先给出策略优化原理：</p>
<blockquote>
<p>对于一个策略$\pi’$，如果对于所有的状态$s$都有$\sum<em>a \pi’(a|s) q</em>\pi(s, a) \geq \sum<em>a \pi(a|s) q</em>\pi(s, a) $</p>
</blockquote>
<p>那么，策略$\pi’$就不坏于策略$\pi$。<br>对于任意的$v<em>\pi(s)$, 数值上，有$v</em>\pi(s) \leq q<em>\pi(s,\pi’(s)) \leq v</em>{\pi’}(s)$<br>后者可以通过期望式展开成序列形式为：$\leq \mathbb{E}[R<em>{t+1} + \gamma v</em>\pi(S<em>{t+1})|S_t = s] \leq \mathbb{E}</em>{\pi’}[R<em>t+1 + \gamma q</em>\pi(S<em>{t+1}, \pi’(S</em>{t+1}| S_t = s))]$<br>TODO: 详细证明</p>
<p>策略优化算法如下：</p>
<pre><code>init V[s] in R and pi(s) in A(s) arbitrarily, for all s in S

# policy evaluation
Repeat 
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := sum_s&#39;,r(p(s&#39;, r|s, a)[r + gamma * V[s&#39;]])
  until delta &lt; theta

# policy improvement
Repeat
  policy-stable := true
  for each s in S:
    a := pi(s)
    pi(s) := argmax_a sum_s,r(p(s&#39;, r|s, a)[r + gamma V[s&#39;]])

    if a != pi(s) then policy-stable := false
  if policy-stable == true then stop; else goto evaluation
</code></pre><h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><p>价值迭代与策略迭代的不同在于：策略迭代每次先进行evaluation,然后根据evaluation的结果选择动作，而价值迭代直接计算每一个动作的期望，根据期望来选取动作。</p>
<p>算法如下:</p>
<pre><code>init array V abitrarily

Repeat
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := max_a sum_s,r(p(s&#39;, r| s, a)[r + gamma * V(s&#39;)])
    delta := max(delta, |v - V[s])
  until delta &lt; theta

  output deterministic policy pi, where
    pi(s) = argmax_a sum_s,r(p(s&#39;, r|s, a)[r + gamma*V[s&#39;]])
</code></pre><h1 id="Monte-Carlo-Methods蒙特卡洛法"><a href="#Monte-Carlo-Methods蒙特卡洛法" class="headerlink" title="Monte-Carlo Methods蒙特卡洛法"></a>Monte-Carlo Methods蒙特卡洛法</h1><p>动态规划成立的前提是，我们知道环境动态函数$p$，而蒙特卡洛法是为了解决在没有环境动态函数的情况下进行强化学习的问题的统计学方法。</p>
<p>蒙特卡洛法是一种通过经验学习价值函数的方法，其中的经验有两种：</p>
<ol>
<li>实际经验，从环境中真实学习的经验。</li>
<li>模拟经验，使用一个模型来近似真实的环境</li>
</ol>
<p>在蒙特卡洛法中，价值$v_\pi$被定义成对回报的采样的平均数。</p>
<script type="math/tex; mode=display">
v_\pi(s) \circeq \mathbb{E}[\sum_{k=0}^{T-1}\gamma^kR_{k+1}] \sim \frac{1}{\Epsilon(s)} \sum_{t_i \in \Epsilon(s)}\sum_{k=t_i}^{\gamma^{k-1}R^i_{k+1}}</script><h2 id="蒙特卡洛估计"><a href="#蒙特卡洛估计" class="headerlink" title="蒙特卡洛估计"></a>蒙特卡洛估计</h2><p>蒙特卡洛有两种计算方法，当每个状态，动作的访问次数趋于无穷时，它们是等价的:</p>
<ol>
<li>first-visit MC: 只考虑每个episode第一次访问到的(S, A) 对</li>
<li>every-visit MC: 对所有的(S, A)对进行采样</li>
</ol>
<p>我们这里给出first-visit 的算法：</p>
<pre><code>Init:
  pi := policy to be evaluated
  V := arbitrarily init
  Returns[s] := [] for all s in S

Repeat forever:
  Generate an episode using pi
  for each state s in episode:
    G := return following first occurence of s
    Append G to Returns[s]
    V[s] := average(Return[s])
</code></pre><p>蒙特卡洛法同样可以对动作价值进行估计：</p>
<script type="math/tex; mode=display">
q_\pi(s, a) \circeq \mathbb{E}[G_t | S_t = s, A_t = a]</script><h2 id="MC控制"><a href="#MC控制" class="headerlink" title="MC控制"></a>MC控制</h2><p>采取贪婪策略进行动作选择满足策略优化定律，我们假设MC的迭代是无限的，给出蒙特卡洛控制算法：</p>
<pre><code>Init 
  for all s in S, a in A:
    Q(s, a) := arbitrarily
    pi(s) := arbitrarily
    Returns(s, a) := []

Repeat forever:
  Choose S_0 in S, A_0 in A[S_0] as start point s.t all pairs have prob &gt; 0
  Generate an episode according to pi
  For (s, a) in episode:
    G := return of s, a
    Append G to Returns(s, a)
    Q(s, a) := average(Returns(s, a))
  For each s in episode:
    pi[s] &lt;- argmax_a Q(s, a]
</code></pre><p>单纯的贪婪会使得更新参数变得很慢—很可能陷入某个局部最优然后不断强化，忽视探索其他动作，我们通常使用e-soft 策略来保证探索。e-greedy不改变期望。</p>
<h2 id="Off-policy-蒙特卡洛"><a href="#Off-policy-蒙特卡洛" class="headerlink" title="Off-policy 蒙特卡洛"></a>Off-policy 蒙特卡洛</h2><p>estimate的时候的策略$$ 与目标策略$pi$ 不完全相等时，我们称该方法为off-policy方法。</p>
<p>使用蒙特卡洛法时，对于off-policy方法，我们的estimate期望将不基于目标Policy, 而是基于我们的估计策略的policy.<br>在这种情况下，我们可以将整个采样过程看做一个重要性采样，采样的分布是基于估计策略$u$的分布。<br>我们需要用<strong>重要性系数</strong>来修正期望：</p>
<script type="math/tex; mode=display">
\rho_{t:T} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{u(A_k)|S_k}</script><p>在重要性系数修正下：</p>
<script type="math/tex; mode=display">
\mathbb{E}[\rho_{t:T}G_t|S_t = s] = v_\pi(s)</script><p>由于重要性采样会导致方差存在上升至无限大的可能性(将重要性采样看做是在某一区间特别密集地取值)，我们还需要对采样的平均长度进行修正：</p>
<script type="math/tex; mode=display">
\eta^{-1} = \sum_{t_i in \Epsilon(s)}\rho_{t:T}</script><h1 id="时间差分算法-Temporal-Difference"><a href="#时间差分算法-Temporal-Difference" class="headerlink" title="时间差分算法(Temporal-Difference)"></a>时间差分算法(Temporal-Difference)</h1><p>最基本的时间差分算法(TD(0))可以看做每次仅仅往前方看一步进行evaluation。由于这种情况下我们没有办法获得整个序列的回报，我们用当前估计的期望来作为我们的Target。</p>
<h2 id="TD-Evaluation"><a href="#TD-Evaluation" class="headerlink" title="TD Evaluation"></a>TD Evaluation</h2><p>最基础的TD Evaluation 算法(TD(0))如下，可以看出同MC方法相比，TD(0)最重要的改变是将更新公式中的累计回报换成了当前的奖励信号与当前估计的下一状态的期望之和:</p>
<pre><code>input policy pi
step size 0 &lt; alpha &lt;= 1
init V[s] for all s in S, arbitrarily except V(terminal) = 0

for each episode:
  init S
  for each step in episode:
    A := action from pi(S)
    take action, observe R, S&#39;
    V[s] := V[s] + alpha[R + gamma * V[S&#39;] - V[S]]
    S := S&#39;
  until S is terminal
</code></pre><p>TD算法的好处：首先，每次都要进行更新，避免了蒙特卡洛法需要迭代完整的一个episode再进行更新的问题，其次，需要的计算力与空间更少</p>
<h2 id="TD-Control"><a href="#TD-Control" class="headerlink" title="TD Control"></a>TD Control</h2><p>根据是否on-policy, 我们可以将TD Control 分成两种算法:</p>
<ol>
<li>SARSA: on-policy control</li>
<li>Q-learning: off-policy control</li>
</ol>
<h3 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h3><p>我们给出SARSA算法如下</p>
<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0

Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S&#39;
    Choose A&#39; fomr S&#39; using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * Q[S&#39;, A&#39;] - Q[S, A]]
    S := S&#39;
    A := A
  Until S is terminal
</code></pre><p>SARSA有一种变种：不使用<code>Q[S&#39;, A]</code> 来进行更新，而是使用下一步状态的期望<code>sum_a (pi(S&#39;)Q(S&#39;, a))</code>来进行更新，其思路主要是通过期望计算来降低方差，从而提升学习效率。</p>
<h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>Q-learning 使用 算法如下：</p>
<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0


Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S&#39;
    Choose A&#39; fomr S&#39; using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * max_a (Q[S&#39;]) - Q[S, A]]
    S := S&#39;
  Until S is terminal
</code></pre><p>注意，同MC Off-policy相比，这个方法不需要重要性系数。其原因是动作a此处是确定的(argmax(Q[S]))，而非随机变量。</p>
<h2 id="N-Step-TD"><a href="#N-Step-TD" class="headerlink" title="N-Step TD"></a>N-Step TD</h2><p>N-Step TD 是在单步TD与MC方法中间的桥梁：</p>
<script type="math/tex; mode=display">
G_{t:t+n} = \sum_{k=1}^{n}\gamma^{k-1}R_{t+k} + \gamma^n V_{t+n-1}(S_{t+n})</script><p>在n-step TD的情况下，我们需要使用重要性系数来修正我们的off-policy算法的价值估计。</p>
<script type="math/tex; mode=display">
Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha \rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]

\rho_{t:h} = \prod_{k=t}^{\min(h,T-1)} \frac{\pi(A_k|S_k)}{u(A_k|S_k)}</script></article><div class="post-footer"><div class="tags"><span><i class="fas fa-tag"></i></span><a href="/tags/ml/"><span class="tag">ml</span></a><a href="/tags/rl/"><span class="tag">rl</span></a></div><div class="share"><div class="social-share" data-image="/images/avatar.png" data-sites="weibo,wechat,qq,twitter,facebook"></div><script src="/thirdparty/sharejs/dist/js/share.min.js"></script></div></div></div><footer><div><span>Site construction by</span><span> MeowAlien喵星人 </span><span>using </span><span></span><a href="http://hexo.io">hexo</a><span>.</span><br></div></footer></div></div><script src="//cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><script src="/js/sprinter.js"></script><script>configMath()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } }
});
MathJax.Hub.Queue({
  function(){
    // $('.MathJax_Display').wrap("<div class='formula'></div>")
  }
});
</script><script id="MathJax-script" src="//cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script src="/thirdparty/highlightjs/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script></body></html>