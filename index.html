<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Entry &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
     <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
     </script> -->

<!-- Share.js -->
<!-- <link href="/assets/css/share.min.css" rel="stylesheet"> -->
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js"></script> -->
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Entry">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/machine learning/">
				    machine learning (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/RL/">
				    RL (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=22958285">ZERO | STAR影法師 [pixiv]</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://MeowAlienOwO.github.io/images/22958285.jpg" alt="Entry">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>喵窝[0]号机</h1>
      <h2>一个伪装成技术博客的吐槽网站。</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-04-29T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/reinforcement-learning-3/">April 29, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/reinforcement-learning-3/" rel="bookmark" title="强化学习（三）" itemprop="url">强化学习（三）</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>强化学习导论第三部分，planning, value function approximation, eligibility traces, policy gradient methods</p>

<h1 id="planning">Planning</h1>

<p>之前我们涉及到的强化学习方法中，DP是在$p$函数已知的情况下，其余的MC，TD都是通过统计方法来近似价值函数的期望，不需要一个先验的模型(model-free)。但是，在
许多情况下，我们有很多对模型的先验知识，如果抛弃这些先验知识可能会使得训练时间过长从而不实际。我们通过引入一个包含先验知识的模型来解决这个问题。</p>

<p>我们定义<code>计划</code>(planning)为使用模型，进行一系列动作规划的过程，根据这个定义，动态规划就是一种计划，因为其使用了$p$函数来描述环境模型。</p>

<p>对于模型而言有两种主要的分类：</p>
<ol>
  <li>分布模型(distributional model): 解析地表示出给定的分布函数$p$</li>
  <li>模拟模型(simulation model): 生成一系列从模型的分布中采样的数据</li>
</ol>

<p>实际而言，很多问题的分布模型很复杂或者无法用解析方法表示，在这种情况下，我们选用模拟模型。</p>

<h2 id="dyna-q">Dyna-Q</h2>
<p>Dyna-Q 算法是一种将planning 与 learning结合起来的算法。其具体流程如下：</p>

<pre><code>Init
  Q(s, a), Model(s, a) for s in S, a in A

Repeat forever:
  S := current state
  A := e-greedy(S, Q)
  Execute action A, observe R, S'

  # this step is direct RL:
  Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S',a)) - Q(S, A)]

  # learning model
  Model(S, A).update(R, S') 

  # use model planning
  Repeat n times:
    S := random previously observed state
    A := random action previously taken in S
    R, S' := Model(S, A)
    Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S', A)) - Q(S, A)]

</code></pre>

<p>Dyna-Q 算法的关键步骤大致分为三步：第一步，正常的通过RL方法更新动作价值函数$Q$；第二步，根据历史的经验，更新模型，这里单纯地记忆当前的模型的下一步的反馈；第三步，将过去的经验中，已经经历过的(s,a)对提取出来，使用模型计算该(s, a)对的下一状态与奖励，然后更新$Q$函数。</p>

<p>这里，我们重复一下模型，价值函数，策略之间的关系:</p>
<ol>
  <li>通过价值函数与策略我们可以生成当前需要进行的动作</li>
  <li>在环境中执行动作(action)我们可以得到经验(experience)</li>
  <li>我们可以通过经验来优化我们的价值函数，或者策略</li>
  <li>经验同样地可以被用来进行模型的学习(model learning)</li>
  <li>根据学习后的模型，通过计划，我们同样可以得到价值函数或者策略</li>
</ol>

<p><img src="/images/3-planning-learning-relation.png" alt="" /></p>

<p>Dyna-Q 相比普通的Q-learning，其提升在于: Q-learning一开始只会对经验过的policy进行更新；但是Dyna-Q利用的planning,可以继续生成一系列模拟的经验，从而有着更好的更新覆盖率。如果用机器人走迷宫的例子来说，第一个episode，只有终点的奖励，Q-learning只会更新终点前一格的价值；但是另一方面，通过planning，我们可以生成一系列的(s, a)对，从而对前面若干步的格子的价值函数进行更新。</p>

<p>另一方面，模型有可能是错的：环境可能改变，先验的知识也不一定符合现状。环境变化有两种可能性：一种是，环境变“坏”了，即我们当前的策略在新的环境下没有办法获取或者只能获取低水平的奖励；第二种是，环境变“好”了，我们的解依然能够获取同样水平的奖励，但是存在一个更优的解。在环境变坏的情况下，通过planning我们一般不能得到最优的policy，但是通过不断的exploit，模型能够发现这个这个解不是最优，从而慢慢找到更新的解。但是在环境变好的情况下，模型很难去发现更优的解法，而是会陷入在原有的策略中不断exploit。为了解决这个问题，我们需要平衡探索与exploit。</p>

<p>Dyna-Q+是一个变种的Dyna-Q算法。在这个算法中，我们用$R + k\sqrt{\tau}$来代替奖励信号，其中$\tau$是一个衡量上一次访问该状态距今的时间，$k$是控制平衡的系数。这种做法能够赋予那些长时间未访问的状态更高权重，从而让模型能够更新那些很久都没有访问过的状态的价值。</p>

<p>另一种Dyna-Q的变种Prioritized Sweeping Dyna-Q考虑从模型中选取状态的随机性问题。随机性容易产生很多无效的(s, a)对，在遍历这些状态的时候，我们的价值实际上不更新。换而言之，虽然planning扩展了更新价值的范围，但是这些扩展的范围仍旧局限在终点附近。对于每一步，我们将[Target-OldEstimate]这一项作为优先级，将我们的历史经验存储在优先级队列之中。选取模型的时候，我们从优先级队列中选取，并且将新生成的(s, a)对重新按照优先级放入优先级队列中。优先级系数有一个截断阈值$\theta$来控制。这种算法有着更优的模型收敛速度。</p>

<h2 id="rollout-planning">Rollout Planning</h2>
<p>Dyna-Q的模型重用了历史经验，而rollout planning使用模型来模拟将来的轨迹(trajectory)。每个轨迹从当前状态出发来</p>

<p>我们给出前向更新的rollout算法：</p>
<pre><code>Input 
  Model for simulation

Init Q(s, a) for all s, a

for t = (0, ...) do:
  S_t := current state
  for n times(n rollouts) do
    S := S_t
    while s non-terminal/within fixed length do:
      action A based on Q(S,:), with exploration # i.e. e-greedy
      (R, S') sample from Model(S, A)
      Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S', a)) - Q(S, A)]
      S := S'

  Select action A_t greedly from Q(S_t, :)
</code></pre>

<p>这种算法下，如果模型是正确的且满足每个状态访问无限次，该算法能够学习到最优策略，反之如果模型不正确，则无法得到最优解。</p>

<p>我们给出反向更新的rollout算法，这个算法的目的是更好的利用奖励</p>

<pre><code>Input 
  Model for simulation

Init 
  Q(s, a) for all s, a
  trace = [], stack

for t = (0, ...) do:
  S_t := current state
  for n times(n rollouts) do
    S := S_t
    # rollout
    while s non-terminal/within fixed length do:
      action A based on Q(S,:), with exploration # i.e. e-greedy
      (R, S') sample from Model(S, A)
      trace.push(S,A,R,S')
      S := S'

    # backprop
    while trace != [] do:
      (S,A,R,S') := trace.pop()
      Q(S, A) := Q(S, A) + alpha[R + gamma * max_a(Q(S', a)) - Q(S, A)]


  Select action A_t greedly from Q(S_t, :)
</code></pre>

<p>在走迷宫问题上，前向传播可以看做是这一次的奖励更新奖励点前最后一个状态与上一次的奖励更新前面所有状态的复合，而反向传播会直接用当前的奖励更新所有的状态。在这个意义上，反向传播有着较正向传播稍快的学习速度。</p>

<h2 id="monte-carlo-tree-search-蒙特卡洛树搜索">Monte-Carlo Tree Search 蒙特卡洛树搜索</h2>

<p>蒙特卡洛树搜索(MCTS)是一种更加通用，高效的rollout planner。
对于每一个状态，MCTS都被用于动作的选取。每次执行MCTS都是一个迭代过程，这个迭代过程模拟从当前状态触发到终止状态的轨迹/衰减率限制。
MCTS的核心是通过不断拓展当前状态的一部分在早期模拟中获得高回报(high evaluation)的轨迹，来集中在多个模拟上(原文比较难懂)
MCTS不保存从一个动作选取到下一个的近似价值函数或者策略。</p>

<p>自己的理解：MCTS存储一个部分的(partial)$Q$函数，每次MCTS会从最可能的那一个节点进行扩展。</p>

<p><img src="/images/MCTS.jpg" alt="" /></p>

<p>MCTS根据模拟的输出结果构造搜索树，由以下四个主要步骤组成：</p>
<ol>
  <li>选择(selection): 从根节点开始，递归的选择最优的子节点直到叶节点(不同于终止状态)</li>
  <li>扩展(expansion): 如果选择的节点不是终止状态，则对这个节点进行扩展，创建这个节点的一个或者多个子节点</li>
  <li>模拟(simulation): 选取上步创建的一个子节点，从子节点开始使用默认的策略(default policy)来模拟，直到终止状态</li>
  <li>反向传播(back propagation):用模拟的输出结果，更新从根节点到扩展的子节点的价值</li>
</ol>

<p>一般的MCTS算法如下：</p>

<pre><code>Init
  S_0: init state 
  DefaultPolicy: default policy 
  Q: {v_0: S_0} MCTS tree with root node S_0

S := S_0
Repeat :
  v := node in Q s.t. state(v) = S
  while computationally possible:
    v_t := TreePolicy(v_0) # best polict by tree
    delta := DefaultPolicy(state(v_t))
    Backprop(v_t, delta)
  action A is BestChild(v_0)
  
</code></pre>

<p>在模拟这一部分，我们使用的是一个不同于Tree policy的策略。这个策略可以是最简单地随机策略，也可以使用一些启发式搜索或者多重模拟的平均。</p>

<h3 id="upper-confidence-bounds-for-treesuct-上确界树">Upper Confidence Bounds for Trees(UCT) 上确界树</h3>
<p>如果我们将选择步骤的每一层都看做多臂赌博机问题，我们可以选用不同的方法来平衡探索与收益。
UCT使用UCB来进行每一个子节点的选择。在这里，UCB公式可以写成如下形式:</p>

<script type="math/tex; mode=display">v_i + C \sqrt{\frac{\log N}{n_i}}</script>

<p>其中$v_i$是节点的估计价值，$n_i$是节点被访问的次数，$N$是父节点被访问的次数，$C$是平衡系数。</p>

<h2 id="在线离线学习">在线/离线学习</h2>

<p>RL同样有在线学习与离线学习两种。</p>

<p>在线学习:</p>
<ol>
  <li>在真正的游戏执行之前，用MDP找到最优策略</li>
  <li>策略是完备的：对所有可能的状态,都能够找到最优策略</li>
  <li>Use as much time as needed to find policy
Dyna-Q, DP
离线学习:</li>
  <li>在游戏中使用MDP寻找最优策略</li>
  <li>策略是不完备的：仅对当前的状态寻找最优策略</li>
  <li>有限的时间（e.g.下棋读秒）</li>
</ol>

<h1 id="value-function-approximation">Value Function Approximation</h1>

<p>对于非常巨大的状态空间而言，直接存储价值表是不现实的，我们自然就会想到使用一个函数$\hat{v}<em>\pi$来近似$v</em>\pi$，这样我们可以在有限的内存下仍然能够处理巨大的状态空间。
另一个问题是，相比较于状态空间，能获取的样本只是其中很小的一个部分，因此遍历所有的状态是不可能的。由这两个问题，我们引出近似价值函数。</p>

<p>我们的目标是，使用参数化的函数来近似价值表:</p>

<script type="math/tex; mode=display">\hat{v}(s, \mathbf{w}) \simeq v_\pi(s)
\hat{q}{s, a, \mathbf{w}} \simeq q_\pi(s, a)</script>

<p>这带来两个好处：</p>
<ol>
  <li>参数的数量一般而言远小于状态空间的大小</li>
  <li>泛化能力，一个参数的变化可以对应若干对应的状态/动作价值的变化</li>
</ol>

<p>价值函数的学习一般而言通过有监督学习，我们使用一个状态-价值对来训练我们的函数：(S, U)</p>
<ol>
  <li>MC: U = G</li>
  <li>TD(0): U = R + gamma * v(S’, wt)</li>
  <li>n-step TD: U = R + … + gamma^n-1 Rn + gamma^n v(Sn, wn-1)</li>
</ol>

<p>这种方法一般而言能够进行增量更新，以及能够处理噪音。</p>

<p>我们一般采取均方误差来作为我们的损失函数。</p>

<p>在损失函数可导的情况下，我们采用随机梯度下降来增量学习参数$w$
<script type="math/tex">\mathbf{w}_{t+1} = \mathbf{w}_t - \frac{1}{2}\alpha \nabla J(\mathbf{w}_t) = \mathbf{w}_t + \alpha [U_t - \hat{v}(S_t, \mathbf{w}_t) \nabla\hat{v}(S_t, ]\mathbf{w}_t)]</script></p>

<p>对于MC而言，$\mathbb{E}[U|S]$是一个对$v_\pi$的无偏估计，我们可以放心使用；但是对于TD而言，由于其为bootstrap方法，$U$是一个有偏估计。
在这种情况下，我们使用半梯度(semi-gradient)。之所以叫半梯度是因为$U=R+\gamma\hat{v}(S,\mathbb{w})$跟$w$有关：</p>

<pre><code>Input:
  pi: policy 
  v_hat: differenciable function , v_hat(terminal, :) = 0
  alpha: step size 
  w: weights, arbitrarily init

loop for each episode:
  init S
  for each step in episode:
    A chosen from pi(S)
    take action A, observe R, S'
    w := w + alpha[R+gamma v_hat(S', w) - v_hat(S, w)] d(v_hat(S,w))/dw
    S := S'
  until S terminal
  
</code></pre>
<p>在线性的情况下，只有一个最优：</p>
<ul>
  <li>MC收敛至全局最优</li>
  <li>TD收敛至接近全局最优的不动点</li>
</ul>

<p>我们通过coarse/tile coding 来抽取特征：
粗编码(coarse)将整个状态空间划分为重叠的若干子空间，每一个空间内的点点亮当前的子空间，并且同时点亮同该空间互相重叠的子空间–邻域，
在粗编码的情况下，我们可以仅仅对在状态空间内相邻的状态值进行更新。
片编码(tile)对状态点附近的一个方形区域进行切割，然后将该区域上下左右平移得到新的领域。</p>

<h2 id="控制问题">控制问题</h2>
<p>同状态价值相似，我们可以直接写出动作价值的梯度表示，从而实现控制。在线性函数下，我们有:</p>

<ol>
  <li>SARSA: U = R + gamma q(S, A, w)</li>
  <li>Q-learning: U = R + gamma max_a q(S, a, w)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Expected SARSA: U = R + gamma sum_a \pi(a</td>
          <td>S) q(S, a, w)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<h1 id="eligibility-trace-资格迹">Eligibility Trace 资格迹</h1>

<p>资格迹可以看做是另外一种MC/TD的插值方式。
在n-step中，我们逐步计算n步的奖励，之后的奖励用现在的v值近似。
n-step 存在的问题是：更新需要等待n步，有时太长了。
我们可以换一种思路：对不同的n的G值做加权平均，可以保证其价值函数的收敛性。</p>

<h2 id="tdlambda">TD(lambda)</h2>

<p>TD(lambda)的基本思路是：对从当前时刻开始的所有G进行加权求和，使用一个指数权重来控制从当前时刻开始每一个时刻$t+k$的期望，我们称这个期望为lambda-return。</p>

<script type="math/tex; mode=display">G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}</script>

<p>这个公式可以写成如下形式，以方便我们观察Termination：</p>

<script type="math/tex; mode=display">G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_t</script>

<p>我们可以认为这个式子截断了n步以后的情形，以后项来代替截断后的期望。可以看出，lambda项等于0的时候相当于TD(0), 等于1的时候相当于MC方法。
线下的lambda-return算法的更新公式为：$\alpha[G_t^\lambda-\hat{v}(S_t,\mathbb{w_t})]\nabla\hat{v}(S_t,\mathbb{w}_t)$</p>

<h3 id="前向与后向解释">前向与后向解释</h3>
<p>TD(lambda)的前向视角就是lambda-return的视角：向前看若干步，然后进行lambda加权平均。</p>
<blockquote>
  <p>对于每个访问到的state，我们都是从它开始向前看所有的未来reward，并决定如何结合这些reward来更新当前的state。每次我们更新完当前state，我们就到下一个state，永不再回头关心前面的state</p>
</blockquote>

<p>我们来更形象地表述一下后向视角的过程：每次在当前访问的状态得到一个误差量的时候，这个误差量都会根据之前每个状态的资格迹来分配当前误差。这就像是一个小人，拿着当前的误差，然后对准前面的状态们按比例扔回去。
TD(λ)的后向视角非常有意义，因为它在概念上和计算上都是可行而且简单的。具体来说，前向视角只提供了一个非常好但却无法直接实现的思路，因为它在每一个timestep都需要用到很多步之后的信息，这在工程上很不高效。而后向视角恰恰解决了这个问题，采用一种带有明确因果性的递增机制来实现TD(λ)，最终的效果是在on-line case和前向视角近似，在off-line case和前向视角精确一致。</p>

<h2 id="资格迹">资格迹</h2>
<p>引入一个同每个状态都相关的变量$z$，在每一个episode的时候初始化为0，以$\gamma\lambda$为衰减率，对每一个时刻的梯度进行加权:</p>

<script type="math/tex; mode=display">\mathbb{z}_{-1}=0</script>

<script type="math/tex; mode=display">\mathbb{z}_t = \gamma\lambda\mathbb{z}_{t-1} + \nabla \hat{v}(S_t, \mathbb{w}_t)</script>

<p>资格迹可以看做对权重更新的平滑：当参数更新时，其与历史累计的更新幅度加载一起；当没有激活参数的更新时，之前的更新会慢慢回落至0。
从另一种角度看，当资格迹不作用于参数化函数时，可以看做一个梯度恒为定值的函数。</p>

<p>根据前述的半梯度TD，我们引入Semi-Gradient TD(lambda)算法：</p>
<pre><code>for each episode:
  Init S
  z := 0
  for each step in episode:
    A chosen by pi
    take action A, observe R, S'
    z := gamma * lambda * z + dv(S,w)/dw
    delta := R + gamma * v(S', w) - v(S, w)
    w := w + alpha * z *delta
    S := S'
  until S terminal
</code></pre>
<p>其中， lambda称为衰减率。</p>

<h3 id="online算法">Online算法</h3>

<p>在线的lambda-return算法实际上使用截断的lambda-return：
<script type="math/tex">G_{t:h}^\lambda = (1-\lambda)\sum_{n=1}^{h-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{h-t-1}G_{t:h}</script>
这个算法可以看做是长度为定值的lambda-return算法，我们将整个轨迹在h处截断，后面的概率使用截断后一个时间点的期望$\lambda^{h-1}G_{t:h}$来替代</p>

<h1 id="policy-gradient">Policy Gradient</h1>

<p>同Value Approximation 类似，Policy Gradient 将策略参数化，从而学习到更优的策略。
<script type="math/tex">\pi(a|s, \theta) = Pr{A_t = a| S_t = s, \theta_t = \theta}</script></p>

<p>Policy Gradient的优点:</p>
<ol>
  <li>更容易收敛到局部最优</li>
  <li>在高维与连续的动作空间有更好的表现</li>
  <li>可以学习到随机策略</li>
</ol>

<p>对于离散的动作空间，我们通常使用softmax来描述动作的概率；而对于连续的动作空间，可以使用高斯分布来描述。</p>

<p>我们优化的目标可以是初状态的价值期望：
<script type="math/tex">J(\theta) \circeq v_\pi_\theta(S_0)</script>
或者，如果是连续性的任务，可以优化平均奖励：
<script type="math/tex">J(\theta) \circeq \sum_s P_\pi(s) \sum_{a}\pi(a|s, \theta)\sum_{s', r}p(s',r|s,a)r</script></p>

<h2 id="策略梯度定理">策略梯度定理</h2>

<p>对于任意的可微策略$\pi$，策略梯度为
<script type="math/tex">\nabla J(\theta) = \sum_s d_\pi(s) \sum_a q_\pi(s, a) \nabla \pi(a|s, \theta)</script>
其中，$d_\pi(s)$是在策略$\pi$下的on-policy分布。</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>对于初始状态而言我们有：$d_\pi(s)=\sum_{t=0}^{\infty}\gamma^t Pr{S_t=s</td>
          <td>s_0, \pi}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>对于平均回报我们有：$d_\pi(s) = \lim_{t\rightarrow \infty} Pr{S_t = s</td>
          <td>\pi}$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>我们在这里不需要推导环境的动态函数$p$。</p>

<p><script type="math/tex">% <![CDATA[
\begin{aligned}
\nabla J(\theta) &= \sum_s d_\pi(s) \sum_a q_\pi(s, a) \nabla \pi(a|s, \theta) \\
                 &= \mathbb{E}_\pi[\sum_a q_\pi(s, a) \nabla \pi(a|s, \theta)] \\ 
                 &= \mathbb{E}_\pi[\sum_a \pi(a|s, \theta) q_\pi(s, a) \frac{\nabla \pi(a|s, \theta)}{\pi(a|s, \theta)} ]_\\
                 &= \mathbb{E}_\pi[q_\pi(S_t, A_t)\nabla \ln(\pi(A_t|S_t, \theta))]
\end{aligned} %]]></script>
注意此处对不同的时刻取期望值，我们的更新迭代公式可以写成：</p>

<script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} + \alpha(q_\pi(S_t, A_t)\nabla \ln\pi(A_t|S_t, \theta))</script>

<p>在这里，我们有两个任务要完成：</p>
<ol>
  <li>计算或者近似策略函数的导数</li>
  <li>近似策略$q$</li>
</ol>

<p>为了避免选择无法反应状态之间的量级差异，我们有时会将其与baseline进行比较，在这里我们有：</p>

<script type="math/tex; mode=display">(q_\pi(S_t, A_t) - b(S_t))\nabla \ln \pi(A_t|S_t, \theta)</script>

<p>这个baseline 不会改变期望，但是可以降低方差。</p>

<p>我们给出基于策略梯度定理与baseline的蒙特卡洛更新算法:</p>

<pre><code>Var:
  pi(a|s, theta): differenciable policy
  v(s, w): differenciable state-value function
  a_t, a_w: step size parameters, greater than 0

init policy parameter and state-value weights randomly

for each episode(forever):
  generate episode by pi
  for each step in episode:
    G := sum_(k=t+1:T)(gamma^k R_k)
    delta = G - v(S_t, w)
    w := w + a_w * delta * dv(S_t, w)/dw
    theta := theta + a_t * gamma^t * delta * d(ln(pi(A_t|S_t, theta)))/dtheta
</code></pre>

<h2 id="actor-critic-methods">Actor-Critic Methods</h2>
<p>上述基本算法由于采用了蒙特卡洛法，同样会有蒙特卡洛法的问题：直到一个episode完了才能进行更新，导致学习很慢。
这里，我们使用同样的将蒙特卡洛换成TD的思路，给出Actor-Critic 方法。</p>

<script type="math/tex; mode=display">\theta_{t+1} = \theta_{t} + \alpha [R_t + \gamma v(S_{t+1}, w) - v(S_t, w)]\nabla \ln\pi(A_t|S_t, \theta)</script>

<p>具体算法如下：</p>

<pre><code>Var:
  pi(a|s, theta): differenciable policy
  v(s, w): differenciable state-value function
  a_t, a_w: step size parameters, greater than 0

init policy parameter and state-value weights randomly

for each episode:
  Init S
  I := 1
  for each step in episode:
    A from pi
    take action A, observe S', R
    delta := R + gamma * v(S', w) - v(S, w)
    w := w + a_w * delta * dv(S, w)/dw
    theta := theta + alpha * delta * I * d(ln(pi(A|S, theta)))/dtheta
    I := gamma I
    S := S'
  until S terminate

</code></pre>

 -->
    
        <p>强化学习导论第三部分，planning, value function approximation, eligibility traces, policy gradient methods</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-04-28T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/reinforcement-learning-2/">April 28, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/reinforcement-learning-2/" rel="bookmark" title="强化学习（二）：动态规划，蒙特卡洛法，时间差分" itemprop="url">强化学习（二）：动态规划，蒙特卡洛法，时间差分</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>强化学习第二部分</p>

<h1 id="动态规划">动态规划</h1>

<p>动态规划的基本思路是：将问题划分为可存储的子优化问题，通过解决子问题来最终解决父问题。
在强化学习中，由于MDP的贝尔曼方程的存在，我们可以很容易地将问题递归表示。</p>

<h2 id="策略迭代">策略迭代</h2>

<p>基本的动态规划算法为策略迭代。策略迭代分为两大部分：</p>
<ol>
  <li>对策略的价值估计</li>
  <li>对策略的优化</li>
</ol>

<p>具体而言，就是先用当前的策略进行价值估计得到$v_t$, 然后根据估计的价值来更新$\pi_t$，在下一时刻，使用更新后的策略继续估算价值。
价值估计与优化问题合起来被称为控制问题，这两部分是强化学习所重点关注的地方。</p>

<h3 id="iterative-policy-evaluation-迭代策略估计">Iterative Policy Evaluation 迭代策略估计</h3>

<p>利用贝尔曼方程我们可以递归的计算$v_\pi$:
<script type="math/tex">v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s', r} p(s' r | s, a)[r + \gamma v_k(s')]</script></p>

<p>基本的算法为:</p>

<pre><code>input policy pi
init array V[s]=0 for s in S

Repeat 
  delta := 0
  for each s in S:
    v := V(s)
    V(s) := sum_a(pi(a|s) * sum_s',r(p(s', r| s,a)[r + gamma V[s']]))
    delta := max(delta, |v - V(s)|)
until delta &lt; theta
return V

</code></pre>
<p>TODO: 收敛性证明</p>

<h3 id="策略优化">策略优化</h3>
<p>我们首先给出策略优化原理：</p>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>对于一个策略$\pi’$，如果对于所有的状态$s$都有$\sum_a \pi’(a</td>
        <td>s) q_\pi(s, a) \geq \sum_a \pi(a</td>
        <td>s) q_\pi(s, a) $</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>那么，策略$\pi’$就不坏于策略$\pi$。
对于任意的$v_\pi(s)$, 数值上，有$v_\pi(s) \leq q_\pi(s,\pi’(s)) \leq v_{\pi’}(s)$
后者可以通过期望式展开成序列形式为：$\leq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s] \leq \mathbb{E}<em>{\pi’}[R_t+1 + \gamma q</em>\pi(S_{t+1}, \pi’(S_{t+1}| S_t = s))]$
TODO: 详细证明</p>

<p>策略优化算法如下：</p>

<pre><code>init V[s] in R and pi(s) in A(s) arbitrarily, for all s in S

# policy evaluation
Repeat 
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := sum_s',r(p(s', r|s, a)[r + gamma * V[s']])
  until delta &lt; theta
  
# policy improvement
Repeat
  policy-stable := true
  for each s in S:
    a := pi(s)
    pi(s) := argmax_a sum_s,r(p(s', r|s, a)[r + gamma V[s']])
  
    if a != pi(s) then policy-stable := false
  if policy-stable == true then stop; else goto evaluation
</code></pre>

<h2 id="价值迭代">价值迭代</h2>

<p>价值迭代与策略迭代的不同在于：策略迭代每次先进行evaluation,然后根据evaluation的结果选择动作，而价值迭代直接计算每一个动作的期望，根据期望来选取动作。</p>

<p>算法如下:</p>

<pre><code>init array V abitrarily

Repeat
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := max_a sum_s,r(p(s', r| s, a)[r + gamma * V(s')])
    delta := max(delta, |v - V[s])
  until delta &lt; theta
  
  output deterministic policy pi, where
    pi(s) = argmax_a sum_s,r(p(s', r|s, a)[r + gamma*V[s']])
    
</code></pre>

<h1 id="monte-carlo-methods蒙特卡洛法">Monte-Carlo Methods蒙特卡洛法</h1>

<p>动态规划成立的前提是，我们知道环境动态函数$p$，而蒙特卡洛法是为了解决在没有环境动态函数的情况下进行强化学习的问题的统计学方法。</p>

<p>蒙特卡洛法是一种通过经验学习价值函数的方法，其中的经验有两种：</p>
<ol>
  <li>实际经验，从环境中真实学习的经验。</li>
  <li>模拟经验，使用一个模型来近似真实的环境</li>
</ol>

<p>在蒙特卡洛法中，价值$v_\pi$被定义成对回报的采样的平均数。</p>

<script type="math/tex; mode=display">v_\pi(s) \circeq \mathbb{E}[\sum_{k=0}^{T-1}\gamma^kR_{k+1}] \sim \frac{1}{\Epsilon(s)} \sum_{t_i \in \Epsilon(s)}\sum_{k=t_i}^{\gamma^{k-1}R^i_{k+1}}</script>

<h2 id="蒙特卡洛估计">蒙特卡洛估计</h2>

<p>蒙特卡洛有两种计算方法，当每个状态，动作的访问次数趋于无穷时，它们是等价的:</p>
<ol>
  <li>first-visit MC: 只考虑每个episode第一次访问到的(S, A) 对</li>
  <li>every-visit MC: 对所有的(S, A)对进行采样</li>
</ol>

<p>我们这里给出first-visit 的算法：</p>

<pre><code>Init:
  pi := policy to be evaluated
  V := arbitrarily init
  Returns[s] := [] for all s in S

Repeat forever:
  Generate an episode using pi
  for each state s in episode:
    G := return following first occurence of s
    Append G to Returns[s]
    V[s] := average(Return[s])
</code></pre>

<p>蒙特卡洛法同样可以对动作价值进行估计：
<script type="math/tex">q_\pi(s, a) \circeq \mathbb{E}[G_t | S_t = s, A_t = a]</script></p>

<h2 id="mc控制">MC控制</h2>

<p>采取贪婪策略进行动作选择满足策略优化定律，我们假设MC的迭代是无限的，给出蒙特卡洛控制算法：</p>

<pre><code>Init 
  for all s in S, a in A:
    Q(s, a) := arbitrarily
    pi(s) := arbitrarily
    Returns(s, a) := []

Repeat forever:
  Choose S_0 in S, A_0 in A[S_0] as start point s.t all pairs have prob &gt; 0
  Generate an episode according to pi
  For (s, a) in episode:
    G := return of s, a
    Append G to Returns(s, a)
    Q(s, a) := average(Returns(s, a))
  For each s in episode:
    pi[s] &lt;- argmax_a Q(s, a]
</code></pre>

<p>单纯的贪婪会使得更新参数变得很慢–很可能陷入某个局部最优然后不断强化，忽视探索其他动作，我们通常使用e-soft 策略来保证探索。e-greedy不改变期望。</p>

<h2 id="off-policy-蒙特卡洛">Off-policy 蒙特卡洛</h2>

<p>estimate的时候的策略$$ 与目标策略$pi$ 不完全相等时，我们称该方法为off-policy方法。</p>

<p>使用蒙特卡洛法时，对于off-policy方法，我们的estimate期望将不基于目标Policy, 而是基于我们的估计策略的policy.
在这种情况下，我们可以将整个采样过程看做一个重要性采样，采样的分布是基于估计策略$u$的分布。
我们需要用<strong>重要性系数</strong>来修正期望：</p>

<script type="math/tex; mode=display">\rho_{t:T} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{u(A_k)|S_k}</script>

<p>在重要性系数修正下：</p>

<script type="math/tex; mode=display">\mathbb{E}[\rho_{t:T}G_t|S_t = s] = v_\pi(s)</script>

<p>由于重要性采样会导致方差存在上升至无限大的可能性(将重要性采样看做是在某一区间特别密集地取值)，我们还需要对采样的平均长度进行修正：</p>

<script type="math/tex; mode=display">\eta^{-1} = \sum_{t_i in \Epsilon(s)}\rho_{t:T}</script>

<h1 id="时间差分算法temporal-difference">时间差分算法(Temporal-Difference)</h1>

<p>最基本的时间差分算法(TD(0))可以看做每次仅仅往前方看一步进行evaluation。由于这种情况下我们没有办法获得整个序列的回报，我们用当前估计的期望来作为我们的Target。</p>

<h2 id="td-evaluation">TD Evaluation</h2>

<p>最基础的TD Evaluation 算法(TD(0))如下，可以看出同MC方法相比，TD(0)最重要的改变是将更新公式中的累计回报换成了当前的奖励信号与当前估计的下一状态的期望之和:</p>

<pre><code>input policy pi
step size 0 &lt; alpha &lt;= 1
init V[s] for all s in S, arbitrarily except V(terminal) = 0

for each episode:
  init S
  for each step in episode:
    A := action from pi(S)
    take action, observe R, S'
    V[s] := V[s] + alpha[R + gamma * V[S'] - V[S]]
    S := S'
  until S is terminal
</code></pre>

<p>TD算法的好处：首先，每次都要进行更新，避免了蒙特卡洛法需要迭代完整的一个episode再进行更新的问题，其次，需要的计算力与空间更少</p>

<h2 id="td-control">TD Control</h2>
<p>根据是否on-policy, 我们可以将TD Control 分成两种算法:</p>
<ol>
  <li>SARSA: on-policy control</li>
  <li>Q-learning: off-policy control</li>
</ol>

<h3 id="sarsa">SARSA</h3>

<p>我们给出SARSA算法如下</p>

<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0

Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S'
    Choose A' fomr S' using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * Q[S', A'] - Q[S, A]]
    S := S'
    A := A
  Until S is terminal
</code></pre>

<p>SARSA有一种变种：不使用<code>Q[S', A]</code> 来进行更新，而是使用下一步状态的期望<code>sum_a (pi(S')Q(S', a))</code>来进行更新，其思路主要是通过期望计算来降低方差，从而提升学习效率。</p>

<h3 id="q-learning">Q-learning</h3>

<p>Q-learning 使用 算法如下：</p>

<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0


Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S'
    Choose A' fomr S' using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * max_a (Q[S']) - Q[S, A]]
    S := S'
  Until S is terminal
</code></pre>
<p>注意，同MC Off-policy相比，这个方法不需要重要性系数。其原因是动作a此处是确定的(argmax(Q[S]))，而非随机变量。</p>

<h2 id="n-step-td">N-Step TD</h2>

<p>N-Step TD 是在单步TD与MC方法中间的桥梁：</p>

<script type="math/tex; mode=display">G_{t:t+n} = \sum_{k=1}^{n}\gamma^{k-1}R_{t+k} + \gamma^n V_{t+n-1}(S_{t+n})</script>

<p>在n-step TD的情况下，我们需要使用重要性系数来修正我们的off-policy算法的价值估计。</p>

<script type="math/tex; mode=display">Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha \rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]

\rho_{t:h} = \prod_{k=t}^{\min(h,T-1)} \frac{\pi(A_k|S_k)}{u(A_k|S_k)}</script>

 -->
    
        <p>强化学习第二部分</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-04-27T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/">April 27, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/" rel="bookmark" title="强化学习导论（一）" itemprop="url">强化学习导论（一）</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用</p>

<h1 id="什么是强化学习">什么是强化学习</h1>

<blockquote>
  <p>通过与环境的持续交互学习，从而解决序列性的决策问题。</p>
</blockquote>

<p>强化学习是机器学习的一个分支，其特点为：</p>
<ol>
  <li>没有监督数据，只有奖励信号</li>
  <li>奖励信号不一定是实时的</li>
  <li>行为与环境交互 影响数据</li>
  <li>时间是一个重要因素 &lt;- ?</li>
</ol>

<p>我们定义智能体(agent) 作为强化学习的主体，其能通过动作(action)与环境(environment)交互，从而获取奖励信号(reward)，或者说反馈。
智能体在环境中进行序列性的决策，从而提高累计奖励(accumulate reward)</p>

<h2 id="强化学习的一些挑战">强化学习的一些挑战</h2>

<ol>
  <li>环境未知：有的时候我们无法解析地，或者无法有足够的数据来对环境的状态与奖励进行建模</li>
  <li>探索-采集(exploration-exploitation) 问题：我们需要平衡探索（通过交互获取更多的环境信息）与采集（e.g.贪婪获取更优的奖励）</li>
  <li>延迟奖励：有些奖励同历史动作相关联</li>
</ol>

<h1 id="多臂赌博机-multi-armed-bandit-problem">多臂赌博机 Multi-Armed bandit problem</h1>

<p>多臂赌博机问题是一个最基本的强化学习问题：给定$k$个赌博机，每个时刻$t$可以选择一个赌博机进行操作，从而获取一个标量奖励。每一个赌博机的奖赏分布都是独立的不同分布。
获取的奖励$R_t$是一个随机变量。我们定义$q_{*}(a)$为进行动作$a$的奖励期望：</p>

<script type="math/tex; mode=display">q_{*} = \mathbb{E}[R_t|A_t = a]</script>

<p>为了求得该期望我们可以进行动作价值估计: $Q_t(a) = q_{*}(a)$。最基本的估计使用如下公式:</p>

<script type="math/tex; mode=display">Q_t(a) = \frac{1}{N_t(a)}\sum_{\tau=1}^{t-1}R_{\tau}[A_{\tau}= a]_1</script>

<p>即选取动作$a$的时候，采样的相对应的奖励的平均值。我们可以将$Q$写成递归形式来方便之后的讨论：</p>

<script type="math/tex; mode=display">Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]</script>

<p>根据我们的估计期望$Q$，我们有贪婪算法$A_t^{*} = \arg\max_a Q_t(a)$来选取最佳的动作。根据动作是否贪婪，我们可以将动作分成两部分：</p>
<ol>
  <li>贪婪动作-&gt;采集</li>
  <li>非贪婪动作-&gt; 探索
    <h2 id="epsilon-greedy">$\epsilon$-greedy</h2>
    <p>为了平衡搜索与采集，我们给定一个探索概率$\epsilon$, 即每次选取动作的时候我们有概率$\epsilon$随机选取概率，反之则采取贪婪算法。易得在该条件下，我们有$1-\epsilon + \frac{\epsilon}{|\mathcal{A}|}$的概率来选择贪婪动作。$\epsilon$ 用于调整探索与采集的平衡。</p>
  </li>
</ol>

<h2 id="强化学习的一般更新规则">强化学习的一般更新规则</h2>

<p>根据上述的递归形式，我们不假证明地给出一般的更新规则：</p>

<p>NewEstimate &lt;- OldEstimate + StepSize[Target - OldEstimate]</p>

<p>其中，Target并不固定为单纯的奖励信号。</p>

<h2 id="多臂赌博机算法">多臂赌博机算法</h2>
<h3 id="e-greedy-单多臂赌博机算法">e-greedy 单多臂赌博机算法</h3>
<pre><code>init, for a = 1 to k:
  Q(a) &lt;- 0
  N(a) &lt;- 0
loop forever:
  A &lt;- 1. argmax_a Q(a) 1 - $\epsilon$
       2. random a      $\epsilon$
  R &lt;- bandit(A)
  N(A) &lt;- N(A) + 1
  Q(A) &lt;- Q(A) + \frac{1}{N(A)}[R - Q(A)]
`****
### 非平稳过程
我们之前假设动作价值是不变的，但是在实际中，动作价值可能会随着时间的改变而改变(non-stationary)。在这种情况下，我们不能采样求平均，而是需要用step-size parameter 来控制取一段时间的平均
$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n], \alpha \in (0, 1]
$$

### UCB
上确界动作选取(upper confidence bound, UCB法不对动作价值进行估计，而是估计动作价值的**上确界**来进行动作选取。该方法的好处是，将不确定性也一并纳入估计。
上确界的动作选取法如下：

</code></pre>
<p>A_t = 1. a if N_t(a) = 0,
      2. argmax[Q_t(a) + c Sqrt(log(t)/ N_t(a))]
``**
其中，平凡根项是对不确定性或者说方差的一个度量。c 是一个可控制的常量，用于控制不确定性影响的大小。</p>

<p>UCB 一般而言有更好的性能，但是对于非平稳过程的处理不像e-greedy那么简单。</p>

<h3 id="gradient-bandit">Gradient bandit</h3>

<p>梯度法是一种不通过直接估计动作价值$Q$，而是直接优化动作选取的策略(policy)的强化学习方法。</p>

<p>我们定义$\pi_t(a)$ 为时刻$t$的动作选取策略。$\pi_t(a)$是关于当前状态动作选取概率的分布，我们可以用随机梯度上升法来优化（前提是：策略是一个可微分的函数）。</p>

<p>定义策略为softmax函数:</p>

<script type="math/tex; mode=display">\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}</script>

<p>其中$H_t(a)$ 定义为对动作的偏好度(preference)，从而影响动作的概率。
根据softmax函数的导数我们有:</p>

<script type="math/tex; mode=display">H_{t+1}(a) = H_t(a) + \alpha(R_t - avg R)([a = A_t] - \pi_t(a))</script>

<h1 id="markov决策过程">Markov决策过程</h1>

<p>我们把交互的环境看作是一个马尔科夫链：时刻$t+1$的状态与奖励仅与前一个时刻$t$的状态与采取的动作有关。据此定义马尔科夫决策过程(Markov Decision Process, MDP):</p>
<ol>
  <li>状态空间$\mathcal{S}$</li>
  <li>动作空间$\mathcal{A}$</li>
  <li>奖励空间$\mathcal{R}$</li>
</ol>

<script type="math/tex; mode=display">Pr\{S_{t+1}, R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1},...S_0, A_0\} = Pr\{S_{t+1}, R_{t+1} | S_t, A_t\}</script>

<p>MDP是有限的当且仅当$\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$ 是有限的。</p>

<h2 id="环境动态">环境动态</h2>

<p>我们定义函数$p:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S} \times \mathcal{R}$为MDP的环境动态(Environment Dynamic)。这个函数实际上定义为状态$s’$, 奖励$r$在给定状态$s$, 采取的动作$a$的条件概率分布，即MDP的状态之间是如何转化的。
<script type="math/tex">p(s', r|s, a) = Pr{S_{t+1}=s', R_{t+1}=r | S_t = s, A_t = a}</script></p>

<p>根据动态函数p, 奖励$r$的边缘概率即为状态$s’$的概率分布
<script type="math/tex">p(s'|s, a) = \sum_{r\in \mathcal{R}} p(s', r|s, a)</script></p>

<p>我们定义函数$r:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R}$ 为给定状态$s$, 动作$a$下的奖励期望:</p>

<script type="math/tex; mode=display">r(s, a) = \mathcal{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r\sum_{s'\in S} p(s', r| s, a)</script>

<p>MDP可以被表示成一个有限状态自动机， 见 Sutton书Example 3.3</p>

<h2 id="目标与奖励">目标与奖励</h2>

<p>如前面所述，强化学习的目标实际上是尽可能多的提升累计奖励(cumulative return)。这个目标建立在<strong>奖励假设</strong> (reward hypothesis)上：</p>

<blockquote>
  <p>强化学习目标是最大化标量奖励信号的累计期望</p>
</blockquote>

<p>根据这个假设，我们认为奖励信号实际上定义了我们的目标。奖励信号不说明如何实现目标，但是如果奖励信号设计的好，我们的学习将会提速。
奖励信号与状态空间的设计都被认为是RL中的“工程”部分。</p>

<h2 id="回报">回报</h2>

<p>我们定义 <strong>回报</strong> (return)为奖励信号序列$R_{t}, R_{t+1}, R_{t+2}…$的一个函数，通过最大化该函数来实现我们的目标。最简单的回报函数就是线性加和：</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + ... + R_{T} = R_{t+1} + G_{t+1}</script>

<p>其中$T$是中止时间。以上的定义在有限步骤的情况下是成立的，但是在连续(非停止)情况下，我们可以使用一个衰减概率来控制我们求和的范围:
<script type="math/tex">G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+1+k}</script></p>

<p>当$\gamma &lt; 1$, 求和有上界:</p>

<script type="math/tex; mode=display">\sum_{k=0}^{\infty} \gamma^k R_{t+1+k} \leq r_{\max} \sum_{k=0}^{\infty}\gamma^k = r_{\max} \frac{1}{1-\gamma}</script>

<h2 id="价值函数">价值函数</h2>

<p>给定策略$\pi$, 我们可以计算在该策略下，每一个状态的价值$v_{\pi}(s)$:
<script type="math/tex">v_{\pi}(s) \circeq \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| S_t = s]</script>
同理，我们可以计算给定状态的动作价值$q_\pi$:
<script type="math/tex">q_{\pi}(s, a) \circeq \mathbb{E}_{\pi}[G_t| S_t = s, A_t = a] = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a]</script></p>

<p>(PPT此处有一个直接求序列空间价值函数的东西，感觉没什么用就不写了，基本就是对每一个可能序列的概率进行求和，一个序列的概率是p函数与$\pi$函数的累乘)</p>

<h2 id="bellman方程">Bellman方程</h2>

<p>根据Markov性质，下一时刻的状态-奖励对由且仅由这一时刻的状态与采取的动作决定，也就是说我们可以递归地更新我们的价值函数。我们将价值函数写成递归的形式:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
v_\pi(s) & \circeq \mathbb{E}_\pi[G_t | S_t = s] \\
         & = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma \mathbb{E}_\pi[G_{t+1} | S_{t+1}=s']]\\
         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s') ]
\end{aligned} %]]></script>

<p>同理，我们可以写出动作价值函数的递归形式:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
q_\pi(s, a) & \circeq \mathbb{E}_\pi[G_t | S_t = s, A_t=a] \\
            & = \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')]
\end{aligned} %]]></script>

<p>以上两个函数被称为贝尔曼方程(bellman equation)</p>

<h3 id="最优策略贝尔曼方程">最优策略贝尔曼方程</h3>

<p>我们用价值函数(期望)比较两个策略的好坏：当且仅当策略$\pi$每一个状态的期望价值都不低于另一个策略$\pi’$时，我们可以认为$\pi$有着更优的表现。如果存在一个策略，其对于所有可能的策略都是更优的，我们称该策略为最优策略(optimal policy)</p>

<p>一个策略$\pi$是最优的，当：</p>

<ol>
  <li>$v_\pi(s) = v_{*}(s) = \max_{\pi’}v_{\pi’}(s)$</li>
  <li>$q_\pi(s, a) = q_{*}(s, a) = \max_{\pi’} q_{\pi’}(s, a)$</li>
</ol>

<p>在最优策略下，我们可以用最优的动作来替代策略下的条件分布：</p>

<script type="math/tex; mode=display">v_{*}(s) = \max_{a} \sum_{s', r} p(s', r|s, a)[r + \gamma v_{*}(s')]</script>

<script type="math/tex; mode=display">q_{*}(s, a) = \sum_{s', r} p(s', r|s, a)[r + \gamma \max_{a'} q_{*}(s', a)]</script>

<p>对于有限的MDP与non-terminate episode而言，每个策略$\pi$都会遍历状态空间，空间中的每个状态理想情况下都会被访问无限次。
我们定义时间趋于无穷时，状态的分布为平稳状态分布$P_\pi(s) = Pr{S_t = s, |A_0, …, A_{t-1} \sim \pi }$。 
此时，我们使用平均奖励(average reward)来评价策略的价值:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}

r(\pi) &= \lim_{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}[R_t | S_0, A_0, ...]

       &= \sum_{s} P_{\pi}(s)\sum_a \pi(a|s) \sum_{s', r} p(s', r\|s, a) r

\end{aligned} %]]></script>

<p>最大化在平稳状态分布下的回报等同于最大化平均奖励。</p>

<h1 id="ref">Ref</h1>

<ol>
  <li>https://zhuanlan.zhihu.com/p/28084904</li>
  <li>Sutton, An Introduction To Reinforcement Learning</li>
</ol>
 -->
    
        <p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/" title="Machine Learning: Linear Regression">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60323285_p0.jpg" alt="Machine Learning: Linear Regression">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-10-02T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/">October 02, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/" rel="bookmark" title="Machine Learning: Linear Regression" itemprop="url">Machine Learning: Linear Regression</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>Linear regression is one of the oldest method for statistics, also regarded as an guide algorithm for machine learning.</p>

<p>In this article, I will follow what I’ve learned in MLPR, with content talking about the algorithm itself, regularization, logistics regression (sigmoid function), Raditional Basis Function(RBF). Besides, I’ll talk about some topic that we will come through all algoritms: overfitting, measure of performance.</p>

<h3 id="music">music</h3>
<p><del>Finally I got somewhere to make audio for the blog ;P</del></p>
<audio controls=""><source src="http://mp3.flash127.com/music/47306.mp3" type="audio/mpeg" />您的浏览器不支持音频格式</audio>

<p><em>脱獄　Datsugoku</em></p>

<p><code>Producer</code>: Neru(押入p)</p>

<p><code>Origin Vocaloid</code>: 鏡音りん Kagamine Rin</p>

<p><code>Singer</code>: まふまふ　mafumafu</p>

<p style="color: orange">エンジンがヒートして　機体がどうしたって</p>

<p style="color: orange">気にもしない程に　トリップしてしまう大空は偉大さ</p>

<h1 id="introduction">Introduction</h1>
<p>Linear regression is a typical regression task and supervised learning method. 
According to Andrew Ng, there we have 2 definitions:</p>

<ol>
  <li><strong>Supervised Learning</strong>: Give the dataset of right answers, the system is used to make more “correct” answers</li>
  <li><strong>Regression</strong>: Predict <em>continuous</em> value output</li>
</ol>

<p>Example: Given a set of past house price, trying to predict the price in future.</p>

<h2 id="main-task">Main task</h2>

<p>Linear regression is a system that predict the relations, and assume them as linear, with input data $\mathbf{x}$ and the result data $y$. Here, $\mathbf{x}$ is a row vector: $\mathbf{x}_d = [ x_1, x_2, x_3 … x_d]^T$.
We have a learning function $f$ which gives a predicted value $f(\mathbf{x})$. Since we predict that $\mathbf{x}$ and $\mathbf{y}$ has linear relationship, we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
f(\mathbf{x}) &= (w_1 x_1 + w_2 x_2 + ... + w_d x_d) + b \\

&= \mathbf{w}^T\mathbf{x} + b \\
&= [\mathbf{w} \ b]^T [\mathbf{x} \ 1] \\
&= \mathbf{w}'^T \mathbf{x}'

\end{aligned} %]]></script>

<p>Consider the input dataset of size $n$, we could re-organize our input and output data into more general matrix form:</p>

<script type="math/tex; mode=display">\mathbf{y} = [y_1\ y_2\ ...\ y_n]^T \\
 
 \mathbf{x} = [\mathbf{x}_1^T\ \mathbf{x}_2^T\ ...\ \mathbf{x}_n^T]^T</script>

<p>we can extend our predict function into:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}

\mathbf{f} &= \begin{bmatrix}

                  f(\mathbf{x}_1) \\
                  f(\mathbf{x}_2) \\
                  ... \\
                  f(\mathbf{x}_n)
                  \end{bmatrix}
              &= \mathbf{w}^T\mathbf{x}

\end{aligned} %]]></script>

<p>Where $\mathbf{w}$ is a $1 * (m+1)$ matrix, $\mathbf{x}$ is a $(m +1) * n$ matrix, $m$ is the number of features, $n$ is the number of dataset.</p>

<h2 id="cost-function">Cost function</h2>

<p>The task is to find a “best” linear function that could predict the future value. The question here is: how can we found the “best” linear function?
To solve this problem, we need to define a cost function (or error function) and minimize it. Here the cost is defined as the bias between predicted value and target value. It is very natural that we use the Euclid distance between predition and target value as the “cost” or “error” we want:</p>

<script type="math/tex; mode=display">Cost(n) = | y^n - f(\mathbf{x}^(n)) |^2</script>

<p>If we look at the whole feature space, we could extend the function into:</p>

<script type="math/tex; mode=display">Cost(\mathbf{x}) = (\mathbf{y} - f(\mathbf{x}))^T(\mathbf{y} - f(\mathbf{x}))</script>

<p>By simple linear algebra, we could easily tell that the square of a column vector, as matrix, is the transpose of vetor times itself.</p>

<p>Consider a dataset with $N$ data, we could extend the cost function into a more general mode:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  Cost(\mathbf{x}) &= \sum_{i=1}^{n} |y^{i} - f(\mathbf{x}_{i})|^2 \\
                   &= (\mathbf{y} - \mathbf{f})^T(\mathbf{y} - \mathbf{f})
  

\end{align} %]]></script>

<h2 id="least-square-approximation">Least Square Approximation</h2>

<p>To be continued</p>

<h1 id="reference">Reference</h1>

<ol>
  <li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning online course</a> , Andrew Ng.</li>
</ol>

 -->
    
        <p>Linear regression is one of the oldest method for statistics, also regarded as an guide algorithm for machine learning.</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/" title="Machine Learning series -- Introduction">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/39496577_p0.jpg" alt="Machine Learning series -- Introduction">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-10-01T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/">October 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/" rel="bookmark" title="Machine Learning series -- Introduction" itemprop="url">Machine Learning series -- Introduction</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>

<p>Gintama is one of my favorite anime/manga, which was ended in <em>Jump</em>. The ending is soooooo… Gorilla. Thanks for accompany for years, although later I did not watch it much.</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/q5iRnqKWlJ6dqKQ.jpg" /></p>

<p>↑Above are some useless words.</p>

<p>↓Below are real blog article.</p>

<p>This series of article is aiming to review/preview the mathematic which is necessary for machine learning courses, including MLPR class in UoE and Machine Learning by Andrew Ng, which is held in Coursera.</p>

<h1 id="motivation">Motivation</h1>

<p>I’ve been to Edinburgh for almost one month. As a MSc AI student, I’ve chosen a class called Machine Learning and Pattern Recognization.
Math reasoning and application in the class are quite hard for me, hence I need to make more effort to recall and study necessary mathematic tools.
As for the reason why I did not gather enough math ability for machine learning, well, one of the reason was I spent whole year working for a start-up company.
The works there was mainly aims on application and dealing with lots of – sometimes even silly – requirements and management problems.
The other reason, that I <strong>MUST</strong> critisize, is we did not have <strong>enough</strong> mathematic training during undergraduate period.
As computer student, we don’t have class on linear algebra, probability, and we only have some very basic calculus which was taught in first year. 
It is really ridiculus to those student graduated from chinese university education background. 
Amoung my classmates there are lot of people came from mathematic/physics background. So it is really challenging to catch up the fast-pace classes.</p>

<p>Writing blogs was one of my best learning approach, that’s why I write this series of article. On the otherhand, it is also a good practice of time management and English writing. The blog is also inspired by <a href="https://tony4ai.com">tony4ai</a>, which helped me a lot at the very beginning of this semester.</p>

<p>The mountain is there, stop half way will gain nothing. The only way to success is to reach the top.</p>

<p>As the beginning says, there is nothing can last forever, including your parents, money, room, T-shirt, me, you, and anime Gintama.</p>

<p>And also past success and pride.</p>

<h1 id="content">Content</h1>

<p>In this series I will write:</p>

<ol>
  <li>Mathematic basis of machine learning. Including probability, linear algebra, calculus, statitistics and basic algorithms. I’ll try my best to do the proving and mathematic reasoning.</li>
  <li>Machine learning methods in detail. I’ll recall what I have learned during undergraduate, and all the knowledge I gained not only here but also many other resources of machine learning. Including Coursera classes, books, blogs and many other machine learning related medias.</li>
  <li>Pratical cases of machine learning. Here will mostly concentrate on NLP stuff but if possible, also some other tasks including image processing, etc. Most  of them will contain programming.</li>
  <li>Mathematical practice for other topics, like computer graphics, if I have time.</li>
</ol>

<h1 id="machine-learning-introduction">Machine Learning Introduction</h1>

<p>To be continued.(JOJO Pose!)</p>

<h1 id="main-category">Main Category</h1>

<h2 id="math">Math</h2>

<h2 id="machine-learning">Machine learning</h2>

<p>To be continued.</p>

<h2 id="practical-cases-for-ml">Practical Cases for ML</h2>

<p>To be continued.</p>

<h2 id="mathematical-practice">Mathematical Practice</h2>

<p>To be continued.</p>

<h1 id="reference">Reference</h1>

<h1 id="music">music</h1>
<audio controls="controls" autoplay="" playsinline="" webkit-playsinline="">
  <source src="http://www.170mv.com/kw/other.web.ra01.sycdn.kuwo.cn/resource/n3/2011/06/10/418067833.mp3" type="audio/mpeg" />
</audio>

 -->
    
        <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page2">2</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page7">7</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page8">8</a>
        
      </li>
    
  </ul>
  
    <a href="http://MeowAlienOwO.github.io/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js"></script> -->
<!-- <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
     inlineMath: [['$', '$'], [ '\\(', '\\)']],
     displayMath: [['$$', '$$']]
     }});
     </script>
-->
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->





<script>
 var _config = {
     title: 'Entry',
     image: '22958285.jpg'
 }

 socialShare('.social-share', _config)
</script>

          

</body>
</html>
