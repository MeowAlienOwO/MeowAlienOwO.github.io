<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Entry &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
     <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
     </script> -->

<!-- Share.js -->
<!-- <link href="/assets/css/share.min.css" rel="stylesheet"> -->
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js"></script> -->
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Entry">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/machine learning/">
				    machine learning (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/RL/">
				    RL (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=22958285">ZERO | STAR影法師 [pixiv]</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://MeowAlienOwO.github.io/images/22958285.jpg" alt="Entry">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>喵窝[0]号机</h1>
      <h2>一个伪装成技术博客的吐槽网站。</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-04-27T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/">April 27, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/" rel="bookmark" title="强化学习导论（一）" itemprop="url">强化学习导论（一）</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用</p>

<h1 id="什么是强化学习">什么是强化学习</h1>

<blockquote>
  <p>通过与环境的持续交互学习，从而解决序列性的决策问题。</p>
</blockquote>

<p>强化学习是机器学习的一个分支，其特点为：</p>
<ol>
  <li>没有监督数据，只有奖励信号</li>
  <li>奖励信号不一定是实时的</li>
  <li>行为与环境交互 影响数据</li>
  <li>时间是一个重要因素 &lt;- ?</li>
</ol>

<p>我们定义智能体(agent) 作为强化学习的主体，其能通过动作(action)与环境(environment)交互，从而获取奖励信号(reward)，或者说反馈。
智能体在环境中进行序列性的决策，从而提高累计奖励(accumulate reward)</p>

<h2 id="强化学习的一些挑战">强化学习的一些挑战</h2>

<ol>
  <li>环境未知：有的时候我们无法解析地，或者无法有足够的数据来对环境的状态与奖励进行建模</li>
  <li>探索-采集(exploration-exploitation) 问题：我们需要平衡探索（通过交互获取更多的环境信息）与采集（e.g.贪婪获取更优的奖励）</li>
  <li>延迟奖励：有些奖励同历史动作相关联</li>
</ol>

<h1 id="多臂赌博机-multi-armed-bandit-problem">多臂赌博机 Multi-Armed bandit problem</h1>

<p>多臂赌博机问题是一个最基本的强化学习问题：给定$k$个赌博机，每个时刻$t$可以选择一个赌博机进行操作，从而获取一个标量奖励。每一个赌博机的奖赏分布都是独立的不同分布。
获取的奖励$R_t$是一个随机变量。我们定义$q_{*}(a)$为进行动作$a$的奖励期望：</p>

<script type="math/tex; mode=display">q_{*} = \mathbb{E}[R_t|A_t = a]</script>

<p>为了求得该期望我们可以进行动作价值估计: $Q_t(a) = q_{*}(a)$。最基本的估计使用如下公式:</p>

<script type="math/tex; mode=display">Q_t(a) = \frac{1}{N_t(a)}\sum_{\tau=1}^{t-1}R_{\tau}[A_{\tau}= a]_1</script>

<p>即选取动作$a$的时候，采样的相对应的奖励的平均值。我们可以将$Q$写成递归形式来方便之后的讨论：</p>

<script type="math/tex; mode=display">Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]</script>

<p>根据我们的估计期望$Q$，我们有贪婪算法$A_t^{*} = \arg\max_a Q_t(a)$来选取最佳的动作。根据动作是否贪婪，我们可以将动作分成两部分：</p>
<ol>
  <li>贪婪动作-&gt;采集</li>
  <li>非贪婪动作-&gt; 探索
    <h2 id="epsilon-greedy">$\epsilon$-greedy</h2>
    <p>为了平衡搜索与采集，我们给定一个探索概率$\epsilon$, 即每次选取动作的时候我们有概率$\epsilon$随机选取概率，反之则采取贪婪算法。易得在该条件下，我们有$1-\epsilon + \frac{\epsilon}{|\mathcal{A}|}$的概率来选择贪婪动作。$\epsilon$ 用于调整探索与采集的平衡。</p>
  </li>
</ol>

<h2 id="强化学习的一般更新规则">强化学习的一般更新规则</h2>

<p>根据上述的递归形式，我们不假证明地给出一般的更新规则：</p>

<p>NewEstimate &lt;- OldEstimate + StepSize[Target - OldEstimate]</p>

<p>其中，Target并不固定为单纯的奖励信号。</p>

<h2 id="多臂赌博机算法">多臂赌博机算法</h2>
<h3 id="e-greedy-单多臂赌博机算法">e-greedy 单多臂赌博机算法</h3>
<pre><code>init, for a = 1 to k:
  Q(a) &lt;- 0
  N(a) &lt;- 0
loop forever:
  A &lt;- 1. argmax_a Q(a) 1 - $\epsilon$
       2. random a      $\epsilon$
  R &lt;- bandit(A)
  N(A) &lt;- N(A) + 1
  Q(A) &lt;- Q(A) + \frac{1}{N(A)}[R - Q(A)]
`****
### 非平稳过程
我们之前假设动作价值是不变的，但是在实际中，动作价值可能会随着时间的改变而改变(non-stationary)。在这种情况下，我们不能采样求平均，而是需要用step-size parameter 来控制取一段时间的平均
$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n], \alpha \in (0, 1]
$$

### UCB
上确界动作选取(upper confidence bound, UCB法不对动作价值进行估计，而是估计动作价值的**上确界**来进行动作选取。该方法的好处是，将不确定性也一并纳入估计。
上确界的动作选取法如下：

</code></pre>
<p>A_t = 1. a if N_t(a) = 0,
      2. argmax[Q_t(a) + c Sqrt(log(t)/ N_t(a))]
``**
其中，平凡根项是对不确定性或者说方差的一个度量。c 是一个可控制的常量，用于控制不确定性影响的大小。</p>

<p>UCB 一般而言有更好的性能，但是对于非平稳过程的处理不像e-greedy那么简单。</p>

<h3 id="gradient-bandit">Gradient bandit</h3>

<p>梯度法是一种不通过直接估计动作价值$Q$，而是直接优化动作选取的策略(policy)的强化学习方法。</p>

<p>我们定义$\pi_t(a)$ 为时刻$t$的动作选取策略。$\pi_t(a)$是关于当前状态动作选取概率的分布，我们可以用随机梯度上升法来优化（前提是：策略是一个可微分的函数）。</p>

<p>定义策略为softmax函数:</p>

<script type="math/tex; mode=display">\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}</script>

<p>其中$H_t(a)$ 定义为对动作的偏好度(preference)，从而影响动作的概率。
根据softmax函数的导数我们有:</p>

<script type="math/tex; mode=display">H_{t+1}(a) = H_t(a) + \alpha(R_t - avg R)([a = A_t] - \pi_t(a))</script>

<h1 id="markov决策过程">Markov决策过程</h1>

<p>我们把交互的环境看作是一个马尔科夫链：时刻$t+1$的状态与奖励仅与前一个时刻$t$的状态与采取的动作有关。据此定义马尔科夫决策过程(Markov Decision Process, MDP):</p>
<ol>
  <li>状态空间$\mathcal{S}$</li>
  <li>动作空间$\mathcal{A}$</li>
  <li>奖励空间$\mathcal{R}$</li>
</ol>

<script type="math/tex; mode=display">Pr\{S_{t+1}, R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1},...S_0, A_0\} = Pr\{S_{t+1}, R_{t+1} | S_t, A_t\}</script>

<p>MDP是有限的当且仅当$\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$ 是有限的。</p>

<h2 id="环境动态">环境动态</h2>

<p>我们定义函数$p:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S} \times \mathcal{R}$为MDP的环境动态(Environment Dynamic)。这个函数实际上定义为状态$s’$, 奖励$r$在给定状态$s$, 采取的动作$a$的条件概率分布，即MDP的状态之间是如何转化的。
<script type="math/tex">p(s', r|s, a) = Pr{S_{t+1}=s', R_{t+1}=r | S_t = s, A_t = a}</script></p>

<p>根据动态函数p, 奖励$r$的边缘概率即为状态$s’$的概率分布
<script type="math/tex">p(s'|s, a) = \sum_{r\in \mathcal{R}} p(s', r|s, a)</script></p>

<p>我们定义函数$r:: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R}$ 为给定状态$s$, 动作$a$下的奖励期望:</p>

<script type="math/tex; mode=display">r(s, a) = \mathcal{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r\sum_{s'\in S} p(s', r| s, a)</script>

<p>MDP可以被表示成一个有限状态自动机， 见 Sutton书Example 3.3</p>

<h2 id="目标与奖励">目标与奖励</h2>

<p>如前面所述，强化学习的目标实际上是尽可能多的提升累计奖励(cumulative return)。这个目标建立在<strong>奖励假设</strong> (reward hypothesis)上：</p>

<blockquote>
  <p>强化学习目标是最大化标量奖励信号的累计期望</p>
</blockquote>

<p>根据这个假设，我们认为奖励信号实际上定义了我们的目标。奖励信号不说明如何实现目标，但是如果奖励信号设计的好，我们的学习将会提速。
奖励信号与状态空间的设计都被认为是RL中的“工程”部分。</p>

<h2 id="回报">回报</h2>

<p>我们定义 <strong>回报</strong> (return)为奖励信号序列$R_{t}, R_{t+1}, R_{t+2}…$的一个函数，通过最大化该函数来实现我们的目标。最简单的回报函数就是线性加和：</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + ... + R_{T} = R_{t+1} + G_{t+1}</script>

<p>其中$T$是中止时间。以上的定义在有限步骤的情况下是成立的，但是在连续(非停止)情况下，我们可以使用一个衰减概率来控制我们求和的范围:
<script type="math/tex">G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+1+k}</script></p>

<p>当$\gamma &lt; 1$, 求和有上界:</p>

<script type="math/tex; mode=display">\sum_{k=0}^{\infty} \gamma^k R_{t+1+k} \leq r_{\max} \sum_{k=0}^{\infty}\gamma^k = r_{\max} \frac{1}{1-\gamma}</script>

<h2 id="价值函数">价值函数</h2>

<p>给定策略$\pi$, 我们可以计算在该策略下，每一个状态的价值$v_{\pi}(s)$:
<script type="math/tex">v_{\pi}(s) \circeq \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| S_t = s]</script>
同理，我们可以计算给定状态的动作价值$q_\pi$:
<script type="math/tex">q_{\pi}(s, a) \circeq \mathbb{E}_{\pi}[G_t| S_t = s, A_t = a] = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a]</script></p>

<p>(PPT此处有一个直接求序列空间价值函数的东西，感觉没什么用就不写了，基本就是对每一个可能序列的概率进行求和，一个序列的概率是p函数与$\pi$函数的累乘)</p>

<h2 id="bellman方程">Bellman方程</h2>

<p>根据Markov性质，下一时刻的状态-奖励对由且仅由这一时刻的状态与采取的动作决定，也就是说我们可以递归地更新我们的价值函数。我们将价值函数写成递归的形式:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
v_\pi(s) & \circeq \mathbb{E}_\pi[G_t | S_t = s] \\
         & = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma \mathbb{E}_\pi[G_{t+1} | S_{t+1}=s']]\\
         & = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s') ]
\end{aligned} %]]></script>

<p>同理，我们可以写出动作价值函数的递归形式:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
q_\pi(s, a) & \circeq \mathbb{E}_\pi[G_t | S_t = s, A_t=a] \\
            & = \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')]
\end{aligned} %]]></script>

<p>以上两个函数被称为贝尔曼方程(bellman equation)</p>

<h3 id="最优策略贝尔曼方程">最优策略贝尔曼方程</h3>

<p>我们用价值函数(期望)比较两个策略的好坏：当且仅当策略$\pi$每一个状态的期望价值都不低于另一个策略$\pi’$时，我们可以认为$\pi$有着更优的表现。如果存在一个策略，其对于所有可能的策略都是更优的，我们称该策略为最优策略(optimal policy)</p>

<p>一个策略$\pi$是最优的，当：</p>

<ol>
  <li>$v_\pi(s) = v_{*}(s) = \max_{\pi’}v_{\pi’}(s)$</li>
  <li>$q_\pi(s, a) = q_{*}(s, a) = \max_{\pi’} q_{\pi’}(s, a)$</li>
</ol>

<p>在最优策略下，我们可以用最优的动作来替代策略下的条件分布：</p>

<ol>
  <li>
    <script type="math/tex; mode=display">v_{*}(s) = \max\_{a} \sum\_{s', r} p(s', r|s, a)[r + \gamma v_{*}(s')]</script>
  </li>
  <li>
    <script type="math/tex; mode=display">q_{*}(s, a) = \sum\_{s', r} p(s', r|s, a)[r + \gamma \max_{a'} q_{*}(s', a)]</script>
  </li>
</ol>

<p>对于有限的MDP与non-terminate episode而言，每个策略$\pi$都会遍历状态空间，空间中的每个状态理想情况下都会被访问无限次。
我们定义时间趋于无穷时，状态的分布为平稳状态分布$P_\pi(s) = Pr{S_t = s, |A_0, …, A_{t-1} \sim \pi }$。 
此时，我们使用平均奖励(average reward)来评价策略的价值:</p>

<script type="math/tex; mode=display">% <![CDATA[
\being{aligned}

r(\pi) &= \lim_{h\rightarrow \infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}[R_t | S_0, A_0, ...]
       &= \sum_{s} P_{\pi}(s)\sum_a \pi(a|s) \sum_{s', r}p(s', r|s, a)r

\end{aligned} %]]></script>

<p>最大化在平稳状态分布下的回报等同于最大化平均奖励。</p>

<h1 id="ref">Ref</h1>

<ol>
  <li>https://zhuanlan.zhihu.com/p/28084904</li>
  <li>Sutton, An Introduction To Reinforcement Learning</li>
</ol>
 -->
    
        <p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/" title="Machine Learning: Linear Regression">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60323285_p0.jpg" alt="Machine Learning: Linear Regression">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-10-02T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/">October 02, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/" rel="bookmark" title="Machine Learning: Linear Regression" itemprop="url">Machine Learning: Linear Regression</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>Linear regression is one of the oldest method for statistics, also regarded as an guide algorithm for machine learning.</p>

<p>In this article, I will follow what I’ve learned in MLPR, with content talking about the algorithm itself, regularization, logistics regression (sigmoid function), Raditional Basis Function(RBF). Besides, I’ll talk about some topic that we will come through all algoritms: overfitting, measure of performance.</p>

<h3 id="music">music</h3>
<p><del>Finally I got somewhere to make audio for the blog ;P</del></p>
<audio controls=""><source src="http://mp3.flash127.com/music/47306.mp3" type="audio/mpeg" />您的浏览器不支持音频格式</audio>

<p><em>脱獄　Datsugoku</em></p>

<p><code>Producer</code>: Neru(押入p)</p>

<p><code>Origin Vocaloid</code>: 鏡音りん Kagamine Rin</p>

<p><code>Singer</code>: まふまふ　mafumafu</p>

<p style="color: orange">エンジンがヒートして　機体がどうしたって</p>

<p style="color: orange">気にもしない程に　トリップしてしまう大空は偉大さ</p>

<h1 id="introduction">Introduction</h1>
<p>Linear regression is a typical regression task and supervised learning method. 
According to Andrew Ng, there we have 2 definitions:</p>

<ol>
  <li><strong>Supervised Learning</strong>: Give the dataset of right answers, the system is used to make more “correct” answers</li>
  <li><strong>Regression</strong>: Predict <em>continuous</em> value output</li>
</ol>

<p>Example: Given a set of past house price, trying to predict the price in future.</p>

<h2 id="main-task">Main task</h2>

<p>Linear regression is a system that predict the relations, and assume them as linear, with input data $\mathbf{x}$ and the result data $y$. Here, $\mathbf{x}$ is a row vector: $\mathbf{x}_d = [ x_1, x_2, x_3 … x_d]^T$.
We have a learning function $f$ which gives a predicted value $f(\mathbf{x})$. Since we predict that $\mathbf{x}$ and $\mathbf{y}$ has linear relationship, we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
f(\mathbf{x}) &= (w_1 x_1 + w_2 x_2 + ... + w_d x_d) + b \\

&= \mathbf{w}^T\mathbf{x} + b \\
&= [\mathbf{w} \ b]^T [\mathbf{x} \ 1] \\
&= \mathbf{w}'^T \mathbf{x}'

\end{aligned} %]]></script>

<p>Consider the input dataset of size $n$, we could re-organize our input and output data into more general matrix form:</p>

<script type="math/tex; mode=display">\mathbf{y} = [y_1\ y_2\ ...\ y_n]^T \\
 
 \mathbf{x} = [\mathbf{x}_1^T\ \mathbf{x}_2^T\ ...\ \mathbf{x}_n^T]^T</script>

<p>we can extend our predict function into:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}

\mathbf{f} &= \begin{bmatrix}

                  f(\mathbf{x}_1) \\
                  f(\mathbf{x}_2) \\
                  ... \\
                  f(\mathbf{x}_n)
                  \end{bmatrix}
              &= \mathbf{w}^T\mathbf{x}

\end{aligned} %]]></script>

<p>Where $\mathbf{w}$ is a $1 * (m+1)$ matrix, $\mathbf{x}$ is a $(m +1) * n$ matrix, $m$ is the number of features, $n$ is the number of dataset.</p>

<h2 id="cost-function">Cost function</h2>

<p>The task is to find a “best” linear function that could predict the future value. The question here is: how can we found the “best” linear function?
To solve this problem, we need to define a cost function (or error function) and minimize it. Here the cost is defined as the bias between predicted value and target value. It is very natural that we use the Euclid distance between predition and target value as the “cost” or “error” we want:</p>

<script type="math/tex; mode=display">Cost(n) = | y^n - f(\mathbf{x}^(n)) |^2</script>

<p>If we look at the whole feature space, we could extend the function into:</p>

<script type="math/tex; mode=display">Cost(\mathbf{x}) = (\mathbf{y} - f(\mathbf{x}))^T(\mathbf{y} - f(\mathbf{x}))</script>

<p>By simple linear algebra, we could easily tell that the square of a column vector, as matrix, is the transpose of vetor times itself.</p>

<p>Consider a dataset with $N$ data, we could extend the cost function into a more general mode:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  Cost(\mathbf{x}) &= \sum_{i=1}^{n} |y^{i} - f(\mathbf{x}_{i})|^2 \\
                   &= (\mathbf{y} - \mathbf{f})^T(\mathbf{y} - \mathbf{f})
  

\end{align} %]]></script>

<h2 id="least-square-approximation">Least Square Approximation</h2>

<p>To be continued</p>

<h1 id="reference">Reference</h1>

<ol>
  <li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning online course</a> , Andrew Ng.</li>
</ol>

 -->
    
        <p>Linear regression is one of the oldest method for statistics, also regarded as an guide algorithm for machine learning.</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/" title="Machine Learning series -- Introduction">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/39496577_p0.jpg" alt="Machine Learning series -- Introduction">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-10-01T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/">October 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/" rel="bookmark" title="Machine Learning series -- Introduction" itemprop="url">Machine Learning series -- Introduction</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>

<p>Gintama is one of my favorite anime/manga, which was ended in <em>Jump</em>. The ending is soooooo… Gorilla. Thanks for accompany for years, although later I did not watch it much.</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/q5iRnqKWlJ6dqKQ.jpg" /></p>

<p>↑Above are some useless words.</p>

<p>↓Below are real blog article.</p>

<p>This series of article is aiming to review/preview the mathematic which is necessary for machine learning courses, including MLPR class in UoE and Machine Learning by Andrew Ng, which is held in Coursera.</p>

<h1 id="motivation">Motivation</h1>

<p>I’ve been to Edinburgh for almost one month. As a MSc AI student, I’ve chosen a class called Machine Learning and Pattern Recognization.
Math reasoning and application in the class are quite hard for me, hence I need to make more effort to recall and study necessary mathematic tools.
As for the reason why I did not gather enough math ability for machine learning, well, one of the reason was I spent whole year working for a start-up company.
The works there was mainly aims on application and dealing with lots of – sometimes even silly – requirements and management problems.
The other reason, that I <strong>MUST</strong> critisize, is we did not have <strong>enough</strong> mathematic training during undergraduate period.
As computer student, we don’t have class on linear algebra, probability, and we only have some very basic calculus which was taught in first year. 
It is really ridiculus to those student graduated from chinese university education background. 
Amoung my classmates there are lot of people came from mathematic/physics background. So it is really challenging to catch up the fast-pace classes.</p>

<p>Writing blogs was one of my best learning approach, that’s why I write this series of article. On the otherhand, it is also a good practice of time management and English writing. The blog is also inspired by <a href="https://tony4ai.com">tony4ai</a>, which helped me a lot at the very beginning of this semester.</p>

<p>The mountain is there, stop half way will gain nothing. The only way to success is to reach the top.</p>

<p>As the beginning says, there is nothing can last forever, including your parents, money, room, T-shirt, me, you, and anime Gintama.</p>

<p>And also past success and pride.</p>

<h1 id="content">Content</h1>

<p>In this series I will write:</p>

<ol>
  <li>Mathematic basis of machine learning. Including probability, linear algebra, calculus, statitistics and basic algorithms. I’ll try my best to do the proving and mathematic reasoning.</li>
  <li>Machine learning methods in detail. I’ll recall what I have learned during undergraduate, and all the knowledge I gained not only here but also many other resources of machine learning. Including Coursera classes, books, blogs and many other machine learning related medias.</li>
  <li>Pratical cases of machine learning. Here will mostly concentrate on NLP stuff but if possible, also some other tasks including image processing, etc. Most  of them will contain programming.</li>
  <li>Mathematical practice for other topics, like computer graphics, if I have time.</li>
</ol>

<h1 id="machine-learning-introduction">Machine Learning Introduction</h1>

<p>To be continued.(JOJO Pose!)</p>

<h1 id="main-category">Main Category</h1>

<h2 id="math">Math</h2>

<h2 id="machine-learning">Machine learning</h2>

<p>To be continued.</p>

<h2 id="practical-cases-for-ml">Practical Cases for ML</h2>

<p>To be continued.</p>

<h2 id="mathematical-practice">Mathematical Practice</h2>

<p>To be continued.</p>

<h1 id="reference">Reference</h1>

<h1 id="music">music</h1>
<audio controls="controls" autoplay="" playsinline="" webkit-playsinline="">
  <source src="http://www.170mv.com/kw/other.web.ra01.sycdn.kuwo.cn/resource/n3/2011/06/10/418067833.mp3" type="audio/mpeg" />
</audio>

 -->
    
        <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" title="Machine Learning Review: Neural Network">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60827595.jpg" alt="Machine Learning Review: Neural Network">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-09T18:36:10+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/">January 09, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" rel="bookmark" title="Machine Learning Review: Neural Network" itemprop="url">Machine Learning Review: Neural Network</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>呼……终于到最后一篇了，也是开学学的内容。</p>

<p>今年也请多多指教。</p>

<h1 id="神经网络neural-network">神经网络（Neural Network）</h1>

<p>神经网络是一种很古老，但是到现在依然有很广泛的用途的机器学习算法。包括
现在火的深度学习，其基础也是神经网络。</p>

<p>神经网络的定义（来自西瓜书，引用T.Kohonen）：</p>

<blockquote>
  <p>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，其组织能
够模拟生物神经系统对真实世界作出的交互反应。</p>
</blockquote>

<h2 id="神经元模型neuron-model">神经元模型(Neuron Model)</h2>

<p>如上述的简单单元就是神经网络的最基本组成部分–神经元。一个神经元可以接
受若干个来自其他神经元的输入信号，这些信号通过带权重的连接进行传递，总
输入值与神经元的阈值进行比较，然后通过激活函数处理，形成神经元的输出。</p>

<p>神经元的数学形式可以表示如下：</p>

<script type="math/tex; mode=display">y = f(\sum\limits_{i=1}^{n}w_ix_i - \theta)</script>

<p>其中，$f$表示激活函数，$w_i$表示权重，$x_i$表示属性值，$\theta$表示阈
值。</p>

<p>一种比较基本的激活函数是<strong>阶跃函数</strong>（signal function），即等于或超过阈值则为1，反之为0。
但是这个函数不方便求导与计算误差，我们在实际中比较常见的函数有sigmoid
函数：</p>

<script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script>

<p>神经元具有学习功能，会根据输出值与真实值的误差对权重进行调整，从而达到
泛化的目的。表示神经元的学习方式的函数叫做<strong>训练函数</strong>(training function)</p>

<h2 id="感知机perceptron">感知机（Perceptron）</h2>

<p>感知机是一种最基本的神经网络，其包含两层神经元，输入层接受外界的信号，
传递给输出神经元。输出神经元对输入的数据作加权处理，然后与阈值
（threshold）做比较，判断神经元是否被激活。</p>

<p>感知机可以被认为是一种线性分类器，即在样本空间中寻找一个超平面，使得样
本能够分布在超平面的两端。这需要我们的样本是<strong>线性可分</strong>的。这也是我们
对应用感知机场景的一个基本假设。例如，XOR问题就不是线性可分的，因此无
法应用感知机。</p>

<p>感知机会根据输出值与目标分类值之间的误差更新权重，这就是感知机的**学习
**过程。</p>

<p>基本的感知机表示如下：</p>

<p><script type="math/tex">R = \theta + \sum\limits_{i=1}{m}w_ix_i \\
o = sign(R) = \lbrace +1; if~R > 0 \\ -1; otherwise</script>
其中，阈值$\theta$可以看做一个输入值恒为-1（或者-1）的”哑节点”（dummy
node），那么感知机的表达方式就可以统一为权重的学习。</p>

<p>感知机的训练函数为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta (d - o) x_i, i = 1,2,...,n</script>

<p>其中，$w_i$表示权重，$\eta$表示学习率。学习率是一个常数，用于控制学习
的速度。太低会导致学习过程缓慢，太高则有可能导致学习失败：感知机无法收
敛到一个比较适当的区间。$d$表示类别的值，$o$表示感知机的输出，我们也可
以将$d - o$统一成为误差。</p>

<p>我们不加证明地给出收敛性定理：</p>

<blockquote>
  <p>如果样本是线性可分的，那么感知机将会一定可以在有限的步骤内收敛到一个
解。</p>
</blockquote>

<h2 id="自适应线性神经元adaptive-linear-elements">自适应线性神经元（Adaptive Linear Elements）</h2>

<p>感知机由于采用了阶跃函数作为激发函数，容易出现难以收敛的情况。同时在
样本集不是线性可分的情况下，感知机难以找出一个恰当的近似。为此，我们引
入自适应线性神经元。</p>

<p>简单地说，自适应线性神经元取消了阶跃函数，直接以输入的加权求和（包括阈
值，或者说哑输入）作为输出值。</p>

<script type="math/tex; mode=display">o = \theta + \sum\limits_{i=1}{n} w_i x_i</script>

<p>我们的误差函数也需要从原来的简单相减进行改变：</p>

<script type="math/tex; mode=display">Err(W) = \frac{1}{2} \sum\limits_{k = 1}^{K}(d_k - o_k)^2</script>

<p>其中，k用于表示第k个训练样本，$d_k$表示样本的目标分类，$o_k$表示输出值。</p>

<p>这里常数1\2是用来处理求导后产生的常数。我们从误差函数中可以看出，如果
误差函数越小，神经元就有更好的近似。</p>

<h2 id="梯度下降法gradient-decent">梯度下降法（Gradient Decent）</h2>

<p>对于自适应神经元，我们的训练目标是在样本空间中，对于给定的训练集，使其
误差最小。为此，我们需要引入梯度下降法(Gradient Decent)。</p>

<p>在高维的情况下，同斜率等效的一阶导数称作<a href="https://en.wikipedia.org/wiki/Gradient">梯度</a>(gradient)。我们知道，二维
的情况下函数的极小值是“山谷”的位置，即斜率为0且左右领域函数值皆大于
极小值。推广至高维，高维函数的极小值同样是梯度为0的点。
我们如果要使误差向极小值移动，我们需要判定其移动方向。在二维的情况下，
我们选取<strong>斜率减小</strong>的方向，即函数值减小的防线。</p>

<p>同样的，我们在高维需要选取<strong>梯度减小</strong>的方向。对于权重的某个取值
$\mathbf{w}$，我们可以求其梯度：</p>

<script type="math/tex; mode=display">\nabla F(w_1, w_2,...,w_m)</script>

<p>符号$\nabla$是一个用来表示向量微分的算子。为了更新权重，我们更加关注的
是，权重向量在某一维度上的<strong>分量</strong>的改变趋势。为此，我们需要求取函数在
某一维度上的导数，即为<a href="https://en.wikipedia.org/wiki/Partial_derivative">偏导数</a>：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial{w = w_i}} Err(w)</script>

<p>梯度可以写作:</p>

<script type="math/tex; mode=display">\nabla Err(\mathbf{w}) = [ \frac{\partial}{\partial{w_1}}, \frac{\partial}{\partial{w_2}},...,\frac{\partial}{\partial{w_m}}]</script>

<p>对于单个分量$w_i$而言，我们的训练函数改写如下：</p>

<script type="math/tex; mode=display">w_i \gets w_i - \eta \frac{\partial E}{\partial{w_j}}</script>

<p>因此，我们需要计算误差函数的偏导数。对于有K个样本的训练集，求总的
偏导数：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \frac{\partial}{\partial w_i}\frac{1}{2}\sum\limits_{k=1}^{m}
 (\mathbf{d} - \mathbf{o})^2</script>

<p>提取常数项</p>

<script type="math/tex; mode=display">= \frac{1}{2} \frac{\partial}{\partial w_i} \sum\limits_{k=1}^K
(\mathbf{d} - \mathbf{o})^2</script>

<p>提取和式</p>

<script type="math/tex; mode=display">= \frac{1}{2} \sum\limits_{k=1}^K \frac{\partial}{\partial w_i}
(\mathbf{d} - \mathbf{o})^2\\</script>

<p>考虑到$(\mathbf{d} - \mathbf{o})^2$是关于$w_i$的函数，应用链式法则并消去常数项</p>

<script type="math/tex; mode=display">= \sum\limits{k=1}^K (\mathbf{d} - \mathbf{o})
\frac{\partial}{\partial{w_i}} (\mathbf{d} - \mathbf{o})</script>

<p>接下来，求$\mathbf{d} - \mathbf{o}$的偏导数。我们可以很容易地看出，这是一个线性函数，这意
味着其他分量的导数为0（参考偏导数定义），有作用的只有$w_ix_i$这一项。保留符号，我们有</p>

<script type="math/tex; mode=display">= -\sum\limits{k=1}^K(\mathbf{d} - \mathbf{o})x_i(k)</script>

<p>于是我们的训练函数更改为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta \sum\limits_{k=1}{K}(\mathbf{d} - \mathbf{o}) x_i(k)</script>

<p>这被称为Delta法则(Delta Rule)</p>

<p>我们同样可以迭代求梯度，区别在于不是一次求取所有训练集的误差与更新值，
而是每次计算一个训练样本。但是，这两者实际的结果会有所差异。
每次训练的时候，我们迭代所有的训练样本一次，称作一个epoch。每次迭代样
本的排序一般而言会改变。</p>

<p>在满足以下两个条件之一的时候，停止训练：</p>

<ol>
  <li>当预设的epoch数量运行完毕时</li>
  <li>当误差小于某个预设值</li>
</ol>

<h2 id="神经网络">神经网络</h2>
<p>由于线性神经元只能识别线性可分的情况，不能寻找非线性决策平面。于是，人
们将若干神经元通过某些方式组合起来，形成神经网络。
下图展示了神经网络的一种基本形式，其拓扑结构为单向向前输出的神经网
络，后一层的输出是下一层的输入。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" alt="" /></p>

<p>在神经网络中，激发函数一般使用sigmoid函数。它有一个很好的数学性质：</p>

<script type="math/tex; mode=display">f'(x) = f(x)(1 - f(x))</script>

<p>应用sigmoid函数计算偏导数如下：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} Err(w) = -\sum\limits_{k=1}^K (d_k - o_k)
\frac{\partial}{\partial w_i}sigmoid(net(\mathbf{w}\mathbf{x}))</script>

<p>其中$net(k)$是神经网络的对样本k的输出，即权重与输入的加权求和式。原来
的函数无法一步求导到位，应用
链式法则拆成两个变量相同的函数：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} sigmoid(net(k)) = \frac{\partial
o_k}{\partial net(k)} \frac{\partial sum(k)}{w_i}</script>

<p>根据sigmoid函数的特征，我们有</p>

<script type="math/tex; mode=display">\frac{\partial sigmoid(net(k)}{\partial sum(k)} = sigmoid(sum(k))(1 -
sigmoid(net(k))) = o_k(1 - o_k)</script>

<p>则我们的偏导数为：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \sum\limits_{k=1}^K x_i(k) o_k
(o_k - d_k)(1 - o_k)</script>

<h2 id="bp算法">BP算法</h2>

<p>我们以简单的三层神经网络（输入，隐层,输出）来讨论整个神经网络的权重更
新算法：反向传播法(back propagation)</p>

<p>BP算法大致可以描述如下：</p>
<ol>
  <li>输入训练样本，计算其误差</li>
  <li>根据误差，计算梯度，更新权重</li>
  <li>根据上一步的梯度改变值，计算上一层的权重改变量，更新权重</li>
  <li>重复直到获得满意成果或者epoch用完</li>
</ol>

<p>考虑步骤2,3，我们不难发现，在应用上面的偏导数的前提下，需要知道如何**
递归地**求取前面层级的权重更新量。</p>

<p>我们记</p>

<script type="math/tex; mode=display">\delta = - \frac{\partial Err}{\partial o} \frac{\partial o}{net}= o (o - d)(1 - o)</script>

<p>其中$net$表示该神经元的输入，即上一层的输出向量与权重的加权。</p>

<p>令输出层的某个神经元为l,隐层的某个神经元为m,输入层某个神经元为n。
则，对于连接输出层与隐层的权重$w_{lm}$个，我们的更新公式可以写成:</p>

<script type="math/tex; mode=display">w_{lm} = w_{lm} + \eta \delta_l x_m</script>

<p>其中$\delta_l$是根据输出神经元l计算的值；$x_m$指的是输入向量的值。</p>

<p>连接输入层与隐层的权重的更新公式表示如下:</p>

<script type="math/tex; mode=display">w_{mn} = w_{mn} - \eta \Delta w_{mn}\\
 = w_{mn} - \eta \frac{\partial Err}{\partial w_{mn}}\\</script>

<p>注意到$x_m$是隐层的输入也是输入层的输出，用链式法则拆分偏导数：</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial w_{mn}} = - \frac{\partial
Err}{\partial x_m} \frac{\partial x_m}{\partial w_{mn}} \\
= - \frac{\partial Err}{\partial x_m} \frac{\partial x_m}{w_{mn}}</script>

<p>使用sigmoid函数的特性，有</p>

<script type="math/tex; mode=display">= -\frac{\partial Err}{\partial x_m} x_m(1 - x_m)</script>

<p>对于前面部分的偏导数，我们有</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial x_m} = -\sum\limits_{i = 1}^{l}
\frac{\partial Err}{\partial net} \frac{\partial net}{\partial x_m}\\
= \sum\limits_{i=1}{l} w_{im}g_{i}</script>

<p>于是输入层的更新函数就为：</p>

<script type="math/tex; mode=display">w_{mn} \gets w_mn - \eta x_m(1-x_m)\sum\limits_{i=1}^l w_{im}g_{i} x_n</script>

<p>注：此处限于本人数学水平，对为何要求和没有很好的理解，只能模糊地认
为需要汇总所有的输出层的改变量，方向大致是全微分但是还是没能很好理解，
待有识之士指导。</p>

<p>以下是一个演示图：</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/08135834-8e9b8ff2212545c0aeb1d68103ef3d64.gif" alt="" /></p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>郭兰哲，BP算法，https://jlunevermore.github.io/2016/06/25/10.BP%E7%AE%97%E6%B3%95/</li>
  <li>daniel-D’s blog，BP算法之一种直观的解释 http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html</li>
  <li>
    <p>daniel-D’s blog，BP算法之向后传导 http://www.cnblogs.com/daniel-D/archive/2013/06/06/3121742.html</p>
  </li>
  <li>大量的网络资料（记不清了- -）</li>
</ol>
 -->
    
        <p>呼……终于到最后一篇了，也是开学学的内容。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" title="Machine Learning Review: K-means & KNN">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/56118124.jpg" alt="Machine Learning Review: K-means & KNN">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-08T23:09:34+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/">January 08, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" rel="bookmark" title="Machine Learning Review: K-means & KNN" itemprop="url">Machine Learning Review: K-means & KNN</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>啊……感觉要加快点节奏，不然来不及……</p>

<h1 id="聚类学习">聚类学习</h1>
<p>聚类学习(clustering)是无监督学习(unsupervised learning)的一种。训练样
本的标记信息是未知的，目的是通过无监督学习的算法来揭示数据的内部联系。
聚类学习的目的是，将原有的数据集合划分成若干个不相交的子集。每个子集被
称为一个簇(cluster)。但是，这些簇对于聚类算法而言具体代表什么意义是未
知的，需要使用者去标记。</p>

<h2 id="k-means">K-Means</h2>

<p>K-Means是一个典型的聚类学习算法。给定样本集$D = {x_1, x_2,…,x_m}$，
K-Means的目标是产生k个簇$C = {C_1, C_2,…,C_k}$，使得其平方误差最小：</p>

<script type="math/tex; mode=display">E = \sum\limits_{i=1}^{k}\sum\limits_{x \in C_i} \|\mathbf{x} - \mu_i\|^2</script>

<p>其中，$\mu$是划分为该簇的向量的均值，类似物理学中的质心。</p>

<p>K-means的算法可以大致描述如下：</p>

<ol>
  <li>随机选取k个初始均值向量$\mu_1,\mu_2,…,\mu_k$，用来表示簇的中心</li>
  <li>对于所有样本，根据样本数据与均值向量之间的距离$dst = $|\mathbf{x} - \mu$|$,将样
本划分至最近的簇中</li>
  <li>划分完后，对于所有的簇，根据现有的样本，重新计算其均值向量并且更新。</li>
  <li>重复步骤2,3，直到所有的均值向量不变或者变动范围小于某个阈值</li>
</ol>

<p>K-Means可以看做是不断地更新质心的过程。</p>

<h2 id="对距离的处理">对距离的处理</h2>

<p>K-Means处理距离的时候常使用闵可夫斯基距离(Minkowski distance)。</p>

<script type="math/tex; mode=display">dst_mk(\mathbf{x_i}, \mathbf{x_j}) = (\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^p)^{\frac{1}{p}}</script>

<p>在p值取2的时候，闵可夫斯基距离就是普通的欧几里得距离(Euclidean
distance)：
<script type="math/tex">dst_eu(\mathbf{x_i}, \mathbf{x_j}) =
\sqrt{\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^2}</script></p>

<p>p值取1的时候，闵可夫斯基距离变成曼哈顿距离。</p>

<h1 id="竞争学习">竞争学习</h1>
<h2 id="竞争神经网络competitive-network">竞争神经网络(Competitive Network)</h2>

<p>竞争神经网络是无监督学习的一种，即训练集中的样本没有给出最终的分类结果。
神经网络同样也可以用于无监督学习。其处理方式为增加了一道”竞争”机制。</p>

<p>竞争学习的神经网络通常只有两层：输入层与单层输出节点，输出神经元与输入
节点完全相连。</p>

<p>对于每次输入，只有一个输出神经元会被激活并更新权重。竞争神经网络采取
“赢家通吃”的方法来决定，即只有输出值最大的神经元得到激活并且更新权重。如果下一次
有相近的输入，那么该神经元也会有更大的概率被激活。</p>

<h2 id="最小距离分类minimun-distance-classifier">最小距离分类(minimun distance classifier)</h2>

<p>最小距离首先计算每一个已知类别的平均点，然后对于新的样本，分别计算其到
类别平均点的距离，选取距离最近的那个类别进行分类并且更新平均点。</p>

<h1 id="k邻近算法">K邻近算法</h1>

<p>K邻近算法(K-nearest neighbour)是一个简单的分类算法。其具体思想是，将样
本同训练集中的元素进行一一比较，取前k个最近的元素，选取这$k$个中最多的
类别进行分类。</p>

<p>K邻近算法没有学习过程，被称之为懒惰学习。</p>

<p>具体算法如下：</p>
<ol>
  <li>将数据集划分为测试集与训练集，并且保证其均匀分布</li>
  <li>保存训练集数据</li>
  <li>对每一个测试集中数据，计算其与所有的训练集之间的距离，取前K个。K值
预先指定</li>
  <li>取K个最近距离中类别最多的一项，标记类别</li>
</ol>

<p>K邻近算法虽然简单，但是泛化效率很高。但是，对大的数据集而言，它的效率
不太让人满意。</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>liangdas,最小距离分类法介绍, http://blog.csdn.net/liangdas/article/details/17039583</li>
  <li>51to.com, 7.6 竞争网络和竞争学习（1）, http://book.51cto.com/art/201302/380106.htm</li>
</ol>

 -->
    
        <p>啊……感觉要加快点节奏，不然来不及……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page2">2</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page7">7</a>
        
      </li>
    
  </ul>
  
    <a href="http://MeowAlienOwO.github.io/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js"></script> -->
<!-- <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
     inlineMath: [['$', '$'], [ '\\(', '\\)']],
     displayMath: [['$$', '$$']]
     }});
     </script>
-->
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->





<script>
 var _config = {
     title: 'Entry',
     image: '22958285.jpg'
 }

 socialShare('.social-share', _config)
</script>

          

</body>
</html>
