<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Entry &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
</script>

<!-- Share.js -->
<link href="/assets/css/share.min.css" rel="stylesheet">
<script type="text/javascript" src="/assets/js/vendor/share.min.js"></script>
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="Entry">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (3)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Math/">
				    Math (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.pixiv.net/member_illust.php?mode=medium&illust_id=22958285">ZERO | STAR影法師 [pixiv]</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://MeowAlienOwO.github.io/images/22958285.jpg" alt="Entry">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>喵窝[0]号机</h1>
      <h2>一个伪装成技术博客的吐槽网站。</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/math/math-notes-introduction/" title="Machine Learning series -- Introduction">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/39496577_p0.jpg" alt="Machine Learning series -- Introduction">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-10-01T00:00:00+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/math/math-notes-introduction/">October 01, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/math/math-notes-introduction/" rel="bookmark" title="Machine Learning series -- Introduction" itemprop="url">Machine Learning series -- Introduction</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>

<p>Gintama is one of my favorite anime/manga, which was ended in <em>Jump</em>. The ending is soooooo… Gorilla. Thanks for accompany for years, although later I did not watch it much.</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/q5iRnqKWlJ6dqKQ.jpg" /></p>

<p>↑Above are some useless words.</p>

<p>↓Below are real blog article.</p>

<p>This series of article is aiming to review/preview the mathematic which is necessary for machine learning courses, including MLPR class in UoE and Machine Learning by Andrew Ng, which is held in Coursera.</p>

<h1 id="motivation">Motivation</h1>

<p>I’ve been to Edinburgh for almost one month. As a MSc AI student, I’ve chosen a class called Machine Learning and Pattern Recognization.
Math reasoning and application in the class are quite hard for me, hence I need to make more effort to recall and study necessary mathematic tools.
As for the reason why I did not gather enough math ability for machine learning, well, one of the reason was I spent whole year working for a start-up company.
The works there was mainly aims on application and dealing with lots of – sometimes even silly – requirements and management problems.
The other reason, that I <strong>MUST</strong> critisize, is we did not have <strong>enough</strong> mathematic training during undergraduate period.
As computer student, we don’t have class on linear algebra, probability, and we only have some very basic calculus which was taught in first year. 
It is really ridiculus to those student graduated from chinese university education background. 
Amoung my classmates there are lot of people came from mathematic/physics background. So it is really challenging to catch up the fast-pace classes.</p>

<p>Writing blogs was one of my best learning approach, that’s why I write this series of article. On the otherhand, it is also a good practice of time management and English writing. The blog is also inspired by <a href="https://tony4ai.com">tony4ai</a>, which helped me a lot at the very beginning of this semester.</p>

<p>The mountain is there, stop half way will gain nothing. The only way to success is to reach the top.</p>

<p>As the beginning says, there is nothing can last forever, including your parents, money, room, T-shirt, me, you, and anime Gintama.</p>

<p>And also past success and pride.</p>

<h1 id="content">Content</h1>

<p>In this series I will write:</p>

<ol>
  <li>Mathematic basis of machine learning. Including probability, linear algebra, calculus, statitistics and basic algorithms. I’ll try my best to do the proving and mathematic reasoning.</li>
  <li>Machine learning methods in detail. I’ll recall what I have learned during undergraduate, and all the knowledge I gained not only here but also many other resources of machine learning. Including Coursera classes, books, blogs and many other machine learning related medias.</li>
  <li>Pratical cases of machine learning. Here will mostly concentrate on NLP stuff but if possible, also some other tasks including image processing, etc. Most  of them will contain programming.</li>
  <li>Mathematical practice for other topics, like computer graphics, if I have time.</li>
</ol>

<h1 id="main-category">Main Category</h1>

<h2 id="math">Math</h2>

<h2 id="machine-learning">Machine learning</h2>

<p>To be continued.</p>

<h2 id="practical-cases-for-ml">Practical Cases for ML</h2>

<p>To be continued.</p>

<h2 id="mathematical-practice">Mathematical Practice</h2>

<p>To be continued.</p>

 -->
    
        <p>いつまでもあると思うな、親と金と若さと部屋とTシャツと私とあなたとアニメ銀魂。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" title="Machine Learning Review: Neural Network">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60827595.jpg" alt="Machine Learning Review: Neural Network">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-09T18:36:10+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/">January 09, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_neural_network/" rel="bookmark" title="Machine Learning Review: Neural Network" itemprop="url">Machine Learning Review: Neural Network</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>呼……终于到最后一篇了，也是开学学的内容。</p>

<p>今年也请多多指教。</p>

<h1 id="神经网络neural-network">神经网络（Neural Network）</h1>

<p>神经网络是一种很古老，但是到现在依然有很广泛的用途的机器学习算法。包括
现在火的深度学习，其基础也是神经网络。</p>

<p>神经网络的定义（来自西瓜书，引用T.Kohonen）：</p>

<blockquote>
  <p>神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，其组织能
够模拟生物神经系统对真实世界作出的交互反应。</p>
</blockquote>

<h2 id="神经元模型neuron-model">神经元模型(Neuron Model)</h2>

<p>如上述的简单单元就是神经网络的最基本组成部分–神经元。一个神经元可以接
受若干个来自其他神经元的输入信号，这些信号通过带权重的连接进行传递，总
输入值与神经元的阈值进行比较，然后通过激活函数处理，形成神经元的输出。</p>

<p>神经元的数学形式可以表示如下：</p>

<script type="math/tex; mode=display">y = f(\sum\limits_{i=1}^{n}w_ix_i - \theta)</script>

<p>其中，$f$表示激活函数，$w_i$表示权重，$x_i$表示属性值，$\theta$表示阈
值。</p>

<p>一种比较基本的激活函数是<strong>阶跃函数</strong>（signal function），即等于或超过阈值则为1，反之为0。
但是这个函数不方便求导与计算误差，我们在实际中比较常见的函数有sigmoid
函数：</p>

<script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script>

<p>神经元具有学习功能，会根据输出值与真实值的误差对权重进行调整，从而达到
泛化的目的。表示神经元的学习方式的函数叫做<strong>训练函数</strong>(training function)</p>

<h2 id="感知机perceptron">感知机（Perceptron）</h2>

<p>感知机是一种最基本的神经网络，其包含两层神经元，输入层接受外界的信号，
传递给输出神经元。输出神经元对输入的数据作加权处理，然后与阈值
（threshold）做比较，判断神经元是否被激活。</p>

<p>感知机可以被认为是一种线性分类器，即在样本空间中寻找一个超平面，使得样
本能够分布在超平面的两端。这需要我们的样本是<strong>线性可分</strong>的。这也是我们
对应用感知机场景的一个基本假设。例如，XOR问题就不是线性可分的，因此无
法应用感知机。</p>

<p>感知机会根据输出值与目标分类值之间的误差更新权重，这就是感知机的**学习
**过程。</p>

<p>基本的感知机表示如下：</p>

<p><script type="math/tex">R = \theta + \sum\limits_{i=1}{m}w_ix_i \\
o = sign(R) = \lbrace +1; if~R > 0 \\ -1; otherwise</script>
其中，阈值$\theta$可以看做一个输入值恒为-1（或者-1）的”哑节点”（dummy
node），那么感知机的表达方式就可以统一为权重的学习。</p>

<p>感知机的训练函数为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta (d - o) x_i, i = 1,2,...,n</script>

<p>其中，$w_i$表示权重，$\eta$表示学习率。学习率是一个常数，用于控制学习
的速度。太低会导致学习过程缓慢，太高则有可能导致学习失败：感知机无法收
敛到一个比较适当的区间。$d$表示类别的值，$o$表示感知机的输出，我们也可
以将$d - o$统一成为误差。</p>

<p>我们不加证明地给出收敛性定理：</p>

<blockquote>
  <p>如果样本是线性可分的，那么感知机将会一定可以在有限的步骤内收敛到一个
解。</p>
</blockquote>

<h2 id="自适应线性神经元adaptive-linear-elements">自适应线性神经元（Adaptive Linear Elements）</h2>

<p>感知机由于采用了阶跃函数作为激发函数，容易出现难以收敛的情况。同时在
样本集不是线性可分的情况下，感知机难以找出一个恰当的近似。为此，我们引
入自适应线性神经元。</p>

<p>简单地说，自适应线性神经元取消了阶跃函数，直接以输入的加权求和（包括阈
值，或者说哑输入）作为输出值。</p>

<script type="math/tex; mode=display">o = \theta + \sum\limits_{i=1}{n} w_i x_i</script>

<p>我们的误差函数也需要从原来的简单相减进行改变：</p>

<script type="math/tex; mode=display">Err(W) = \frac{1}{2} \sum\limits_{k = 1}^{K}(d_k - o_k)^2</script>

<p>其中，k用于表示第k个训练样本，$d_k$表示样本的目标分类，$o_k$表示输出值。</p>

<p>这里常数1\2是用来处理求导后产生的常数。我们从误差函数中可以看出，如果
误差函数越小，神经元就有更好的近似。</p>

<h2 id="梯度下降法gradient-decent">梯度下降法（Gradient Decent）</h2>

<p>对于自适应神经元，我们的训练目标是在样本空间中，对于给定的训练集，使其
误差最小。为此，我们需要引入梯度下降法(Gradient Decent)。</p>

<p>在高维的情况下，同斜率等效的一阶导数称作<a href="https://en.wikipedia.org/wiki/Gradient">梯度</a>(gradient)。我们知道，二维
的情况下函数的极小值是“山谷”的位置，即斜率为0且左右领域函数值皆大于
极小值。推广至高维，高维函数的极小值同样是梯度为0的点。
我们如果要使误差向极小值移动，我们需要判定其移动方向。在二维的情况下，
我们选取<strong>斜率减小</strong>的方向，即函数值减小的防线。</p>

<p>同样的，我们在高维需要选取<strong>梯度减小</strong>的方向。对于权重的某个取值
$\mathbf{w}$，我们可以求其梯度：</p>

<script type="math/tex; mode=display">\nabla F(w_1, w_2,...,w_m)</script>

<p>符号$\nabla$是一个用来表示向量微分的算子。为了更新权重，我们更加关注的
是，权重向量在某一维度上的<strong>分量</strong>的改变趋势。为此，我们需要求取函数在
某一维度上的导数，即为<a href="https://en.wikipedia.org/wiki/Partial_derivative">偏导数</a>：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial{w = w_i}} Err(w)</script>

<p>梯度可以写作:</p>

<script type="math/tex; mode=display">\nabla Err(\mathbf{w}) = [ \frac{\partial}{\partial{w_1}}, \frac{\partial}{\partial{w_2}},...,\frac{\partial}{\partial{w_m}}]</script>

<p>对于单个分量$w_i$而言，我们的训练函数改写如下：</p>

<script type="math/tex; mode=display">w_i \gets w_i - \eta \frac{\partial E}{\partial{w_j}}</script>

<p>因此，我们需要计算误差函数的偏导数。对于有K个样本的训练集，求总的
偏导数：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \frac{\partial}{\partial w_i}\frac{1}{2}\sum\limits_{k=1}^{m}
 (\mathbf{d} - \mathbf{o})^2</script>

<p>提取常数项</p>

<script type="math/tex; mode=display">= \frac{1}{2} \frac{\partial}{\partial w_i} \sum\limits_{k=1}^K
(\mathbf{d} - \mathbf{o})^2</script>

<p>提取和式</p>

<script type="math/tex; mode=display">= \frac{1}{2} \sum\limits_{k=1}^K \frac{\partial}{\partial w_i}
(\mathbf{d} - \mathbf{o})^2\\</script>

<p>考虑到$(\mathbf{d} - \mathbf{o})^2$是关于$w_i$的函数，应用链式法则并消去常数项</p>

<script type="math/tex; mode=display">= \sum\limits{k=1}^K (\mathbf{d} - \mathbf{o})
\frac{\partial}{\partial{w_i}} (\mathbf{d} - \mathbf{o})</script>

<p>接下来，求$\mathbf{d} - \mathbf{o}$的偏导数。我们可以很容易地看出，这是一个线性函数，这意
味着其他分量的导数为0（参考偏导数定义），有作用的只有$w_ix_i$这一项。保留符号，我们有</p>

<script type="math/tex; mode=display">= -\sum\limits{k=1}^K(\mathbf{d} - \mathbf{o})x_i(k)</script>

<p>于是我们的训练函数更改为：</p>

<script type="math/tex; mode=display">w_i \gets w_i + \eta \sum\limits_{k=1}{K}(\mathbf{d} - \mathbf{o}) x_i(k)</script>

<p>这被称为Delta法则(Delta Rule)</p>

<p>我们同样可以迭代求梯度，区别在于不是一次求取所有训练集的误差与更新值，
而是每次计算一个训练样本。但是，这两者实际的结果会有所差异。
每次训练的时候，我们迭代所有的训练样本一次，称作一个epoch。每次迭代样
本的排序一般而言会改变。</p>

<p>在满足以下两个条件之一的时候，停止训练：</p>

<ol>
  <li>当预设的epoch数量运行完毕时</li>
  <li>当误差小于某个预设值</li>
</ol>

<h2 id="神经网络">神经网络</h2>
<p>由于线性神经元只能识别线性可分的情况，不能寻找非线性决策平面。于是，人
们将若干神经元通过某些方式组合起来，形成神经网络。
下图展示了神经网络的一种基本形式，其拓扑结构为单向向前输出的神经网
络，后一层的输出是下一层的输入。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" alt="" /></p>

<p>在神经网络中，激发函数一般使用sigmoid函数。它有一个很好的数学性质：</p>

<script type="math/tex; mode=display">f'(x) = f(x)(1 - f(x))</script>

<p>应用sigmoid函数计算偏导数如下：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} Err(w) = -\sum\limits_{k=1}^K (d_k - o_k)
\frac{\partial}{\partial w_i}sigmoid(net(\mathbf{w}\mathbf{x}))</script>

<p>其中$net(k)$是神经网络的对样本k的输出，即权重与输入的加权求和式。原来
的函数无法一步求导到位，应用
链式法则拆成两个变量相同的函数：</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial w_i} sigmoid(net(k)) = \frac{\partial
o_k}{\partial net(k)} \frac{\partial sum(k)}{w_i}</script>

<p>根据sigmoid函数的特征，我们有</p>

<script type="math/tex; mode=display">\frac{\partial sigmoid(net(k)}{\partial sum(k)} = sigmoid(sum(k))(1 -
sigmoid(net(k))) = o_k(1 - o_k)</script>

<p>则我们的偏导数为：</p>

<script type="math/tex; mode=display">\frac{\partial Err}{\partial w_i} = \sum\limits_{k=1}^K x_i(k) o_k
(o_k - d_k)(1 - o_k)</script>

<h2 id="bp算法">BP算法</h2>

<p>我们以简单的三层神经网络（输入，隐层,输出）来讨论整个神经网络的权重更
新算法：反向传播法(back propagation)</p>

<p>BP算法大致可以描述如下：</p>
<ol>
  <li>输入训练样本，计算其误差</li>
  <li>根据误差，计算梯度，更新权重</li>
  <li>根据上一步的梯度改变值，计算上一层的权重改变量，更新权重</li>
  <li>重复直到获得满意成果或者epoch用完</li>
</ol>

<p>考虑步骤2,3，我们不难发现，在应用上面的偏导数的前提下，需要知道如何**
递归地**求取前面层级的权重更新量。</p>

<p>我们记</p>

<script type="math/tex; mode=display">\delta = - \frac{\partial Err}{\partial o} \frac{\partial o}{net}= o (o - d)(1 - o)</script>

<p>其中$net$表示该神经元的输入，即上一层的输出向量与权重的加权。</p>

<p>令输出层的某个神经元为l,隐层的某个神经元为m,输入层某个神经元为n。
则，对于连接输出层与隐层的权重$w_{lm}$个，我们的更新公式可以写成:</p>

<script type="math/tex; mode=display">w_{lm} = w_{lm} + \eta \delta_l x_m</script>

<p>其中$\delta_l$是根据输出神经元l计算的值；$x_m$指的是输入向量的值。</p>

<p>连接输入层与隐层的权重的更新公式表示如下:</p>

<script type="math/tex; mode=display">w_{mn} = w_{mn} - \eta \Delta w_{mn}\\
 = w_{mn} - \eta \frac{\partial Err}{\partial w_{mn}}\\</script>

<p>注意到$x_m$是隐层的输入也是输入层的输出，用链式法则拆分偏导数：</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial w_{mn}} = - \frac{\partial
Err}{\partial x_m} \frac{\partial x_m}{\partial w_{mn}} \\
= - \frac{\partial Err}{\partial x_m} \frac{\partial x_m}{w_{mn}}</script>

<p>使用sigmoid函数的特性，有</p>

<script type="math/tex; mode=display">= -\frac{\partial Err}{\partial x_m} x_m(1 - x_m)</script>

<p>对于前面部分的偏导数，我们有</p>

<script type="math/tex; mode=display">- \frac{\partial Err}{\partial x_m} = -\sum\limits_{i = 1}^{l}
\frac{\partial Err}{\partial net} \frac{\partial net}{\partial x_m}\\
= \sum\limits_{i=1}{l} w_{im}g_{i}</script>

<p>于是输入层的更新函数就为：</p>

<script type="math/tex; mode=display">w_{mn} \gets w_mn - \eta x_m(1-x_m)\sum\limits_{i=1}^l w_{im}g_{i} x_n</script>

<p>注：此处限于本人数学水平，对为何要求和没有很好的理解，只能模糊地认
为需要汇总所有的输出层的改变量，方向大致是全微分但是还是没能很好理解，
待有识之士指导。</p>

<p>以下是一个演示图：</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/08135834-8e9b8ff2212545c0aeb1d68103ef3d64.gif" alt="" /></p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>郭兰哲，BP算法，https://jlunevermore.github.io/2016/06/25/10.BP%E7%AE%97%E6%B3%95/</li>
  <li>daniel-D’s blog，BP算法之一种直观的解释 http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html</li>
  <li>
    <p>daniel-D’s blog，BP算法之向后传导 http://www.cnblogs.com/daniel-D/archive/2013/06/06/3121742.html</p>
  </li>
  <li>大量的网络资料（记不清了- -）</li>
</ol>
 -->
    
        <p>呼……终于到最后一篇了，也是开学学的内容。</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" title="Machine Learning Review: K-means & KNN">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/56118124.jpg" alt="Machine Learning Review: K-means & KNN">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-08T23:09:34+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/">January 08, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_k-means_&_knn/" rel="bookmark" title="Machine Learning Review: K-means & KNN" itemprop="url">Machine Learning Review: K-means & KNN</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>啊……感觉要加快点节奏，不然来不及……</p>

<h1 id="聚类学习">聚类学习</h1>
<p>聚类学习(clustering)是无监督学习(unsupervised learning)的一种。训练样
本的标记信息是未知的，目的是通过无监督学习的算法来揭示数据的内部联系。
聚类学习的目的是，将原有的数据集合划分成若干个不相交的子集。每个子集被
称为一个簇(cluster)。但是，这些簇对于聚类算法而言具体代表什么意义是未
知的，需要使用者去标记。</p>

<h2 id="k-means">K-Means</h2>

<p>K-Means是一个典型的聚类学习算法。给定样本集$D = {x_1, x_2,…,x_m}$，
K-Means的目标是产生k个簇$C = {C_1, C_2,…,C_k}$，使得其平方误差最小：</p>

<script type="math/tex; mode=display">E = \sum\limits_{i=1}^{k}\sum\limits_{x \in C_i} \|\mathbf{x} - \mu_i\|^2</script>

<p>其中，$\mu$是划分为该簇的向量的均值，类似物理学中的质心。</p>

<p>K-means的算法可以大致描述如下：</p>

<ol>
  <li>随机选取k个初始均值向量$\mu_1,\mu_2,…,\mu_k$，用来表示簇的中心</li>
  <li>对于所有样本，根据样本数据与均值向量之间的距离$dst = $|\mathbf{x} - \mu$|$,将样
本划分至最近的簇中</li>
  <li>划分完后，对于所有的簇，根据现有的样本，重新计算其均值向量并且更新。</li>
  <li>重复步骤2,3，直到所有的均值向量不变或者变动范围小于某个阈值</li>
</ol>

<p>K-Means可以看做是不断地更新质心的过程。</p>

<h2 id="对距离的处理">对距离的处理</h2>

<p>K-Means处理距离的时候常使用闵可夫斯基距离(Minkowski distance)。</p>

<script type="math/tex; mode=display">dst_mk(\mathbf{x_i}, \mathbf{x_j}) = (\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^p)^{\frac{1}{p}}</script>

<p>在p值取2的时候，闵可夫斯基距离就是普通的欧几里得距离(Euclidean
distance)：
<script type="math/tex">dst_eu(\mathbf{x_i}, \mathbf{x_j}) =
\sqrt{\sum\limits_{u=1}^{n}\|x_{iu} - x_{ju}\|^2}</script></p>

<p>p值取1的时候，闵可夫斯基距离变成曼哈顿距离。</p>

<h1 id="竞争学习">竞争学习</h1>
<h2 id="竞争神经网络competitive-network">竞争神经网络(Competitive Network)</h2>

<p>竞争神经网络是无监督学习的一种，即训练集中的样本没有给出最终的分类结果。
神经网络同样也可以用于无监督学习。其处理方式为增加了一道”竞争”机制。</p>

<p>竞争学习的神经网络通常只有两层：输入层与单层输出节点，输出神经元与输入
节点完全相连。</p>

<p>对于每次输入，只有一个输出神经元会被激活并更新权重。竞争神经网络采取
“赢家通吃”的方法来决定，即只有输出值最大的神经元得到激活并且更新权重。如果下一次
有相近的输入，那么该神经元也会有更大的概率被激活。</p>

<h2 id="最小距离分类minimun-distance-classifier">最小距离分类(minimun distance classifier)</h2>

<p>最小距离首先计算每一个已知类别的平均点，然后对于新的样本，分别计算其到
类别平均点的距离，选取距离最近的那个类别进行分类并且更新平均点。</p>

<h1 id="k邻近算法">K邻近算法</h1>

<p>K邻近算法(K-nearest neighbour)是一个简单的分类算法。其具体思想是，将样
本同训练集中的元素进行一一比较，取前k个最近的元素，选取这$k$个中最多的
类别进行分类。</p>

<p>K邻近算法没有学习过程，被称之为懒惰学习。</p>

<p>具体算法如下：</p>
<ol>
  <li>将数据集划分为测试集与训练集，并且保证其均匀分布</li>
  <li>保存训练集数据</li>
  <li>对每一个测试集中数据，计算其与所有的训练集之间的距离，取前K个。K值
预先指定</li>
  <li>取K个最近距离中类别最多的一项，标记类别</li>
</ol>

<p>K邻近算法虽然简单，但是泛化效率很高。但是，对大的数据集而言，它的效率
不太让人满意。</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>liangdas,最小距离分类法介绍, http://blog.csdn.net/liangdas/article/details/17039583</li>
  <li>51to.com, 7.6 竞争网络和竞争学习（1）, http://book.51cto.com/art/201302/380106.htm</li>
</ol>

 -->
    
        <p>啊……感觉要加快点节奏，不然来不及……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/" title="Machine Learning Review: Bayesian Learning">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60680252.jpg" alt="Machine Learning Review: Bayesian Learning">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-08T15:02:38+08:00"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/">January 08, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/computer%20science/machine_learning_review-_bayesian_learning/" rel="bookmark" title="Machine Learning Review: Bayesian Learning" itemprop="url">Machine Learning Review: Bayesian Learning</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>死亡冲师匠失败(艸□`..)但是还是姑且把这个当成攒人品吧……</p>

<h1 id="贝叶斯分类器">贝叶斯分类器</h1>
<h2 id="数学准备">数学准备</h2>
<p>主要用到概率论的一些知识，简要地复习一下。</p>
<h3 id="条件概率">条件概率</h3>
<p>条件概率指的是在事件$B$发生的情况下，事件$A$发生的概率。数学符号记为
$P(A | B)$</p>

<p><img src="http://og78s5hbx.bkt.clouddn.com/bg2011082502.jpg" alt="" /></p>

<p>显然，如果要计算在事件$B$发生的情况下$A$发生的概率，我们有：</p>

<script type="math/tex; mode=display">P(A \| B) = \frac{P(A \cap B)}{P(B)}</script>

<p>那么，我们可以很容易得出：</p>

<script type="math/tex; mode=display">P(A \cap B) = P(A \| B)P(B)</script>

<p>同理，将事件换个位置我们有：</p>

<script type="math/tex; mode=display">P(A\cap B) = P(B \| A)P(A)</script>

<p>那么，我们有：</p>

<script type="math/tex; mode=display">P(A \| B)P(B) = P(B \| A )P(A)</script>

<p>这就是条件概率公式。</p>
<h3 id="全概率公式">全概率公式</h3>

<p><img src="http://og78s5hbx.bkt.clouddn.com/bg2011082504.jpg" alt="" /></p>

<p>假定我们有样本空间$S$与事件$A$，事件$A$与事件$\not A$构成了整个样本空
间。我们引入事件$B$，那么有：</p>

<script type="math/tex; mode=display">P(B) = P(B\|A)P(A) + P(B\| \not A)P(\not A)</script>

<p>证明：
由于$A$与$\not A$之和为整个样本空间，对于事件B我们有:</p>

<script type="math/tex; mode=display">P(B) = P(B \cap A) + P(B \cap \not A)</script>

<p>根据条件概率，我们有：</p>

<script type="math/tex; mode=display">P(B) = P(B \| A)P(A) + P(B \| \not A) P(\not A)</script>

<h3 id="贝叶斯推断">贝叶斯推断</h3>

<p>对条件概率公式变形，我们有如下形式:</p>

<script type="math/tex; mode=display">P(A \| B) = P(A)\frac{P(B \| A)}{P(B)}</script>

<p>我们把$P(A)$看作是<strong>先验概率</strong>（即我们在观察样本之前对事件$A$发生概率的估
计），$P(A|B)$是<strong>后验概率</strong>（即事件$B$发生的情况下，事件$A$发生的概率），
而$P(B |A) / P(B)$被称作<strong>似然度</strong>（likelyhood）。那么，我们的条件概
率公式就可以被认为是：</p>

<blockquote>
  <p>后验概率 = 先验概率 × 似然度</p>
</blockquote>

<p>这就是贝叶斯推断的意义。</p>

<p>用西瓜做比喻。假设我们从一堆西瓜中挑出了若干个，
那么根据贝叶斯公式，我们认为：</p>

<blockquote>
  <p>P(我们预计是甜的情况下真的是甜的) = P(我们预计是甜的) × P(真的是甜
的情况下且我们预计是甜的) / P(真的是甜的)</p>
</blockquote>

<h3 id="例子假阳性问题">例子：假阳性问题</h3>
<p>医学上的假阳性问题是一个很经典的例子，可以用来解释条件概率与贝叶斯定理。
假定某种疾病发生的概率是1%，现在有一种准确率为99%的检查手段可以检测病
情，即患者有病的情况下，检测为阳性的概率为99%。但是，这种手段的误报率
是5%，即患者如果没有病，它有5%的概率呈现阳性。现在有一个病人的检验结果
为阳性，求其得病的概率？</p>

<p>我们的先验概率，即疾病发生的概率是1%。我们要计算的是：“在检测为阳性的
情况下患者患病的概率”，即后验概率。根据贝叶斯定理，我们有：</p>

<script type="math/tex; mode=display">P(A \| B) = P(A)\frac{P(B \| A)}{P(B)}</script>

<p>其中$P(A)$为先验概率，$P(B | A)$是在患者患病的情况下的检测成功率。
$P(B)$是患者被检测出阳性的总概率。对于未知的$P(B)$，我们可以用全概率公
式求解：</p>

<script type="math/tex; mode=display">P(B) = P(B \| A) * P(A) + P(B \| \not A) * P(\not A)</script>

<p>则我们可以计算出，患者被检测出阳性的总概率大致为5.94%，从而，患者检测
为阳性的情况下患病率大致为16.7%。这是一个反直觉的结论，即检测结果为阳
性也只能说明检测对象有大约百分之十七的概率患病。如果发病率更低，检测结
果的效果更差，大家不妨试一试。</p>

<h2 id="贝叶斯分类器的原理">贝叶斯分类器的原理</h2>

<p>将贝叶斯定理与分类器的原理进行结合，我们就得到了贝叶斯分类器。其原理可
以简单地叙述如下：</p>

<blockquote>
  <p>给定一个包含若干个属性${x_1, x_2,…,x_m}$的样本，在已知所有相关概率的情况下，在类别空间
$D = {d_1,d_2,…,d_n}$上寻找最大可能的概率。</p>
</blockquote>

<p>我们先根据某些原则（过去的经验，某些性质比较好的点，等等）给出一个概率
的先验分布，我们的目的是最大化我们的后验概率(即，在样本发生的情况下，
其可以被分为某类的概率)。因此，贝叶斯分类器的数学形式可以表达如下：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(d_i \| \mathbf{x}) \\
~~= argmax_{d_i \in D} P(d_i) \frac{P(\mathbf{x} \| d_i)}{P(\mathbf{x})}</script>

<h2 id="极大似然估计mle与极大后验概率map">极大似然估计（MLE）与极大后验概率（MAP）</h2>

<p>由上式我们可以看出，样本的发生概率$P(\mathbf{x})$对于给定的数据集是一
个常数。但是，对于如何处理分子项$P(d_i)P(\mathbf{x} | d_i)$而言，根据
不同的视角而言会有不同的结论。</p>

<p>目前，概率学界大致有两种看法：频率主义认为，虽然频率的分布是未知的，但
是在实际上是一个确定的值；但是贝叶斯学派认为，频率的分布也是一个未知的
随机变量。</p>

<p>根据频率主义学派的观点，我们有极大似然估计(maximum likelihood
estimation)方法。频率学派认为，样本的分布是确定的，这意味着如果样本满
足独立同分布条件（虽然是什么我不清楚），先验概率
$P(d_i)$这一项在实际上是常数。我们在做计算的时候，只需要考虑似然度
$P(\mathbf{x} | d_i)$这一项即可。根据极大似然估计，我们可以将贝叶斯分
类器转化为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(\mathbf{x} \| d_i)</script>

<p>贝叶斯学派认为，样本的分布是不确定的，先验概率也是随机变量之一，于是，
$P(d_i)$项不为常数，在求最大值时无法约去。我们把这种处理方式称为极大后
验概率(maximum posteriori)。这时，贝叶斯分类器转化为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} P(d_i)P(\mathbf{x} \| d_i)</script>

<p>根据贝叶斯学派的观点，先验概率的选取会对结果造成影响，在现实世界中，这
也意味着之前的认知会对之后的估计造成影响，这里不展开。</p>

<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>

<p>基于贝叶斯公式估计后验概率的时候，由于样本的属性向量$\mathbf{x}$会随着
属性的不同而不同，在属性很多的情况下会是一个天文数字。这导致了两个问题：
在计算的时候出现组合爆炸；在泛化的时候又过于稀疏，难以估计广泛情况。为
了避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”(attribute
conditional independence assumption)，即假设所有的属性对结果的影响是独
立的。</p>

<p>根据属性条件独立性假设，我们可以将贝叶斯分类器重写为：</p>

<script type="math/tex; mode=display">Y = argmax_{d_i \in D} \frac{P(d_i)}{P(\mathbf{x})}
\prod_{j=1}^{m} P(x_j \| d_i)</script>

<p>由于对于所有的属性$P(\mathbf{x})$均为常数，我们可以将其约去。先验概率
的选取需要考虑问题的性质，如果我们可以很有信心地得出先验概率，则用已经
确定的先验概率。否则，我们可以对样本集进行统计，或者使用正态分布等。
特征是连续值的情况下，通常假设其服从于高斯分布（正态分布）。</p>

<p>当连乘式中出现某个属性没有出现，即其概率为0的情况，无论其他属性有多高
的可能性，其总概率为0，而这是我们不想看到的。其解决方法是m-估计(m-estimation):</p>

<script type="math/tex; mode=display">\frac{\|D_c\| + m\|D_c\|p}{\|D\| + m}</script>

<p>其中，$|D_c|$指的是该类别的样本总量，$|D|$是样本总量，$p$是<strong>该类
别</strong>的先验概率，$m$是等效样本的大小(equivalent sample size)。这里，$m$
可以取正的任意值。在样本量足够大的情况下，取一个较小的m不会对最终结果
有很大的影响。</p>

<h1 id="思想及其应用">思想及其应用</h1>

<p>在Paul Graham的《黑客与画家》一书中，使用了朴素贝叶斯分类器来分辨垃圾
邮件，具体操作是抽取关键词，并且计算关键词出现与邮件为垃圾邮件的概率。
朴素贝叶斯分类器的效率极高，在这方面的应用往往有99%以上的正确率。</p>

<p>另外，朴素贝叶斯的基本思想–将属性分开来独立地看待，也是在自然语言处理
方面很有用的隐马尔科夫模型的基本假设。</p>

<h1 id="reference">Reference</h1>
<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>阮一峰，贝叶斯推断及其互联网应用（一）：定理简介，http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html</li>
  <li>阮一峰，贝叶斯推断及其互联网应用（二）：过滤垃圾邮件，http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_two.html</li>
  <li>冰枫的随笔， 贝叶斯方法的m-估计，http://blog.csdn.net/cyningsun/article/details/8671975</li>
  <li>张洋,算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification),http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html</li>
</ol>

 -->
    
        <p>死亡冲师匠失败(艸□`..)但是还是姑且把这个当成攒人品吧……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" title="Machine Learning Review: Decision Tree & Random Forest">
            
            <img src="http://og78s5hbx.bkt.clouddn.com/60741116.jpg" alt="Machine Learning Review: Decision Tree & Random Forest">
            
        </a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2017-01-06T21:08:28+08:00"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/">January 06, 2017</a></time></span><span class="author vcard"><span class="fn"><a href="http://MeowAlienOwO.github.io/about/" title="About 死鱼眼的喵星人">死鱼眼的喵星人</a></span></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/machine%20learning/2017-01-06-machine_learning_review-_decision_tree_&_random_forest/" rel="bookmark" title="Machine Learning Review: Decision Tree & Random Forest" itemprop="url">Machine Learning Review: Decision Tree & Random Forest</a></h1>
    
  </header>
  <div class="entry-content">
    <!-- <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>

<p>死亡冲锋抽师匠！</p>

<h1 id="decision-tree">Decision Tree</h1>

<p>决策树是一个有层次(hierachical)的结构，可以通过分治法
（divide-and-conquer）来将数据分成多个类别。</p>

<p>决策树的一个特点是，可以用来处理不能用数字表示的数据，比如说西瓜的颜色
等等。
决策树学习的目标是，构建一棵树，使之能够表示决策过程。树的节点是决策点，
树的叶子是最后的分类。算法可以大致描述如下：</p>

<ol>
  <li>Input：
    <ul>
      <li>训练集$D$: ${(x_1, y_1), (x_2, y_2)….}$</li>
      <li>属性集$A$: ${a_1, a_2,…a_d}$，每个属性集有若干取值，比如说西瓜：
硬度有很硬，硬，普通，软；大小有很大，大，中，小，很小…这里的硬
度与大小就是不同的$a_n$</li>
    </ul>
  </li>
  <li>Function DecisionTreeGen($D$, $A$)</li>
  <li>生成节点Node</li>
  <li><code>if</code> $D$中所有元素属于属性$C$：
    <ol>
      <li>将Node标记为$C$类叶子，返回</li>
    </ol>
  </li>
  <li><code>if</code> $A$为空集，或者$D$的样本在$A$上均有相同取值:
    <ol>
      <li>将Node标记为叶子，其类别标记为$D$中样本数最多的类，返回</li>
    </ol>
  </li>
  <li><strong>从A中选择最优划分属性</strong>$a_m$</li>
  <li><code>for</code> $a_m$的每一个值$a_m^v$:
    <ol>
      <li>为Node生成一个分支SubNode。</li>
      <li>令$D_v$ 为训练集中在属性$a_m$上取值为$a_m^v$的样本集</li>
      <li><code>if</code> $D_v$为空集：
        <ol>
          <li>SubNode标记为叶子，类别取$D$中样本最多的类，返回</li>
        </ol>
      </li>
      <li><code>else</code>：
        <ol>
          <li>SubNode = TreeGenerate($D_v$, $A - a_m$)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>返回：以Node为根节点的决策树</li>
</ol>

<p>如上算法的3,4,6.3分别表示了决策树的三种递归停止的情形：</p>
<ol>
  <li>当前节点的样本都属于同一类别无需划分(最终划分到大小这一类，都是大的西瓜)</li>
  <li>当前属性集为空，或者所有的样本在所有属性上都有相同的取值（这里指某
一个属性没有办法作为划分属性，比如说如果有属性“是水果”–所有的西
瓜都是水果，这个属性就没有意义）</li>
  <li>当前节点包含的样本集合为空</li>
</ol>

<p>解决递归停止的情形2的方法称为“多数表决”法。</p>

<p>我们可以很明显地看出，步骤5是决定决策树算法性能的关键。</p>

<h2 id="信息熵">信息熵</h2>

<p>我们的目标是寻找一个较好的找出最优的划分属性的方法，使得其划分后的样本
能够<strong>最纯</strong>–换言之，尽可能相近的类别应该尽可能分在一起。为此，我们需
要了解信息熵。</p>

<p>信息熵是对不确定性的测量。一件事情发生的概率越低，描述其所需要的信息量
就越多。同样的，对于决策树而言，最容易划分的属性类别往往应该在高处，不
容易划分的在低处。信息熵的定义为：</p>

<script type="math/tex; mode=display">Ent(D) = - \sum\limits_{k=1}^{\|y\|}p_{k}log_{2}p_{k}</script>

<p>当p为0的时候，信息熵为0。</p>

<h2 id="id3算法与信息增益">ID3算法与信息增益</h2>

<p>但是，单单用信息熵我们是无法找到什么属性能够“最优”地划分样本集–样本
往往由多个属性复合而成，我们选取划分属性的时候应该怎么做呢？ID3算法
(iterative dichotomiser)引入了“信息增益”的概念。</p>

<p>离散属性$a$有$V$个可能的取值${a^1, a^2…a^V}$。如果使用$a$来对样本
进行划分的话，会有$V$个分支节点，每个分支节点下都包含若干属性为$a^v$的
样本，记作$D^v$。我们可以计算每个节点的信息熵，取$|D^v| / |D|$为权
重，然后对所有节点的信息熵进行加权求和。样本集$D$的信息熵与加权求和后
的结果定义为<strong>信息增益</strong>(gain)：</p>

<script type="math/tex; mode=display">Gain(D, a) = Ent(D) - \sum\limits_{v = 1}^{V}\frac{\|D^v\|}{\|D\|}Ent(D^v)</script>

<p>ID3算法在每次选取属性的时候，计算所有属性的增益，然后选取增益最大的一
个属性进行划分，以此类推。</p>

<h2 id="c45算法与增益率">C4.5算法与增益率</h2>

<p>ID3算法的问题在于，它会天然地倾向于属性多的类别。比如说，如果把编号(id)作为
一类计入考虑，ID3会先划分编号–虽然划分非常纯净，但这毫无用处。为了克
服这一缺点，C4.5算法引入了“增益率”(gai ratio)的概念。增益率定义为：</p>

<script type="math/tex; mode=display">GainRatio(D, a) = \frac{Gain(D, a)}{IV(a)}</script>

<p>其中，固有值(intrinsic value)$IV$定义为：</p>

<script type="math/tex; mode=display">IV(a) = - \sum\limits_{v=1}{V} \frac{\|D^v\|}{\|D\|}log_{2}\frac{\|D^v\|}{\|D\|}</script>

<p>但是实际上增益率的处理会导致偏向取值少的类别。在实际应用中，会先用启发
式算法找出信息增益水平高于平均水平的属性，然后再选择高增益的属性。</p>

<h2 id="过拟合">过拟合</h2>

<p>决策树也会过拟合，主要的表现为分支过多，导致判断很多不必要的噪声。降低
过拟合的风险的主要手段是剪枝。剪枝的手段主要有预剪枝与后剪枝两种。预剪
枝指的是在构造树的过程中满足某些条件停止分支的构建，而后剪枝指构建完
整的决策树后再进行剪枝。</p>

<h1 id="随机森林">随机森林</h1>

<p>// 注：这是根据中文网络上比较常见的随机森林与西瓜书写的，可能跟PPT有出
// 入</p>

<h2 id="集成学习">集成学习</h2>

<p>集成学习(ensemble learning)是一种将多个学习器组合起来完成学习任务的方
法。其一般结构为，先生成一组基<span color="black">♂</span>学习器(base
learner)，然后用某种策略组合其输出。其学习算法称作基<span color="black">♂</span>学习算法(base learning algorithm)</p>

<p><img src="http://images.cnitblog.com/blog/633472/201410/181942114048093.png" alt="" /></p>

<p>集成学习的种类大致有两种：</p>
<ol>
  <li>个体间存在强依赖关系，必须串行执行。代表：Boosting</li>
  <li>个体间没有强依赖关系，可以并行执行。代表：Bagging, 随机森林
    <h2 id="baggingbootstrap-aggregating">Bagging（Bootstrap Aggregating）</h2>
  </li>
</ol>

<p>并行学习器的基本思想是，通过使基学习器之间尽量互相独立（有较大差异），
从而提高总体的泛化性能。在此，我们了解一下随机森林的基本型：Bagging算
法。</p>

<p>我们首先对样本进行Bootstrap采样：给定包含$m$个样本的样本集，每次随机抽
选一个，维持原有的采样集不变，重复$m$次，构建新的样本集。正如我们之前
所讨论的，这个样本集大约会包含原有样本集中63.2%的样本。同样的，我们可
以构建任意多个Bootstrap样本集，根据需求，我们构建$T$个样本集，每个样本
集都会包含原有样本集的63.2%。我们对每个采样集训练一个学习器，然后将这
些学习器进行结合。当结论不一致的时候，Bagging使用简单投票法。当得票数
相等时，我们可以随机选一个，或者考察学习器投票的置信率。</p>

<h2 id="随机森林算法">随机森林算法</h2>

<p>随机森林是Bagging的一个变种。随机森林以决策树作为基学习算法，在Bagging
基础上引入随机属性选择。</p>

<p>传统的决策树在选择划分属性的时候，使用一个最优化算法。但是在随机森林中，
对于基决策树的每一个节点，从原有属性集中随机选择一个包含$k$个属性的子
集，然后在这个子集中选取最优属性。$k$的取值决定了随机性的程度，当$k$与
属性集的大小相等时，就为传统决策树；当$k$为1的时候，就是随机选取属性。
一般情况使用的值为：$k=log_{2}d$</p>

<p>随机森林的特性是，起始性能比较差，但是当学习器数量增加时，随机森林的泛
化误差往往比较低。</p>

<h1 id="reference">Reference</h1>

<ol>
  <li>周志华(2015) 《机器学习》（西瓜书）, 2016年1月第一版</li>
  <li>M.Bishop(2006), <strong>Pattern Recognition and Machine Learning</strong></li>
  <li>Lecture Material</li>
  <li>CodingLabs, 算法杂货铺——分类算法之决策树(Decision tree),
http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</li>
</ol>
 -->
    
        <p>GRE姑且考得还算满意吧，复习一个多月有V146还算说得过去……实在不是很想
二刷，太累了……</p>


    
  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page2">2</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page4">4</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page5">5</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page6">6</a>
        
      </li>
    
      <li>
        
          <a href="http://MeowAlienOwO.github.io/page7">7</a>
        
      </li>
    
  </ul>
  
    <a href="http://MeowAlienOwO.github.io/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2018 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>

<script type="text/javascript" src="/assets/js/vendor/share.min.js"></script>




          

</body>
</html>
