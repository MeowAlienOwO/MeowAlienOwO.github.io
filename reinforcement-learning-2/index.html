<!doctype html lang="zh">
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>强化学习（二）：动态规划，蒙特卡洛法，时间差分 &#8211; 喵窝[0]号机</title>
<meta name="description" content="一个伪装成技术博客的吐槽网站。">

<meta name="keywords" content="">

<!-- other plugins config -->
<!-- mathjax -->
<!-- script type="text/javascript"
	   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js"></script> -->
<!-- <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
     </script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script>
     <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {inlineMath: [['$', '$'], [ '\\(', '\\)']]}});
     </script> -->

<!-- Share.js -->
<!-- <link href="/assets/css/share.min.css" rel="stylesheet"> -->
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js"></script> -->
<!-- end of plugins -->

<meta http-equiv="content-type" content="text/html; charset=utf-8" />


<!-- Open Graph -->
<!-- <meta property="og:locale" content="en_US"> -->
<meta property="og:locale" content="zh_CN" />
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习（二）：动态规划，蒙特卡洛法，时间差分">
<meta property="og:description" content="一个伪装成技术博客的吐槽网站。">
<meta property="og:url" content="http://MeowAlienOwO.github.io/reinforcement-learning-2/">
<meta property="og:site_name" content="喵窝[0]号机">





<link rel="canonical" href="http://MeowAlienOwO.github.io/reinforcement-learning-2/">
<link href="http://MeowAlienOwO.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="喵窝[0]号机 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://MeowAlienOwO.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<!--
     <link href="//fonts.useso.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css"> -->

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://MeowAlienOwO.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://MeowAlienOwO.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://MeowAlienOwO.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://MeowAlienOwO.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://MeowAlienOwO.github.io/images/avatar.png" alt="死鱼眼的喵星人 photo" class="author-photo">
					<h4>死鱼眼的喵星人</h4>
					<p>烈風？いいえ、知らない子ですね。（もぐもぐ</p>

				</li>
				<li><a href="http://MeowAlienOwO.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:meowalienowo@outlook.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/https://github.com/MeowAlienOwO"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://MeowAlienOwO.github.io/posts/">All Posts</a></li>
				<li><a href="http://MeowAlienOwO.github.io/tags/">All Tags</a></li>
				
				<li>
				  <a href="/categories/Life/">
				    Life (7)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Computer Science/">
				    Computer Science (19)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Software Engineering/">
				    Software Engineering (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Japanese/">
				    Japanese (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/Machine Learning/">
				    Machine Learning (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/ML/">
				    ML (2)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/category/">
				    category (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/algorithm/">
				    algorithm (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/machine learning/">
				    machine learning (1)
				  </a>
				</li>
				
				<li>
				  <a href="/categories/RL/">
				    RL (1)
				  </a>
				</li>
				
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://MeowAlienOwO.github.io/reinforcement-learning-2/" rel="bookmark" title="强化学习（二）：动态规划，蒙特卡洛法，时间差分">强化学习（二）：动态规划，蒙特卡洛法，时间差分</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2019-04-28T00:00:00+08:00">April 28, 2019</time></span></h2>
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<p>强化学习第二部分</p>

<h1 id="动态规划">动态规划</h1>

<p>动态规划的基本思路是：将问题划分为可存储的子优化问题，通过解决子问题来最终解决父问题。在强化学习中，由于MDP的贝尔曼方程的存在，我们可以很容易地将问题递归表示。</p>

<h2 id="策略迭代">策略迭代</h2>

<p>基本的动态规划算法为策略迭代。策略迭代分为两大部分：</p>
<ol>
  <li>对策略的价值估计</li>
  <li>对策略的优化</li>
</ol>

<p>具体而言，就是先用当前的策略进行价值估计得到$v_t$, 然后根据估计的价值来更新$\pi_t$，在下一时刻，使用更新后的策略继续估算价值。价值估计与优化问题合起来被称为控制问题，这两部分是强化学习所重点关注的地方。</p>

<h3 id="iterative-policy-evaluation-迭代策略估计">Iterative Policy Evaluation 迭代策略估计</h3>

<p>利用贝尔曼方程我们可以递归的计算$v_\pi$:<script type="math/tex">v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s', r} p(s' r | s, a)[r + \gamma v_k(s')]</script></p>

<p>基本的算法为:</p>

<pre><code>input policy pi
init array V[s]=0 for s in S

Repeat 
  delta := 0
  for each s in S:
    v := V(s)
    V(s) := sum_a(pi(a|s) * sum_s',r(p(s', r| s,a)[r + gamma V[s']]))
    delta := max(delta, |v - V(s)|)
until delta &lt; theta
return V

</code></pre>
<p>TODO: 收敛性证明</p>

<h3 id="策略优化">策略优化</h3>
<p>我们首先给出策略优化原理：</p>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>对于一个策略$\pi’$，如果对于所有的状态$s$都有$\sum_a \pi’(a</td>
        <td>s) q_\pi(s, a) \geq \sum_a \pi(a</td>
        <td>s) q_\pi(s, a) $</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>那么，策略$\pi’$就不坏于策略$\pi$。对于任意的$v_\pi(s)$, 数值上，有$v_\pi(s) \leq q_\pi(s,\pi’(s)) \leq v_{\pi’}(s)$后者可以通过期望式展开成序列形式为：$\leq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s] \leq \mathbb{E}<em>{\pi’}[R_t+1 + \gamma q</em>\pi(S_{t+1}, \pi’(S_{t+1}| S_t = s))]$TODO: 详细证明</p>

<p>策略优化算法如下：</p>

<pre><code>init V[s] in R and pi(s) in A(s) arbitrarily, for all s in S

# policy evaluation
Repeat 
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := sum_s',r(p(s', r|s, a)[r + gamma * V[s']])
  until delta &lt; theta
  
# policy improvement
Repeat
  policy-stable := true
  for each s in S:
    a := pi(s)
    pi(s) := argmax_a sum_s,r(p(s', r|s, a)[r + gamma V[s']])
  
    if a != pi(s) then policy-stable := false
  if policy-stable == true then stop; else goto evaluation
</code></pre>

<h2 id="价值迭代">价值迭代</h2>

<p>价值迭代与策略迭代的不同在于：策略迭代每次先进行evaluation,然后根据evaluation的结果选择动作，而价值迭代直接计算每一个动作的期望，根据期望来选取动作。</p>

<p>算法如下:</p>

<pre><code>init array V abitrarily

Repeat
  delta := 0
  for each s in S:
    v := V[s]
    V[s] := max_a sum_s,r(p(s', r| s, a)[r + gamma * V(s')])
    delta := max(delta, |v - V[s])
  until delta &lt; theta
  
  output deterministic policy pi, where
    pi(s) = argmax_a sum_s,r(p(s', r|s, a)[r + gamma*V[s']])
    
</code></pre>

<h1 id="monte-carlo-methods蒙特卡洛法">Monte-Carlo Methods蒙特卡洛法</h1>

<p>动态规划成立的前提是，我们知道环境动态函数$p$，而蒙特卡洛法是为了解决在没有环境动态函数的情况下进行强化学习的问题的统计学方法。</p>

<p>蒙特卡洛法是一种通过经验学习价值函数的方法，其中的经验有两种：</p>
<ol>
  <li>实际经验，从环境中真实学习的经验。</li>
  <li>模拟经验，使用一个模型来近似真实的环境</li>
</ol>

<p>在蒙特卡洛法中，价值$v_\pi$被定义成对回报的采样的平均数。</p>

<script type="math/tex; mode=display">v_\pi(s) \circeq \mathbb{E}[\sum_{k=0}^{T-1}\gamma^kR_{k+1}] \sim \frac{1}{\Epsilon(s)} \sum_{t_i \in \Epsilon(s)}\sum_{k=t_i}^{\gamma^{k-1}R^i_{k+1}}</script>

<h2 id="蒙特卡洛估计">蒙特卡洛估计</h2>

<p>蒙特卡洛有两种计算方法，当每个状态，动作的访问次数趋于无穷时，它们是等价的:</p>
<ol>
  <li>first-visit MC: 只考虑每个episode第一次访问到的(S, A) 对</li>
  <li>every-visit MC: 对所有的(S, A)对进行采样</li>
</ol>

<p>我们这里给出first-visit 的算法：</p>

<pre><code>Init:
  pi := policy to be evaluated
  V := arbitrarily init
  Returns[s] := [] for all s in S

Repeat forever:
  Generate an episode using pi
  for each state s in episode:
    G := return following first occurence of s
    Append G to Returns[s]
    V[s] := average(Return[s])
</code></pre>

<p>蒙特卡洛法同样可以对动作价值进行估计：<script type="math/tex">q_\pi(s, a) \circeq \mathbb{E}[G_t | S_t = s, A_t = a]</script></p>

<h2 id="mc控制">MC控制</h2>

<p>采取贪婪策略进行动作选择满足策略优化定律，我们假设MC的迭代是无限的，给出蒙特卡洛控制算法：</p>

<pre><code>Init 
  for all s in S, a in A:
    Q(s, a) := arbitrarily
    pi(s) := arbitrarily
    Returns(s, a) := []

Repeat forever:
  Choose S_0 in S, A_0 in A[S_0] as start point s.t all pairs have prob &gt; 0
  Generate an episode according to pi
  For (s, a) in episode:
    G := return of s, a
    Append G to Returns(s, a)
    Q(s, a) := average(Returns(s, a))
  For each s in episode:
    pi[s] &lt;- argmax_a Q(s, a]
</code></pre>

<p>单纯的贪婪会使得更新参数变得很慢–很可能陷入某个局部最优然后不断强化，忽视探索其他动作，我们通常使用e-soft 策略来保证探索。e-greedy不改变期望。</p>

<h2 id="off-policy-蒙特卡洛">Off-policy 蒙特卡洛</h2>

<p>estimate的时候的策略$$ 与目标策略$pi$ 不完全相等时，我们称该方法为off-policy方法。</p>

<p>使用蒙特卡洛法时，对于off-policy方法，我们的estimate期望将不基于目标Policy, 而是基于我们的估计策略的policy.在这种情况下，我们可以将整个采样过程看做一个重要性采样，采样的分布是基于估计策略$u$的分布。我们需要用<strong>重要性系数</strong>来修正期望：</p>

<script type="math/tex; mode=display">\rho_{t:T} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{u(A_k)|S_k}</script>

<p>在重要性系数修正下：</p>

<script type="math/tex; mode=display">\mathbb{E}[\rho_{t:T}G_t|S_t = s] = v_\pi(s)</script>

<p>由于重要性采样会导致方差存在上升至无限大的可能性(将重要性采样看做是在某一区间特别密集地取值)，我们还需要对采样的平均长度进行修正：</p>

<script type="math/tex; mode=display">\eta^{-1} = \sum_{t_i in \Epsilon(s)}\rho_{t:T}</script>

<h1 id="时间差分算法temporal-difference">时间差分算法(Temporal-Difference)</h1>

<p>最基本的时间差分算法(TD(0))可以看做每次仅仅往前方看一步进行evaluation。由于这种情况下我们没有办法获得整个序列的回报，我们用当前估计的期望来作为我们的Target。</p>

<h2 id="td-evaluation">TD Evaluation</h2>

<p>最基础的TD Evaluation 算法(TD(0))如下，可以看出同MC方法相比，TD(0)最重要的改变是将更新公式中的累计回报换成了当前的奖励信号与当前估计的下一状态的期望之和:</p>

<pre><code>input policy pi
step size 0 &lt; alpha &lt;= 1
init V[s] for all s in S, arbitrarily except V(terminal) = 0

for each episode:
  init S
  for each step in episode:
    A := action from pi(S)
    take action, observe R, S'
    V[s] := V[s] + alpha[R + gamma * V[S'] - V[S]]
    S := S'
  until S is terminal
</code></pre>

<p>TD算法的好处：首先，每次都要进行更新，避免了蒙特卡洛法需要迭代完整的一个episode再进行更新的问题，其次，需要的计算力与空间更少</p>

<h2 id="td-control">TD Control</h2>
<p>根据是否on-policy, 我们可以将TD Control 分成两种算法:</p>
<ol>
  <li>SARSA: on-policy control</li>
  <li>Q-learning: off-policy control</li>
</ol>

<h3 id="sarsa">SARSA</h3>

<p>我们给出SARSA算法如下</p>

<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0

Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S'
    Choose A' fomr S' using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * Q[S', A'] - Q[S, A]]
    S := S'
    A := A
  Until S is terminal
</code></pre>

<p>SARSA有一种变种：不使用<code>Q[S', A]</code> 来进行更新，而是使用下一步状态的期望<code>sum_a (pi(S')Q(S', a))</code>来进行更新，其思路主要是通过期望计算来降低方差，从而提升学习效率。</p>

<h3 id="q-learning">Q-learning</h3>

<p>Q-learning 使用 算法如下：</p>

<pre><code>Init 
  Q[s, a] for s in S, a in A, arbitrarily, Q[terminate, :] = 0


Repeat for each episode
  Init S
  Choose A from S using policy derived from Q
  Repeat for each step in episode
    Take action A, observe R, S'
    Choose A' fomr S' using policy
    Q[S, A] := Q[S, A] + alpha[R + gamma * max_a (Q[S']) - Q[S, A]]
    S := S'
  Until S is terminal
</code></pre>
<p>注意，同MC Off-policy相比，这个方法不需要重要性系数。其原因是动作a此处是确定的(argmax(Q[S]))，而非随机变量。</p>

<h2 id="n-step-td">N-Step TD</h2>

<p>N-Step TD 是在单步TD与MC方法中间的桥梁：</p>

<script type="math/tex; mode=display">G_{t:t+n} = \sum_{k=1}^{n}\gamma^{k-1}R_{t+k} + \gamma^n V_{t+n-1}(S_{t+n})</script>

<p>在n-step TD的情况下，我们需要使用重要性系数来修正我们的off-policy算法的价值估计。</p>

<script type="math/tex; mode=display">Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha \rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]

\rho_{t:h} = \prod_{k=t}^{\min(h,T-1)} \frac{\pi(A_k|S_k)}{u(A_k|S_k)}</script>

</body></html>

      <footer class="entry-meta">
        <span class="entry-tags"></span>
        
        <div class="social-share" >
  <!-- <ul class="socialcount socialcount-small inline-list"> -->
  <!--   <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://MeowAlienOwO.github.io/reinforcement-learning-2/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li> -->
  <!--   <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://MeowAlienOwO.github.io/reinforcement-learning-2/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li> -->
  <!--   <li class="googleplus"><a href="https://plus.google.com/share?url=http://MeowAlienOwO.github.io/reinforcement-learning-2/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li> -->
  <!-- </ul> -->
</div><!-- /.social-share -->

      </footer>
    </div><!-- /.entry-content -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/" title="强化学习导论（一）">强化学习导论（一）</a></h3>
      <p>强化学习导论的学习内容, 包含上课内容与其他自己找的资料，主要是复习用什么是强化学习  通过与环境的持续交互学习，从而解决序列性的决策问题。强化学习是机器学习的一个分支，其特点为：  没有监督数据，只有奖励信号  奖励信号不一定是实时的  行为与环境交互 影响数据  时间是...&hellip; <a href="http://MeowAlienOwO.github.io/machine%20learning/rl/reinforcement_learning/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://MeowAlienOwO.github.io/ml/algorithm/machine_learning-_linear_regression/" title="Machine Learning: Linear Regression">Machine Learning: Linear Regression</a></h4>
        <span>Published on October 02, 2018</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://MeowAlienOwO.github.io/ml/category/math-notes-introduction/" title="Machine Learning series -- Introduction">Machine Learning series -- Introduction</a></h4>
        <span>Published on October 01, 2018</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->

  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 死鱼眼的喵星人. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/" rel="notfollow">HPSTR Theme</a>.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
<!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
<!-- <script>window.jQuery || document.write('<script src="http://MeowAlienOwO.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js"></script> -->
<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js"></script> -->
<!-- <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
     inlineMath: [['$', '$'], [ '\\(', '\\)']],
     displayMath: [['$$', '$$']]
     }});
     </script>
-->
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<link href="https://cdn.bootcss.com/social-share.js/1.0.16/css/share.min.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/social-share.js/1.0.16/js/social-share.min.js"></script>
<script src="http://MeowAlienOwO.github.io/assets/js/scripts.min.js"></script>
<!-- <script type="text/javascript" src="/assets/js/vendor/share.min.js" async></script> -->



    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'meowalienowogithubio'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



<script>
 var _config = {
     title: '强化学习（二）：动态规划，蒙特卡洛法，时间差分',
     image: ''
 }

 socialShare('.social-share', _config)
</script>



</body>

</html>
